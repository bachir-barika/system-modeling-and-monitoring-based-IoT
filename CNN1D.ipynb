{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN1D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EGmI3n_Iewt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#إستدعاء المكتبيات\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlC3w6gQnxiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.read_csv(\"/data/data1.csv\")\n",
        "df2=pd.read_csv(\"/data/data2.csv\")\n",
        "df3=pd.read_csv(\"/data/data3.csv\")\n",
        "df4=pd.read_csv(\"/data/data4.csv\")\n",
        "df5=pd.read_csv(\"/data/data5.csv\")\n",
        "df6=pd.read_csv(\"/data/data6.csv\")\n",
        "df7=pd.read_csv(\"/data/data7.csv\")\n",
        "df8=pd.read_csv(\"/data/data8.csv\")\n",
        "df9=pd.read_csv(\"/data/data9.csv\")\n",
        "df10=pd.read_csv(\"/data/data10.csv\")\n",
        "df11=pd.read_csv(\"/data/data11.csv\")\n",
        "df12=pd.read_csv(\"/data/data12.csv\")\n",
        "df13=pd.read_csv(\"/data/data13.csv\")\n",
        "df14=pd.read_csv(\"/data/data14.csv\")\n",
        "df15=pd.read_csv(\"/data/data15.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ppyhjKwtbon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#دمج مجموعات البيانات\n",
        "df = pd.concat([df1, df2,df3,df4, df5,df6,df7, df8,df9,df10, df11,df12,df13, df14,df15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBI_-ZxhHpQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84e0129d-7a6e-4178-aeae-a8ccd129d700"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UwHPkklRGdm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "e9da250a-aef3-4de7-8f1d-b47345f1a76b"
      },
      "source": [
        "df.describe(include=\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-PA1:VH</th>\n",
              "      <th>R1-PM1:V</th>\n",
              "      <th>R1-PA2:VH</th>\n",
              "      <th>R1-PM2:V</th>\n",
              "      <th>R1-PA3:VH</th>\n",
              "      <th>R1-PM3:V</th>\n",
              "      <th>R1-PA4:IH</th>\n",
              "      <th>R1-PM4:I</th>\n",
              "      <th>R1-PA5:IH</th>\n",
              "      <th>R1-PM5:I</th>\n",
              "      <th>R1-PA6:IH</th>\n",
              "      <th>R1-PM6:I</th>\n",
              "      <th>R1-PA7:VH</th>\n",
              "      <th>R1-PM7:V</th>\n",
              "      <th>R1-PA8:VH</th>\n",
              "      <th>R1-PM8:V</th>\n",
              "      <th>R1-PA9:VH</th>\n",
              "      <th>R1-PM9:V</th>\n",
              "      <th>R1-PA10:IH</th>\n",
              "      <th>R1-PM10:I</th>\n",
              "      <th>R1-PA11:IH</th>\n",
              "      <th>R1-PM11:I</th>\n",
              "      <th>R1-PA12:IH</th>\n",
              "      <th>R1-PM12:I</th>\n",
              "      <th>R1:F</th>\n",
              "      <th>R1:DF</th>\n",
              "      <th>R1-PA:Z</th>\n",
              "      <th>R1-PA:ZH</th>\n",
              "      <th>R1:S</th>\n",
              "      <th>R2-PA1:VH</th>\n",
              "      <th>R2-PM1:V</th>\n",
              "      <th>R2-PA2:VH</th>\n",
              "      <th>R2-PM2:V</th>\n",
              "      <th>R2-PA3:VH</th>\n",
              "      <th>R2-PM3:V</th>\n",
              "      <th>R2-PA4:IH</th>\n",
              "      <th>R2-PM4:I</th>\n",
              "      <th>R2-PA5:IH</th>\n",
              "      <th>R2-PM5:I</th>\n",
              "      <th>R2-PA6:IH</th>\n",
              "      <th>...</th>\n",
              "      <th>R4-PA2:VH</th>\n",
              "      <th>R4-PM2:V</th>\n",
              "      <th>R4-PA3:VH</th>\n",
              "      <th>R4-PM3:V</th>\n",
              "      <th>R4-PA4:IH</th>\n",
              "      <th>R4-PM4:I</th>\n",
              "      <th>R4-PA5:IH</th>\n",
              "      <th>R4-PM5:I</th>\n",
              "      <th>R4-PA6:IH</th>\n",
              "      <th>R4-PM6:I</th>\n",
              "      <th>R4-PA7:VH</th>\n",
              "      <th>R4-PM7:V</th>\n",
              "      <th>R4-PA8:VH</th>\n",
              "      <th>R4-PM8:V</th>\n",
              "      <th>R4-PA9:VH</th>\n",
              "      <th>R4-PM9:V</th>\n",
              "      <th>R4-PA10:IH</th>\n",
              "      <th>R4-PM10:I</th>\n",
              "      <th>R4-PA11:IH</th>\n",
              "      <th>R4-PM11:I</th>\n",
              "      <th>R4-PA12:IH</th>\n",
              "      <th>R4-PM12:I</th>\n",
              "      <th>R4:F</th>\n",
              "      <th>R4:DF</th>\n",
              "      <th>R4-PA:Z</th>\n",
              "      <th>R4-PA:ZH</th>\n",
              "      <th>R4:S</th>\n",
              "      <th>control_panel_log1</th>\n",
              "      <th>control_panel_log2</th>\n",
              "      <th>control_panel_log3</th>\n",
              "      <th>control_panel_log4</th>\n",
              "      <th>relay1_log</th>\n",
              "      <th>relay2_log</th>\n",
              "      <th>relay3_log</th>\n",
              "      <th>relay4_log</th>\n",
              "      <th>snort_log1</th>\n",
              "      <th>snort_log2</th>\n",
              "      <th>snort_log3</th>\n",
              "      <th>snort_log4</th>\n",
              "      <th>marker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>55663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-15.802424</td>\n",
              "      <td>130764.039577</td>\n",
              "      <td>2.175196</td>\n",
              "      <td>131035.528095</td>\n",
              "      <td>6.834315</td>\n",
              "      <td>131395.717581</td>\n",
              "      <td>-14.334996</td>\n",
              "      <td>393.949321</td>\n",
              "      <td>3.538540</td>\n",
              "      <td>387.438133</td>\n",
              "      <td>6.129781</td>\n",
              "      <td>381.912845</td>\n",
              "      <td>-15.798835</td>\n",
              "      <td>131056.980030</td>\n",
              "      <td>0.207857</td>\n",
              "      <td>297.083556</td>\n",
              "      <td>0.227606</td>\n",
              "      <td>87.397031</td>\n",
              "      <td>-14.504282</td>\n",
              "      <td>386.557188</td>\n",
              "      <td>-1.734936</td>\n",
              "      <td>9.979982</td>\n",
              "      <td>6.123374</td>\n",
              "      <td>9.494176</td>\n",
              "      <td>59.992801</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.018428</td>\n",
              "      <td>788.868750</td>\n",
              "      <td>-15.216491</td>\n",
              "      <td>127033.389923</td>\n",
              "      <td>4.751134</td>\n",
              "      <td>128015.428015</td>\n",
              "      <td>5.510410</td>\n",
              "      <td>128362.246185</td>\n",
              "      <td>15.836436</td>\n",
              "      <td>395.109497</td>\n",
              "      <td>-6.961603</td>\n",
              "      <td>392.508845</td>\n",
              "      <td>-6.437082</td>\n",
              "      <td>...</td>\n",
              "      <td>2.278991</td>\n",
              "      <td>131355.212680</td>\n",
              "      <td>7.065760</td>\n",
              "      <td>131745.074472</td>\n",
              "      <td>-13.931742</td>\n",
              "      <td>391.330912</td>\n",
              "      <td>3.446031</td>\n",
              "      <td>384.399819</td>\n",
              "      <td>6.096400</td>\n",
              "      <td>379.952713</td>\n",
              "      <td>-15.563852</td>\n",
              "      <td>131397.999652</td>\n",
              "      <td>0.257084</td>\n",
              "      <td>292.112647</td>\n",
              "      <td>0.207103</td>\n",
              "      <td>82.439295</td>\n",
              "      <td>-14.144585</td>\n",
              "      <td>384.036050</td>\n",
              "      <td>-1.859917</td>\n",
              "      <td>9.834635</td>\n",
              "      <td>5.989009</td>\n",
              "      <td>9.073233</td>\n",
              "      <td>59.992750</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.016616</td>\n",
              "      <td>749.014459</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.026436</td>\n",
              "      <td>0.026500</td>\n",
              "      <td>0.035597</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>100.876750</td>\n",
              "      <td>8546.118477</td>\n",
              "      <td>111.743169</td>\n",
              "      <td>5393.135370</td>\n",
              "      <td>97.065063</td>\n",
              "      <td>5443.752388</td>\n",
              "      <td>99.601107</td>\n",
              "      <td>190.966011</td>\n",
              "      <td>109.504977</td>\n",
              "      <td>151.277183</td>\n",
              "      <td>95.294904</td>\n",
              "      <td>153.326452</td>\n",
              "      <td>100.877118</td>\n",
              "      <td>6152.379663</td>\n",
              "      <td>13.075863</td>\n",
              "      <td>2687.617199</td>\n",
              "      <td>12.488596</td>\n",
              "      <td>897.541412</td>\n",
              "      <td>99.605025</td>\n",
              "      <td>154.484403</td>\n",
              "      <td>68.383257</td>\n",
              "      <td>47.241783</td>\n",
              "      <td>73.059209</td>\n",
              "      <td>47.875569</td>\n",
              "      <td>0.610045</td>\n",
              "      <td>0.087799</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.242813</td>\n",
              "      <td>14048.448459</td>\n",
              "      <td>101.837622</td>\n",
              "      <td>16155.767175</td>\n",
              "      <td>111.043204</td>\n",
              "      <td>12106.876201</td>\n",
              "      <td>96.270117</td>\n",
              "      <td>11990.863815</td>\n",
              "      <td>99.876094</td>\n",
              "      <td>171.765698</td>\n",
              "      <td>94.996062</td>\n",
              "      <td>152.357765</td>\n",
              "      <td>108.896267</td>\n",
              "      <td>...</td>\n",
              "      <td>111.828597</td>\n",
              "      <td>4733.901358</td>\n",
              "      <td>97.085981</td>\n",
              "      <td>4777.648212</td>\n",
              "      <td>99.653296</td>\n",
              "      <td>187.094100</td>\n",
              "      <td>109.561785</td>\n",
              "      <td>148.882516</td>\n",
              "      <td>95.495537</td>\n",
              "      <td>150.929876</td>\n",
              "      <td>100.882320</td>\n",
              "      <td>5536.542517</td>\n",
              "      <td>13.150046</td>\n",
              "      <td>2621.155809</td>\n",
              "      <td>12.523032</td>\n",
              "      <td>850.696972</td>\n",
              "      <td>99.627784</td>\n",
              "      <td>151.746973</td>\n",
              "      <td>67.783975</td>\n",
              "      <td>47.562328</td>\n",
              "      <td>72.087423</td>\n",
              "      <td>45.998572</td>\n",
              "      <td>0.609958</td>\n",
              "      <td>0.087273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.248023</td>\n",
              "      <td>14041.170907</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.186082</td>\n",
              "      <td>0.160430</td>\n",
              "      <td>0.160618</td>\n",
              "      <td>0.185285</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.009450</td>\n",
              "      <td>0.008749</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.501948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.903018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.010000</td>\n",
              "      <td>1.852102e-01</td>\n",
              "      <td>-3.140569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>...</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-169.984571</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>-178.875387</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-142.790610</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-165.686647</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>-155.458725</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-145.203989</td>\n",
              "      <td>-179.719672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.612733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.460000</td>\n",
              "      <td>6.781427e-03</td>\n",
              "      <td>-3.093717</td>\n",
              "      <td>-45.998332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-100.416583</td>\n",
              "      <td>131057.982300</td>\n",
              "      <td>-102.129727</td>\n",
              "      <td>130732.029800</td>\n",
              "      <td>-69.459673</td>\n",
              "      <td>131133.202100</td>\n",
              "      <td>-98.159129</td>\n",
              "      <td>305.793700</td>\n",
              "      <td>-94.790138</td>\n",
              "      <td>311.836330</td>\n",
              "      <td>-66.279758</td>\n",
              "      <td>303.962600</td>\n",
              "      <td>-100.399394</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-98.227884</td>\n",
              "      <td>307.807910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.291006e+00</td>\n",
              "      <td>-0.028589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-101.562499</td>\n",
              "      <td>128826.461300</td>\n",
              "      <td>-96.490178</td>\n",
              "      <td>128769.000000</td>\n",
              "      <td>-69.052873</td>\n",
              "      <td>128901.681100</td>\n",
              "      <td>-65.091252</td>\n",
              "      <td>310.554560</td>\n",
              "      <td>-83.182013</td>\n",
              "      <td>316.446289</td>\n",
              "      <td>-105.295719</td>\n",
              "      <td>...</td>\n",
              "      <td>-101.992217</td>\n",
              "      <td>131018.156300</td>\n",
              "      <td>-69.430847</td>\n",
              "      <td>131435.093800</td>\n",
              "      <td>-97.643467</td>\n",
              "      <td>304.511930</td>\n",
              "      <td>-94.919132</td>\n",
              "      <td>309.494019</td>\n",
              "      <td>-67.173162</td>\n",
              "      <td>302.598938</td>\n",
              "      <td>-100.009783</td>\n",
              "      <td>131272.515600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-97.723682</td>\n",
              "      <td>306.249634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.347653e+00</td>\n",
              "      <td>-0.028625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-28.865614</td>\n",
              "      <td>131684.814000</td>\n",
              "      <td>8.118812</td>\n",
              "      <td>131358.861500</td>\n",
              "      <td>13.401483</td>\n",
              "      <td>131760.033900</td>\n",
              "      <td>-23.514188</td>\n",
              "      <td>378.671480</td>\n",
              "      <td>1.885031</td>\n",
              "      <td>383.249230</td>\n",
              "      <td>6.881223</td>\n",
              "      <td>376.474160</td>\n",
              "      <td>-28.842695</td>\n",
              "      <td>131609.594200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.749101</td>\n",
              "      <td>380.319470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.011317e+01</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-29.665834</td>\n",
              "      <td>130020.265600</td>\n",
              "      <td>12.330052</td>\n",
              "      <td>129954.758400</td>\n",
              "      <td>10.077210</td>\n",
              "      <td>130080.124800</td>\n",
              "      <td>24.534053</td>\n",
              "      <td>383.615450</td>\n",
              "      <td>-5.729578</td>\n",
              "      <td>388.010090</td>\n",
              "      <td>-6.222322</td>\n",
              "      <td>...</td>\n",
              "      <td>7.969843</td>\n",
              "      <td>131634.667500</td>\n",
              "      <td>13.883972</td>\n",
              "      <td>132060.913100</td>\n",
              "      <td>-23.124577</td>\n",
              "      <td>376.997009</td>\n",
              "      <td>1.512609</td>\n",
              "      <td>380.868800</td>\n",
              "      <td>7.116394</td>\n",
              "      <td>375.009280</td>\n",
              "      <td>-28.742983</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.359489</td>\n",
              "      <td>378.353119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.017212e+01</td>\n",
              "      <td>0.015089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>68.096034</td>\n",
              "      <td>132186.279400</td>\n",
              "      <td>104.897113</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>85.324875</td>\n",
              "      <td>132261.499300</td>\n",
              "      <td>66.274028</td>\n",
              "      <td>456.676340</td>\n",
              "      <td>102.674037</td>\n",
              "      <td>460.338540</td>\n",
              "      <td>82.053286</td>\n",
              "      <td>454.295910</td>\n",
              "      <td>68.096034</td>\n",
              "      <td>132085.986400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.027656</td>\n",
              "      <td>457.775000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>14.667720</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.269372e+01</td>\n",
              "      <td>0.059942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69.035338</td>\n",
              "      <td>130932.615900</td>\n",
              "      <td>104.674992</td>\n",
              "      <td>130857.396100</td>\n",
              "      <td>81.577731</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>102.450263</td>\n",
              "      <td>461.986530</td>\n",
              "      <td>65.091252</td>\n",
              "      <td>465.751648</td>\n",
              "      <td>93.506712</td>\n",
              "      <td>...</td>\n",
              "      <td>105.040353</td>\n",
              "      <td>132325.828100</td>\n",
              "      <td>85.565517</td>\n",
              "      <td>132584.640600</td>\n",
              "      <td>66.849063</td>\n",
              "      <td>454.050049</td>\n",
              "      <td>102.620546</td>\n",
              "      <td>457.489014</td>\n",
              "      <td>82.167877</td>\n",
              "      <td>452.281700</td>\n",
              "      <td>68.321228</td>\n",
              "      <td>132467.359400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.531859</td>\n",
              "      <td>455.348968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.780624</td>\n",
              "      <td>7.992761</td>\n",
              "      <td>6.775070</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.277497e+01</td>\n",
              "      <td>0.057622</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>179.994691</td>\n",
              "      <td>151592.990400</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151567.917200</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151693.283500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1779.462980</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1265.656320</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151618.063700</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>46987.307980</td>\n",
              "      <td>179.467570</td>\n",
              "      <td>17501.142460</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>611.404290</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>723.467610</td>\n",
              "      <td>66.035000</td>\n",
              "      <td>3.720000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.125937</td>\n",
              "      <td>272394.000000</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>145336.656300</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>145167.796900</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>155526.781300</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1416.722070</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1275.910480</td>\n",
              "      <td>179.986276</td>\n",
              "      <td>...</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151749.687500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151924.062500</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1781.324341</td>\n",
              "      <td>179.991768</td>\n",
              "      <td>1266.205650</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1262.726560</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151859.328100</td>\n",
              "      <td>179.964297</td>\n",
              "      <td>45946.320310</td>\n",
              "      <td>179.914860</td>\n",
              "      <td>17351.042970</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1264.740770</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>610.038757</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>715.827942</td>\n",
              "      <td>62.226000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.106656</td>\n",
              "      <td>270336.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R1-PA1:VH       R1-PM1:V  ...    snort_log4  marker\n",
              "count   78377.000000   78377.000000  ...  78377.000000   78377\n",
              "unique           NaN            NaN  ...           NaN       2\n",
              "top              NaN            NaN  ...           NaN  Attack\n",
              "freq             NaN            NaN  ...           NaN   55663\n",
              "mean      -15.802424  130764.039577  ...      0.000077     NaN\n",
              "std       100.876750    8546.118477  ...      0.008749     NaN\n",
              "min      -179.988962       0.000000  ...      0.000000     NaN\n",
              "25%      -100.416583  131057.982300  ...      0.000000     NaN\n",
              "50%       -28.865614  131684.814000  ...      0.000000     NaN\n",
              "75%        68.096034  132186.279400  ...      0.000000     NaN\n",
              "max       179.994691  151592.990400  ...      1.000000     NaN\n",
              "\n",
              "[11 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw1FZpSJA9I_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "outputId": "f18bbe91-144c-441a-8e95-8375e39edff4"
      },
      "source": [
        "df.head(1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-PA1:VH</th>\n",
              "      <th>R1-PM1:V</th>\n",
              "      <th>R1-PA2:VH</th>\n",
              "      <th>R1-PM2:V</th>\n",
              "      <th>R1-PA3:VH</th>\n",
              "      <th>R1-PM3:V</th>\n",
              "      <th>R1-PA4:IH</th>\n",
              "      <th>R1-PM4:I</th>\n",
              "      <th>R1-PA5:IH</th>\n",
              "      <th>R1-PM5:I</th>\n",
              "      <th>R1-PA6:IH</th>\n",
              "      <th>R1-PM6:I</th>\n",
              "      <th>R1-PA7:VH</th>\n",
              "      <th>R1-PM7:V</th>\n",
              "      <th>R1-PA8:VH</th>\n",
              "      <th>R1-PM8:V</th>\n",
              "      <th>R1-PA9:VH</th>\n",
              "      <th>R1-PM9:V</th>\n",
              "      <th>R1-PA10:IH</th>\n",
              "      <th>R1-PM10:I</th>\n",
              "      <th>R1-PA11:IH</th>\n",
              "      <th>R1-PM11:I</th>\n",
              "      <th>R1-PA12:IH</th>\n",
              "      <th>R1-PM12:I</th>\n",
              "      <th>R1:F</th>\n",
              "      <th>R1:DF</th>\n",
              "      <th>R1-PA:Z</th>\n",
              "      <th>R1-PA:ZH</th>\n",
              "      <th>R1:S</th>\n",
              "      <th>R2-PA1:VH</th>\n",
              "      <th>R2-PM1:V</th>\n",
              "      <th>R2-PA2:VH</th>\n",
              "      <th>R2-PM2:V</th>\n",
              "      <th>R2-PA3:VH</th>\n",
              "      <th>R2-PM3:V</th>\n",
              "      <th>R2-PA4:IH</th>\n",
              "      <th>R2-PM4:I</th>\n",
              "      <th>R2-PA5:IH</th>\n",
              "      <th>R2-PM5:I</th>\n",
              "      <th>R2-PA6:IH</th>\n",
              "      <th>...</th>\n",
              "      <th>R4-PA2:VH</th>\n",
              "      <th>R4-PM2:V</th>\n",
              "      <th>R4-PA3:VH</th>\n",
              "      <th>R4-PM3:V</th>\n",
              "      <th>R4-PA4:IH</th>\n",
              "      <th>R4-PM4:I</th>\n",
              "      <th>R4-PA5:IH</th>\n",
              "      <th>R4-PM5:I</th>\n",
              "      <th>R4-PA6:IH</th>\n",
              "      <th>R4-PM6:I</th>\n",
              "      <th>R4-PA7:VH</th>\n",
              "      <th>R4-PM7:V</th>\n",
              "      <th>R4-PA8:VH</th>\n",
              "      <th>R4-PM8:V</th>\n",
              "      <th>R4-PA9:VH</th>\n",
              "      <th>R4-PM9:V</th>\n",
              "      <th>R4-PA10:IH</th>\n",
              "      <th>R4-PM10:I</th>\n",
              "      <th>R4-PA11:IH</th>\n",
              "      <th>R4-PM11:I</th>\n",
              "      <th>R4-PA12:IH</th>\n",
              "      <th>R4-PM12:I</th>\n",
              "      <th>R4:F</th>\n",
              "      <th>R4:DF</th>\n",
              "      <th>R4-PA:Z</th>\n",
              "      <th>R4-PA:ZH</th>\n",
              "      <th>R4:S</th>\n",
              "      <th>control_panel_log1</th>\n",
              "      <th>control_panel_log2</th>\n",
              "      <th>control_panel_log3</th>\n",
              "      <th>control_panel_log4</th>\n",
              "      <th>relay1_log</th>\n",
              "      <th>relay2_log</th>\n",
              "      <th>relay3_log</th>\n",
              "      <th>relay4_log</th>\n",
              "      <th>snort_log1</th>\n",
              "      <th>snort_log2</th>\n",
              "      <th>snort_log3</th>\n",
              "      <th>snort_log4</th>\n",
              "      <th>marker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70.399324</td>\n",
              "      <td>127673.0908</td>\n",
              "      <td>-49.572308</td>\n",
              "      <td>127648.0176</td>\n",
              "      <td>-169.578319</td>\n",
              "      <td>127723.2374</td>\n",
              "      <td>65.689611</td>\n",
              "      <td>605.91099</td>\n",
              "      <td>-57.003571</td>\n",
              "      <td>626.78553</td>\n",
              "      <td>-173.589023</td>\n",
              "      <td>602.43190</td>\n",
              "      <td>70.422243</td>\n",
              "      <td>127673.0908</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.007791</td>\n",
              "      <td>611.58740</td>\n",
              "      <td>118.567886</td>\n",
              "      <td>13.18392</td>\n",
              "      <td>-100.869220</td>\n",
              "      <td>13.91636</td>\n",
              "      <td>59.999</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.391383</td>\n",
              "      <td>0.076290</td>\n",
              "      <td>0</td>\n",
              "      <td>60.658268</td>\n",
              "      <td>124631.8125</td>\n",
              "      <td>-59.295959</td>\n",
              "      <td>124484.3594</td>\n",
              "      <td>-179.338078</td>\n",
              "      <td>124715.0703</td>\n",
              "      <td>-119.550481</td>\n",
              "      <td>612.796753</td>\n",
              "      <td>117.726752</td>\n",
              "      <td>632.532104</td>\n",
              "      <td>0.859680</td>\n",
              "      <td>...</td>\n",
              "      <td>-49.537931</td>\n",
              "      <td>127096.4056</td>\n",
              "      <td>-169.532482</td>\n",
              "      <td>127773.3839</td>\n",
              "      <td>65.643775</td>\n",
              "      <td>604.446110</td>\n",
              "      <td>-56.871791</td>\n",
              "      <td>621.841560</td>\n",
              "      <td>-173.869773</td>\n",
              "      <td>599.868360</td>\n",
              "      <td>70.462350</td>\n",
              "      <td>127522.6512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.950496</td>\n",
              "      <td>608.474530</td>\n",
              "      <td>119.301272</td>\n",
              "      <td>12.26837</td>\n",
              "      <td>-102.060972</td>\n",
              "      <td>11.71904</td>\n",
              "      <td>59.999000</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.341832</td>\n",
              "      <td>0.077897</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Natural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>73.688102</td>\n",
              "      <td>130280.7109</td>\n",
              "      <td>-46.300719</td>\n",
              "      <td>130255.6377</td>\n",
              "      <td>-166.278082</td>\n",
              "      <td>130355.9307</td>\n",
              "      <td>71.831719</td>\n",
              "      <td>483.59351</td>\n",
              "      <td>-50.947407</td>\n",
              "      <td>500.98896</td>\n",
              "      <td>-167.487023</td>\n",
              "      <td>481.39619</td>\n",
              "      <td>73.705291</td>\n",
              "      <td>130305.7842</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.109792</td>\n",
              "      <td>488.35437</td>\n",
              "      <td>125.792884</td>\n",
              "      <td>10.62038</td>\n",
              "      <td>-95.884487</td>\n",
              "      <td>11.35282</td>\n",
              "      <td>60.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.185463</td>\n",
              "      <td>0.024924</td>\n",
              "      <td>0</td>\n",
              "      <td>66.069035</td>\n",
              "      <td>128277.4297</td>\n",
              "      <td>-53.885193</td>\n",
              "      <td>128126.3047</td>\n",
              "      <td>-173.932803</td>\n",
              "      <td>128354.7578</td>\n",
              "      <td>-114.925241</td>\n",
              "      <td>489.349365</td>\n",
              "      <td>122.148740</td>\n",
              "      <td>505.754456</td>\n",
              "      <td>5.468445</td>\n",
              "      <td>...</td>\n",
              "      <td>-46.249153</td>\n",
              "      <td>129704.0257</td>\n",
              "      <td>-166.232245</td>\n",
              "      <td>130381.0040</td>\n",
              "      <td>71.837448</td>\n",
              "      <td>481.762410</td>\n",
              "      <td>-50.792709</td>\n",
              "      <td>496.044990</td>\n",
              "      <td>-167.618803</td>\n",
              "      <td>477.733990</td>\n",
              "      <td>73.756857</td>\n",
              "      <td>130130.2713</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.121251</td>\n",
              "      <td>485.058390</td>\n",
              "      <td>124.274546</td>\n",
              "      <td>10.25416</td>\n",
              "      <td>-95.454769</td>\n",
              "      <td>9.70483</td>\n",
              "      <td>60.005000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.141328</td>\n",
              "      <td>0.027210</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Natural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>73.733939</td>\n",
              "      <td>130305.7842</td>\n",
              "      <td>-46.254883</td>\n",
              "      <td>130280.7109</td>\n",
              "      <td>-166.232245</td>\n",
              "      <td>130381.0040</td>\n",
              "      <td>71.808800</td>\n",
              "      <td>483.59351</td>\n",
              "      <td>-50.913030</td>\n",
              "      <td>500.98896</td>\n",
              "      <td>-167.441186</td>\n",
              "      <td>481.02997</td>\n",
              "      <td>73.751127</td>\n",
              "      <td>130330.8575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.132710</td>\n",
              "      <td>488.35437</td>\n",
              "      <td>125.007932</td>\n",
              "      <td>10.62038</td>\n",
              "      <td>-94.520847</td>\n",
              "      <td>11.35282</td>\n",
              "      <td>60.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.190006</td>\n",
              "      <td>0.027904</td>\n",
              "      <td>0</td>\n",
              "      <td>66.112980</td>\n",
              "      <td>128283.9453</td>\n",
              "      <td>-53.852234</td>\n",
              "      <td>128144.4688</td>\n",
              "      <td>-173.883366</td>\n",
              "      <td>128382.9609</td>\n",
              "      <td>-114.919736</td>\n",
              "      <td>488.885864</td>\n",
              "      <td>122.214665</td>\n",
              "      <td>505.531311</td>\n",
              "      <td>5.462952</td>\n",
              "      <td>...</td>\n",
              "      <td>-46.197587</td>\n",
              "      <td>129729.0990</td>\n",
              "      <td>-166.192138</td>\n",
              "      <td>130381.0040</td>\n",
              "      <td>71.866096</td>\n",
              "      <td>481.396190</td>\n",
              "      <td>-50.781249</td>\n",
              "      <td>496.411210</td>\n",
              "      <td>-167.590155</td>\n",
              "      <td>478.100210</td>\n",
              "      <td>73.796964</td>\n",
              "      <td>130155.3446</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.144169</td>\n",
              "      <td>485.241500</td>\n",
              "      <td>125.277222</td>\n",
              "      <td>10.25416</td>\n",
              "      <td>-95.970431</td>\n",
              "      <td>10.07105</td>\n",
              "      <td>60.005000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.162755</td>\n",
              "      <td>0.026663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Natural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>74.083443</td>\n",
              "      <td>130581.5902</td>\n",
              "      <td>-45.899649</td>\n",
              "      <td>130556.5169</td>\n",
              "      <td>-165.882741</td>\n",
              "      <td>130656.8100</td>\n",
              "      <td>72.152575</td>\n",
              "      <td>482.86107</td>\n",
              "      <td>-50.437475</td>\n",
              "      <td>499.15786</td>\n",
              "      <td>-167.286487</td>\n",
              "      <td>481.39619</td>\n",
              "      <td>74.106361</td>\n",
              "      <td>130581.5902</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.459296</td>\n",
              "      <td>487.62193</td>\n",
              "      <td>127.597701</td>\n",
              "      <td>9.70483</td>\n",
              "      <td>-96.657980</td>\n",
              "      <td>10.43727</td>\n",
              "      <td>60.003</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.171532</td>\n",
              "      <td>0.025617</td>\n",
              "      <td>0</td>\n",
              "      <td>66.508489</td>\n",
              "      <td>128585.4063</td>\n",
              "      <td>-53.445742</td>\n",
              "      <td>128442.6094</td>\n",
              "      <td>-173.487857</td>\n",
              "      <td>128673.4141</td>\n",
              "      <td>-114.815371</td>\n",
              "      <td>489.864349</td>\n",
              "      <td>122.648628</td>\n",
              "      <td>504.226685</td>\n",
              "      <td>5.693665</td>\n",
              "      <td>...</td>\n",
              "      <td>-45.848083</td>\n",
              "      <td>129979.8317</td>\n",
              "      <td>-165.836904</td>\n",
              "      <td>130681.8832</td>\n",
              "      <td>72.181223</td>\n",
              "      <td>481.213080</td>\n",
              "      <td>-50.311424</td>\n",
              "      <td>494.763220</td>\n",
              "      <td>-167.395349</td>\n",
              "      <td>478.283320</td>\n",
              "      <td>74.152198</td>\n",
              "      <td>130431.1505</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.482215</td>\n",
              "      <td>484.692170</td>\n",
              "      <td>126.474704</td>\n",
              "      <td>9.33861</td>\n",
              "      <td>-97.253856</td>\n",
              "      <td>9.15550</td>\n",
              "      <td>60.003000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.135292</td>\n",
              "      <td>0.026595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Natural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74.553268</td>\n",
              "      <td>131083.0556</td>\n",
              "      <td>-45.424094</td>\n",
              "      <td>131057.9823</td>\n",
              "      <td>-165.424375</td>\n",
              "      <td>131158.2754</td>\n",
              "      <td>72.118198</td>\n",
              "      <td>484.50906</td>\n",
              "      <td>-50.013486</td>\n",
              "      <td>497.69298</td>\n",
              "      <td>-167.464104</td>\n",
              "      <td>484.69217</td>\n",
              "      <td>74.570457</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.539510</td>\n",
              "      <td>488.90370</td>\n",
              "      <td>127.666456</td>\n",
              "      <td>7.50751</td>\n",
              "      <td>-99.923839</td>\n",
              "      <td>8.60617</td>\n",
              "      <td>60.001</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.079496</td>\n",
              "      <td>0.032941</td>\n",
              "      <td>0</td>\n",
              "      <td>67.035827</td>\n",
              "      <td>129106.9063</td>\n",
              "      <td>-52.912905</td>\n",
              "      <td>128974.5859</td>\n",
              "      <td>-172.949529</td>\n",
              "      <td>129196.8125</td>\n",
              "      <td>-114.919736</td>\n",
              "      <td>491.294861</td>\n",
              "      <td>123.071594</td>\n",
              "      <td>502.498627</td>\n",
              "      <td>5.413513</td>\n",
              "      <td>...</td>\n",
              "      <td>-45.372528</td>\n",
              "      <td>130506.3704</td>\n",
              "      <td>-165.372808</td>\n",
              "      <td>131183.3486</td>\n",
              "      <td>72.164034</td>\n",
              "      <td>482.861070</td>\n",
              "      <td>-49.881706</td>\n",
              "      <td>493.115230</td>\n",
              "      <td>-167.601614</td>\n",
              "      <td>481.213080</td>\n",
              "      <td>74.627753</td>\n",
              "      <td>130932.6159</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.556699</td>\n",
              "      <td>485.607720</td>\n",
              "      <td>126.595025</td>\n",
              "      <td>7.32440</td>\n",
              "      <td>-101.711468</td>\n",
              "      <td>7.14129</td>\n",
              "      <td>60.001000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.036023</td>\n",
              "      <td>0.033641</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Natural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>-86.149934</td>\n",
              "      <td>131057.9823</td>\n",
              "      <td>153.862086</td>\n",
              "      <td>130431.1505</td>\n",
              "      <td>33.873265</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>-90.756515</td>\n",
              "      <td>480.66375</td>\n",
              "      <td>149.261235</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>29.203659</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>-86.138475</td>\n",
              "      <td>130857.3961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.762244</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.739302</td>\n",
              "      <td>0.080596</td>\n",
              "      <td>0</td>\n",
              "      <td>-93.506712</td>\n",
              "      <td>128701.0949</td>\n",
              "      <td>146.533956</td>\n",
              "      <td>128676.0216</td>\n",
              "      <td>26.533675</td>\n",
              "      <td>128701.0949</td>\n",
              "      <td>81.892858</td>\n",
              "      <td>486.157050</td>\n",
              "      <td>-38.015750</td>\n",
              "      <td>485.973940</td>\n",
              "      <td>-158.142081</td>\n",
              "      <td>...</td>\n",
              "      <td>153.778390</td>\n",
              "      <td>131320.7969</td>\n",
              "      <td>33.725282</td>\n",
              "      <td>131550.4219</td>\n",
              "      <td>-90.683903</td>\n",
              "      <td>477.550507</td>\n",
              "      <td>149.389351</td>\n",
              "      <td>477.888123</td>\n",
              "      <td>29.281311</td>\n",
              "      <td>478.197083</td>\n",
              "      <td>-86.261901</td>\n",
              "      <td>131446.2656</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.667421</td>\n",
              "      <td>477.876648</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.818933</td>\n",
              "      <td>0.076678</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>-86.207230</td>\n",
              "      <td>131032.9090</td>\n",
              "      <td>153.804790</td>\n",
              "      <td>130406.0773</td>\n",
              "      <td>33.821699</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>-90.802351</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>149.232587</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>29.146363</td>\n",
              "      <td>480.29753</td>\n",
              "      <td>-86.190041</td>\n",
              "      <td>130857.3961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.808081</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.739979</td>\n",
              "      <td>0.080800</td>\n",
              "      <td>0</td>\n",
              "      <td>-93.564008</td>\n",
              "      <td>128676.0216</td>\n",
              "      <td>146.482390</td>\n",
              "      <td>128676.0216</td>\n",
              "      <td>26.487839</td>\n",
              "      <td>128701.0949</td>\n",
              "      <td>81.864210</td>\n",
              "      <td>486.157050</td>\n",
              "      <td>-38.055857</td>\n",
              "      <td>485.790830</td>\n",
              "      <td>-158.182188</td>\n",
              "      <td>...</td>\n",
              "      <td>153.728953</td>\n",
              "      <td>131322.4063</td>\n",
              "      <td>33.670350</td>\n",
              "      <td>131544.4531</td>\n",
              "      <td>-90.760804</td>\n",
              "      <td>477.458954</td>\n",
              "      <td>149.361880</td>\n",
              "      <td>478.157043</td>\n",
              "      <td>29.242861</td>\n",
              "      <td>478.569031</td>\n",
              "      <td>-86.316837</td>\n",
              "      <td>131443.5625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.716858</td>\n",
              "      <td>478.059753</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.826095</td>\n",
              "      <td>0.077464</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>-86.230148</td>\n",
              "      <td>131057.9823</td>\n",
              "      <td>153.781872</td>\n",
              "      <td>130431.1505</td>\n",
              "      <td>33.804510</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>-90.848188</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>149.221128</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>29.134904</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>-86.218689</td>\n",
              "      <td>130857.3961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.830999</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>59.999</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.743356</td>\n",
              "      <td>0.081380</td>\n",
              "      <td>0</td>\n",
              "      <td>-93.586926</td>\n",
              "      <td>128701.0949</td>\n",
              "      <td>146.459472</td>\n",
              "      <td>128650.9484</td>\n",
              "      <td>26.459191</td>\n",
              "      <td>128726.1682</td>\n",
              "      <td>81.852751</td>\n",
              "      <td>485.973940</td>\n",
              "      <td>-38.090234</td>\n",
              "      <td>486.157050</td>\n",
              "      <td>-158.216566</td>\n",
              "      <td>...</td>\n",
              "      <td>153.706973</td>\n",
              "      <td>131322.7500</td>\n",
              "      <td>33.648377</td>\n",
              "      <td>131555.3125</td>\n",
              "      <td>-90.788275</td>\n",
              "      <td>477.796570</td>\n",
              "      <td>149.312443</td>\n",
              "      <td>477.693542</td>\n",
              "      <td>29.187929</td>\n",
              "      <td>478.357300</td>\n",
              "      <td>-86.338809</td>\n",
              "      <td>131447.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.760804</td>\n",
              "      <td>477.939606</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.808361</td>\n",
              "      <td>0.077215</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>-86.264526</td>\n",
              "      <td>131057.9823</td>\n",
              "      <td>153.758954</td>\n",
              "      <td>130406.0773</td>\n",
              "      <td>33.764403</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>-90.871106</td>\n",
              "      <td>480.66375</td>\n",
              "      <td>149.146644</td>\n",
              "      <td>480.29753</td>\n",
              "      <td>29.100526</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>-86.253066</td>\n",
              "      <td>130857.3961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.871106</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>59.999</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.738667</td>\n",
              "      <td>0.080399</td>\n",
              "      <td>0</td>\n",
              "      <td>-93.609845</td>\n",
              "      <td>128701.0949</td>\n",
              "      <td>146.430824</td>\n",
              "      <td>128676.0216</td>\n",
              "      <td>26.430543</td>\n",
              "      <td>128726.1682</td>\n",
              "      <td>81.801184</td>\n",
              "      <td>486.157050</td>\n",
              "      <td>-38.124612</td>\n",
              "      <td>485.790830</td>\n",
              "      <td>-158.239484</td>\n",
              "      <td>...</td>\n",
              "      <td>153.674011</td>\n",
              "      <td>131315.2656</td>\n",
              "      <td>33.620913</td>\n",
              "      <td>131558.7500</td>\n",
              "      <td>-90.848694</td>\n",
              "      <td>477.773682</td>\n",
              "      <td>149.279481</td>\n",
              "      <td>477.819458</td>\n",
              "      <td>29.154970</td>\n",
              "      <td>478.214264</td>\n",
              "      <td>-86.371765</td>\n",
              "      <td>131445.2813</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.804749</td>\n",
              "      <td>477.933899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>59.999001</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.811111</td>\n",
              "      <td>0.078534</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>-86.327551</td>\n",
              "      <td>131057.9823</td>\n",
              "      <td>153.684469</td>\n",
              "      <td>130406.0773</td>\n",
              "      <td>33.695648</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>-90.951320</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>149.100807</td>\n",
              "      <td>480.66375</td>\n",
              "      <td>29.037501</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>-86.316092</td>\n",
              "      <td>130857.3961</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.934132</td>\n",
              "      <td>480.48064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.747523</td>\n",
              "      <td>0.081526</td>\n",
              "      <td>0</td>\n",
              "      <td>-93.678600</td>\n",
              "      <td>128701.0949</td>\n",
              "      <td>146.362069</td>\n",
              "      <td>128676.0216</td>\n",
              "      <td>26.361788</td>\n",
              "      <td>128726.1682</td>\n",
              "      <td>81.732429</td>\n",
              "      <td>485.973940</td>\n",
              "      <td>-38.193367</td>\n",
              "      <td>485.973940</td>\n",
              "      <td>-158.319698</td>\n",
              "      <td>...</td>\n",
              "      <td>153.613591</td>\n",
              "      <td>131322.5625</td>\n",
              "      <td>33.554995</td>\n",
              "      <td>131558.0625</td>\n",
              "      <td>-90.920104</td>\n",
              "      <td>477.682129</td>\n",
              "      <td>149.202586</td>\n",
              "      <td>477.762207</td>\n",
              "      <td>29.111024</td>\n",
              "      <td>478.345886</td>\n",
              "      <td>-86.432192</td>\n",
              "      <td>131446.8438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-90.865176</td>\n",
              "      <td>477.928162</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.817507</td>\n",
              "      <td>0.078274</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     R1-PA1:VH     R1-PM1:V   R1-PA2:VH  ...  snort_log3  snort_log4   marker\n",
              "0    70.399324  127673.0908  -49.572308  ...           0           0  Natural\n",
              "1    73.688102  130280.7109  -46.300719  ...           0           0  Natural\n",
              "2    73.733939  130305.7842  -46.254883  ...           0           0  Natural\n",
              "3    74.083443  130581.5902  -45.899649  ...           0           0  Natural\n",
              "4    74.553268  131083.0556  -45.424094  ...           0           0  Natural\n",
              "..         ...          ...         ...  ...         ...         ...      ...\n",
              "995 -86.149934  131057.9823  153.862086  ...           0           0   Attack\n",
              "996 -86.207230  131032.9090  153.804790  ...           0           0   Attack\n",
              "997 -86.230148  131057.9823  153.781872  ...           0           0   Attack\n",
              "998 -86.264526  131057.9823  153.758954  ...           0           0   Attack\n",
              "999 -86.327551  131057.9823  153.684469  ...           0           0   Attack\n",
              "\n",
              "[1000 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5fa0kSIQ_Hd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b47bd8f5-1907-44fc-c6a9-4af1b6c9a776"
      },
      "source": [
        "#معرفة عدد أسطر و أعمدة مجموعة البيانات\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC2e75MwBUgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#تحويل الكلمات إلى أرقام للعمود marker\n",
        "#target equal 0 is natural\n",
        "#target equal 1 is Attack\n",
        "df.loc[df[\"marker\"] == \"Natural\", \"marker\"] = 0\n",
        "df.loc[df[\"marker\"] ==\"Attack\", \"marker\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn_HugvVfUuV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1a1534b-4f86-445b-dca2-dc216ea4439b"
      },
      "source": [
        "#سيعطي هذا مجموعة من الأماكن حيث توجد قيم NA.\n",
        " \n",
        "df[df==np.inf]=np.nan\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#ذا كانت بياناتك تحتوي على Nan ، فجرّب ما يلي:\n",
        "np.isnan(df.values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NRf7LFi1vdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop label for classification and feature low correllation\n",
        "X= df.drop(\"marker\", axis = 1)\n",
        "X=X.astype(float)\n",
        " \n",
        " \n",
        "from sklearn.preprocessing import Normalizer\n",
        "transformer= Normalizer().fit(X) \n",
        "transformer\n",
        "S=transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtvEOKIOHEHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "7b759bc9-f607-46a4-a687-ea6bd432b230"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        " \n",
        "x = pd.DataFrame(ss.fit_transform(S))\n",
        " \n",
        "y = df[\"marker\"]\n",
        " \n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.874304</td>\n",
              "      <td>0.099036</td>\n",
              "      <td>-0.474731</td>\n",
              "      <td>0.061580</td>\n",
              "      <td>-1.867958</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.824335</td>\n",
              "      <td>1.018163</td>\n",
              "      <td>-0.569834</td>\n",
              "      <td>1.564347</td>\n",
              "      <td>-1.946451</td>\n",
              "      <td>1.417422</td>\n",
              "      <td>0.874513</td>\n",
              "      <td>0.069631</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.819024</td>\n",
              "      <td>1.415316</td>\n",
              "      <td>1.806088</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-1.503010</td>\n",
              "      <td>-0.004960</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100647</td>\n",
              "      <td>-0.023363</td>\n",
              "      <td>0.205864</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.763126</td>\n",
              "      <td>0.129576</td>\n",
              "      <td>-0.593351</td>\n",
              "      <td>0.052249</td>\n",
              "      <td>-1.979122</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>-1.394714</td>\n",
              "      <td>1.193198</td>\n",
              "      <td>1.353806</td>\n",
              "      <td>1.556328</td>\n",
              "      <td>0.066997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063134</td>\n",
              "      <td>-0.412118</td>\n",
              "      <td>-0.061167</td>\n",
              "      <td>-1.434133</td>\n",
              "      <td>-0.019137</td>\n",
              "      <td>0.620453</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>-0.567535</td>\n",
              "      <td>0.131541</td>\n",
              "      <td>-1.941687</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.869845</td>\n",
              "      <td>-0.002144</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.814642</td>\n",
              "      <td>1.448899</td>\n",
              "      <td>1.834972</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>-1.538149</td>\n",
              "      <td>0.036196</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100102</td>\n",
              "      <td>-0.022051</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.889684</td>\n",
              "      <td>0.036331</td>\n",
              "      <td>-0.434350</td>\n",
              "      <td>-0.014128</td>\n",
              "      <td>-1.790412</td>\n",
              "      <td>-0.049095</td>\n",
              "      <td>0.869987</td>\n",
              "      <td>0.398801</td>\n",
              "      <td>-0.501111</td>\n",
              "      <td>0.701235</td>\n",
              "      <td>-1.836419</td>\n",
              "      <td>0.602633</td>\n",
              "      <td>0.889830</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864449</td>\n",
              "      <td>0.601895</td>\n",
              "      <td>1.868931</td>\n",
              "      <td>-0.007345</td>\n",
              "      <td>-1.400215</td>\n",
              "      <td>-0.008872</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020642</td>\n",
              "      <td>0.008042</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.801732</td>\n",
              "      <td>0.166731</td>\n",
              "      <td>-0.531031</td>\n",
              "      <td>0.101154</td>\n",
              "      <td>-1.876163</td>\n",
              "      <td>0.083426</td>\n",
              "      <td>-1.318168</td>\n",
              "      <td>0.481710</td>\n",
              "      <td>1.369793</td>\n",
              "      <td>0.693499</td>\n",
              "      <td>0.109440</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.377415</td>\n",
              "      <td>-0.134235</td>\n",
              "      <td>-1.374928</td>\n",
              "      <td>-0.094753</td>\n",
              "      <td>0.655781</td>\n",
              "      <td>0.075937</td>\n",
              "      <td>-0.498681</td>\n",
              "      <td>0.058612</td>\n",
              "      <td>-1.830469</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>0.885227</td>\n",
              "      <td>-0.079381</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860780</td>\n",
              "      <td>0.614187</td>\n",
              "      <td>1.864821</td>\n",
              "      <td>-0.011410</td>\n",
              "      <td>-1.411007</td>\n",
              "      <td>-0.005342</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019336</td>\n",
              "      <td>0.008870</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.890043</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.433884</td>\n",
              "      <td>-0.013045</td>\n",
              "      <td>-1.789712</td>\n",
              "      <td>-0.048005</td>\n",
              "      <td>0.869660</td>\n",
              "      <td>0.398513</td>\n",
              "      <td>-0.500733</td>\n",
              "      <td>0.700832</td>\n",
              "      <td>-1.835701</td>\n",
              "      <td>0.600059</td>\n",
              "      <td>0.890189</td>\n",
              "      <td>-0.002797</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864586</td>\n",
              "      <td>0.601516</td>\n",
              "      <td>1.857189</td>\n",
              "      <td>-0.007370</td>\n",
              "      <td>-1.381310</td>\n",
              "      <td>-0.008874</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020636</td>\n",
              "      <td>0.019097</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.802080</td>\n",
              "      <td>0.166042</td>\n",
              "      <td>-0.530669</td>\n",
              "      <td>0.101267</td>\n",
              "      <td>-1.875407</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>-1.317960</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>1.370322</td>\n",
              "      <td>0.691744</td>\n",
              "      <td>0.109383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>-0.376972</td>\n",
              "      <td>-0.133153</td>\n",
              "      <td>-1.374439</td>\n",
              "      <td>-0.097088</td>\n",
              "      <td>0.655930</td>\n",
              "      <td>0.075577</td>\n",
              "      <td>-0.498514</td>\n",
              "      <td>0.058768</td>\n",
              "      <td>-1.829935</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.885529</td>\n",
              "      <td>-0.078267</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860917</td>\n",
              "      <td>0.614905</td>\n",
              "      <td>1.879398</td>\n",
              "      <td>-0.011434</td>\n",
              "      <td>-1.418009</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019302</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.891902</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.429791</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-1.782334</td>\n",
              "      <td>-0.049518</td>\n",
              "      <td>0.871534</td>\n",
              "      <td>0.390431</td>\n",
              "      <td>-0.495338</td>\n",
              "      <td>0.682959</td>\n",
              "      <td>-1.830180</td>\n",
              "      <td>0.595921</td>\n",
              "      <td>0.892105</td>\n",
              "      <td>-0.007788</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.866303</td>\n",
              "      <td>0.590921</td>\n",
              "      <td>1.891025</td>\n",
              "      <td>-0.023903</td>\n",
              "      <td>-1.407753</td>\n",
              "      <td>-0.010136</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>0.010396</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.804544</td>\n",
              "      <td>0.167318</td>\n",
              "      <td>-0.525924</td>\n",
              "      <td>0.102684</td>\n",
              "      <td>-1.867290</td>\n",
              "      <td>0.085049</td>\n",
              "      <td>-1.314364</td>\n",
              "      <td>0.478504</td>\n",
              "      <td>1.372069</td>\n",
              "      <td>0.677139</td>\n",
              "      <td>0.111401</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>-0.373513</td>\n",
              "      <td>-0.137835</td>\n",
              "      <td>-1.368761</td>\n",
              "      <td>-0.095179</td>\n",
              "      <td>0.657141</td>\n",
              "      <td>0.074537</td>\n",
              "      <td>-0.493178</td>\n",
              "      <td>0.057356</td>\n",
              "      <td>-1.824012</td>\n",
              "      <td>0.047841</td>\n",
              "      <td>0.887439</td>\n",
              "      <td>-0.079738</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.862749</td>\n",
              "      <td>0.605183</td>\n",
              "      <td>1.892994</td>\n",
              "      <td>-0.027845</td>\n",
              "      <td>-1.432902</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019377</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893597</td>\n",
              "      <td>0.033239</td>\n",
              "      <td>-0.423901</td>\n",
              "      <td>-0.017844</td>\n",
              "      <td>-1.770758</td>\n",
              "      <td>-0.052923</td>\n",
              "      <td>0.868269</td>\n",
              "      <td>0.389099</td>\n",
              "      <td>-0.489603</td>\n",
              "      <td>0.661865</td>\n",
              "      <td>-1.824963</td>\n",
              "      <td>0.604001</td>\n",
              "      <td>0.893742</td>\n",
              "      <td>-0.007717</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864216</td>\n",
              "      <td>0.586934</td>\n",
              "      <td>1.884543</td>\n",
              "      <td>-0.063127</td>\n",
              "      <td>-1.447021</td>\n",
              "      <td>-0.012640</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020903</td>\n",
              "      <td>0.037047</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.807090</td>\n",
              "      <td>0.167550</td>\n",
              "      <td>-0.519189</td>\n",
              "      <td>0.103949</td>\n",
              "      <td>-1.854435</td>\n",
              "      <td>0.085472</td>\n",
              "      <td>-1.310773</td>\n",
              "      <td>0.475735</td>\n",
              "      <td>1.371320</td>\n",
              "      <td>0.654512</td>\n",
              "      <td>0.108615</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003178</td>\n",
              "      <td>-0.368471</td>\n",
              "      <td>-0.137439</td>\n",
              "      <td>-1.359878</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>0.654775</td>\n",
              "      <td>0.074295</td>\n",
              "      <td>-0.487400</td>\n",
              "      <td>0.055487</td>\n",
              "      <td>-1.819112</td>\n",
              "      <td>0.048351</td>\n",
              "      <td>0.889183</td>\n",
              "      <td>-0.083081</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860604</td>\n",
              "      <td>0.598972</td>\n",
              "      <td>1.887278</td>\n",
              "      <td>-0.063596</td>\n",
              "      <td>-1.489143</td>\n",
              "      <td>-0.052365</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019594</td>\n",
              "      <td>0.009072</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       125      126       127\n",
              "0  0.874304  0.099036 -0.474731  ... -0.007144 -0.00943 -0.008728\n",
              "1  0.889684  0.036331 -0.434350  ... -0.007144 -0.00943 -0.008728\n",
              "2  0.890043  0.037227 -0.433884  ... -0.007144 -0.00943 -0.008728\n",
              "3  0.891902  0.036003 -0.429791  ... -0.007144 -0.00943 -0.008728\n",
              "4  0.893597  0.033239 -0.423901  ... -0.007144 -0.00943 -0.008728\n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE5YdeYrJhBM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dc225e2e-11fc-47af-f4be-9c2904ad028c"
      },
      "source": [
        "# categorical target y to array \n",
        "from keras.utils.np_utils import to_categorical\n",
        " \n",
        "y_cat = to_categorical(y,2)\n",
        "y_cat[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW_WMm8iJ1RU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e7064598-14cb-4c11-d3a9-7d9d3b9b5088"
      },
      "source": [
        "print('y.shape : ',y.shape)\n",
        "print('\\n')\n",
        "print('y_cat.shape :',y_cat.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y.shape :  (78377,)\n",
            "\n",
            "\n",
            "y_cat.shape : (78377, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovKsp96bJ5Mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b9f3ef03-2f36-42fe-cbd7-ecf4876f9f2a"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ehIE1byJ-OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#divide datasets into to part training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "x_train, x_test, y_train, y_test = train_test_split(x.values, y_cat,test_size=0.1,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv_sJPjYhK6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshape x_train,x_test,y_train and y_test\n",
        "X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
        "X_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
        "Y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1]))\n",
        "Y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx16wWySpefP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0OF-jTz-G8Y",
        "colab_type": "text"
      },
      "source": [
        "RESHAPE FORM  DATASET "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfWrn8JMLJup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshape x_train,x_test,y_train and y_test\n",
        "X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
        "X_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
        "Y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1]))\n",
        "Y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT0Uph_KFu60",
        "colab_type": "text"
      },
      "source": [
        "CNN MODEL TF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOLsnOcuRHMH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "c6ac4e33-c0de-4bbe-e08a-988d2b3bb430"
      },
      "source": [
        "from tensorflow.keras import layers, models\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        " \n",
        "i=x_train.shape[1]\n",
        "model = models.Sequential()\n",
        "#conv1\n",
        " \n",
        "model.add(layers.Conv1D(256,\n",
        "        kernel_size=7,\n",
        "        input_shape=(1, i),\n",
        "        padding=\"same\",activation='tanh'\n",
        "        \n",
        "))\n",
        "model.add(layers.BatchNormalization())\n",
        " \n",
        "#conv2\n",
        " \n",
        "model.add(layers.Conv1D(256,\n",
        "        kernel_size=5,\n",
        "        padding=\"same\"\n",
        "        ,activation='tanh'\n",
        "))\n",
        "model.add(layers.BatchNormalization())\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "#conv3\n",
        " \n",
        "model.add(layers.Conv1D(128,\n",
        "        kernel_size=5,\n",
        "        padding=\"same\"  \n",
        "        ,activation='tanh'\n",
        "))\n",
        "model.add(layers.BatchNormalization())\n",
        " \n",
        " \n",
        "model.add(layers.Conv1D(256,\n",
        "        kernel_size=7,\n",
        "        padding=\"same\"  \n",
        "        ,activation='tanh'\n",
        "))\n",
        "model.add(layers.BatchNormalization())\n",
        " \n",
        " \n",
        " \n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        " \n",
        "model.add(layers.Dense(512, activation='tanh',kernel_initializer='he_uniform'))\n",
        "model.add(layers.Dense(2, activation='sigmoid',kernel_initializer='he_uniform'))\n",
        " \n",
        " \n",
        " \n",
        "optimizer = tf.keras.optimizers.Adam(0.001,decay=0.000005)\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=optimizer,metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 1, 256)            229632    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 1, 256)            1024      \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1, 256)            327936    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 1, 256)            1024      \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1, 128)            163968    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 128)            512       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1, 256)            229632    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1, 256)            1024      \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 1,087,362\n",
            "Trainable params: 1,085,570\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ippZzvgNTSar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "48fdb412-fde9-486c-8c22-b0dc77f15206"
      },
      "source": [
        "!pip install h5py\n",
        "import time\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "# patient early stopping\n",
        " \n",
        "#reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_cnn1d_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "rp = ReduceLROnPlateau(monitor='val_accuracy',patience = 5,verbose=1,factor=0.5,min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DMXyR_wV11R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4dee332-e162-400b-8437-c1f43387e281"
      },
      "source": [
        "start_time = time.time()\n",
        "fit1 = model.fit(X_train,y_train, epochs=250,batch_size=500,verbose=1,callbacks=[es,mc,rp],validation_split=0.1)\n",
        "score=model.evaluate(X_test,y_test)\n",
        "print('Accuracy(on Test-data): ' + str(score[1]))\n",
        "duration = time.time() - start_time\n",
        "print(\"time of training (s)\", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.6050 - accuracy: 0.7024\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71208, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 10ms/step - loss: 0.6050 - accuracy: 0.7024 - val_loss: 0.5985 - val_accuracy: 0.7121\n",
            "Epoch 2/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.5746 - accuracy: 0.7167\n",
            "Epoch 00002: val_accuracy improved from 0.71208 to 0.71335, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.5745 - accuracy: 0.7163 - val_loss: 0.5673 - val_accuracy: 0.7134\n",
            "Epoch 3/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.5578 - accuracy: 0.7195\n",
            "Epoch 00003: val_accuracy improved from 0.71335 to 0.71775, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.5579 - accuracy: 0.7191 - val_loss: 0.5581 - val_accuracy: 0.7177\n",
            "Epoch 4/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5400 - accuracy: 0.7215\n",
            "Epoch 00004: val_accuracy improved from 0.71775 to 0.72484, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.5400 - accuracy: 0.7215 - val_loss: 0.5261 - val_accuracy: 0.7248\n",
            "Epoch 5/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5277 - accuracy: 0.7241\n",
            "Epoch 00005: val_accuracy did not improve from 0.72484\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.5277 - accuracy: 0.7241 - val_loss: 0.5322 - val_accuracy: 0.7214\n",
            "Epoch 6/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5141 - accuracy: 0.7278\n",
            "Epoch 00006: val_accuracy did not improve from 0.72484\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.5141 - accuracy: 0.7278 - val_loss: 0.5659 - val_accuracy: 0.7168\n",
            "Epoch 7/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.5096 - accuracy: 0.7320\n",
            "Epoch 00007: val_accuracy did not improve from 0.72484\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.5096 - accuracy: 0.7317 - val_loss: 0.5236 - val_accuracy: 0.7179\n",
            "Epoch 8/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.5051 - accuracy: 0.7333\n",
            "Epoch 00008: val_accuracy did not improve from 0.72484\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.5055 - accuracy: 0.7331 - val_loss: 0.5234 - val_accuracy: 0.7248\n",
            "Epoch 9/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.4972 - accuracy: 0.7404\n",
            "Epoch 00009: val_accuracy improved from 0.72484 to 0.73774, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.4970 - accuracy: 0.7404 - val_loss: 0.5024 - val_accuracy: 0.7377\n",
            "Epoch 10/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.4922 - accuracy: 0.7447\n",
            "Epoch 00010: val_accuracy did not improve from 0.73774\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4919 - accuracy: 0.7448 - val_loss: 0.5069 - val_accuracy: 0.7316\n",
            "Epoch 11/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.4880 - accuracy: 0.7465\n",
            "Epoch 00011: val_accuracy did not improve from 0.73774\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4876 - accuracy: 0.7469 - val_loss: 0.5063 - val_accuracy: 0.7328\n",
            "Epoch 12/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4815 - accuracy: 0.7509\n",
            "Epoch 00012: val_accuracy did not improve from 0.73774\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4818 - accuracy: 0.7508 - val_loss: 0.5157 - val_accuracy: 0.7280\n",
            "Epoch 13/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4766 - accuracy: 0.7538\n",
            "Epoch 00013: val_accuracy improved from 0.73774 to 0.73859, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.4762 - accuracy: 0.7541 - val_loss: 0.5150 - val_accuracy: 0.7386\n",
            "Epoch 14/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.4719 - accuracy: 0.7581\n",
            "Epoch 00014: val_accuracy improved from 0.73859 to 0.74242, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4721 - accuracy: 0.7578 - val_loss: 0.4960 - val_accuracy: 0.7424\n",
            "Epoch 15/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.4693 - accuracy: 0.7590\n",
            "Epoch 00015: val_accuracy did not improve from 0.74242\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4690 - accuracy: 0.7595 - val_loss: 0.4901 - val_accuracy: 0.7396\n",
            "Epoch 16/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4610 - accuracy: 0.7642\n",
            "Epoch 00016: val_accuracy improved from 0.74242 to 0.74440, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4613 - accuracy: 0.7639 - val_loss: 0.4957 - val_accuracy: 0.7444\n",
            "Epoch 17/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.4611 - accuracy: 0.7648\n",
            "Epoch 00017: val_accuracy improved from 0.74440 to 0.75191, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.4607 - accuracy: 0.7650 - val_loss: 0.4981 - val_accuracy: 0.7519\n",
            "Epoch 18/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.4531 - accuracy: 0.7693\n",
            "Epoch 00018: val_accuracy improved from 0.75191 to 0.75262, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4523 - accuracy: 0.7699 - val_loss: 0.4765 - val_accuracy: 0.7526\n",
            "Epoch 19/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4469 - accuracy: 0.7728\n",
            "Epoch 00019: val_accuracy did not improve from 0.75262\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4472 - accuracy: 0.7727 - val_loss: 0.5113 - val_accuracy: 0.7448\n",
            "Epoch 20/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.7751\n",
            "Epoch 00020: val_accuracy did not improve from 0.75262\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4434 - accuracy: 0.7752 - val_loss: 0.4820 - val_accuracy: 0.7494\n",
            "Epoch 21/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4368 - accuracy: 0.7779\n",
            "Epoch 00021: val_accuracy improved from 0.75262 to 0.75886, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.4367 - accuracy: 0.7781 - val_loss: 0.4779 - val_accuracy: 0.7589\n",
            "Epoch 22/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4327 - accuracy: 0.7792\n",
            "Epoch 00022: val_accuracy did not improve from 0.75886\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4328 - accuracy: 0.7791 - val_loss: 0.4771 - val_accuracy: 0.7586\n",
            "Epoch 23/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.7826\n",
            "Epoch 00023: val_accuracy did not improve from 0.75886\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4269 - accuracy: 0.7828 - val_loss: 0.4808 - val_accuracy: 0.7540\n",
            "Epoch 24/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.7864\n",
            "Epoch 00024: val_accuracy improved from 0.75886 to 0.76141, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4235 - accuracy: 0.7861 - val_loss: 0.4699 - val_accuracy: 0.7614\n",
            "Epoch 25/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4160 - accuracy: 0.7903\n",
            "Epoch 00025: val_accuracy did not improve from 0.76141\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4161 - accuracy: 0.7903 - val_loss: 0.4687 - val_accuracy: 0.7614\n",
            "Epoch 26/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4121 - accuracy: 0.7928\n",
            "Epoch 00026: val_accuracy did not improve from 0.76141\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4124 - accuracy: 0.7926 - val_loss: 0.4737 - val_accuracy: 0.7607\n",
            "Epoch 27/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4065 - accuracy: 0.7951\n",
            "Epoch 00027: val_accuracy improved from 0.76141 to 0.76467, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.4065 - accuracy: 0.7951 - val_loss: 0.4625 - val_accuracy: 0.7647\n",
            "Epoch 28/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.7996\n",
            "Epoch 00028: val_accuracy improved from 0.76467 to 0.76807, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.3990 - accuracy: 0.7998 - val_loss: 0.4720 - val_accuracy: 0.7681\n",
            "Epoch 29/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3963 - accuracy: 0.8008\n",
            "Epoch 00029: val_accuracy did not improve from 0.76807\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3963 - accuracy: 0.8013 - val_loss: 0.4693 - val_accuracy: 0.7652\n",
            "Epoch 30/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3914 - accuracy: 0.8044\n",
            "Epoch 00030: val_accuracy improved from 0.76807 to 0.77516, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3914 - accuracy: 0.8044 - val_loss: 0.4569 - val_accuracy: 0.7752\n",
            "Epoch 31/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8087\n",
            "Epoch 00031: val_accuracy improved from 0.77516 to 0.77630, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3851 - accuracy: 0.8084 - val_loss: 0.4573 - val_accuracy: 0.7763\n",
            "Epoch 32/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3820 - accuracy: 0.8103\n",
            "Epoch 00032: val_accuracy improved from 0.77630 to 0.77814, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.3820 - accuracy: 0.8103 - val_loss: 0.4600 - val_accuracy: 0.7781\n",
            "Epoch 33/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.3763 - accuracy: 0.8153\n",
            "Epoch 00033: val_accuracy improved from 0.77814 to 0.78452, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.3760 - accuracy: 0.8154 - val_loss: 0.4484 - val_accuracy: 0.7845\n",
            "Epoch 34/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.3687 - accuracy: 0.8189\n",
            "Epoch 00034: val_accuracy did not improve from 0.78452\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3686 - accuracy: 0.8190 - val_loss: 0.4591 - val_accuracy: 0.7770\n",
            "Epoch 35/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.3654 - accuracy: 0.8192\n",
            "Epoch 00035: val_accuracy improved from 0.78452 to 0.78650, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3655 - accuracy: 0.8190 - val_loss: 0.4419 - val_accuracy: 0.7865\n",
            "Epoch 36/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.3607 - accuracy: 0.8223\n",
            "Epoch 00036: val_accuracy did not improve from 0.78650\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3612 - accuracy: 0.8218 - val_loss: 0.4474 - val_accuracy: 0.7788\n",
            "Epoch 37/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8252\n",
            "Epoch 00037: val_accuracy improved from 0.78650 to 0.79643, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.3554 - accuracy: 0.8251 - val_loss: 0.4357 - val_accuracy: 0.7964\n",
            "Epoch 38/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3452 - accuracy: 0.8323\n",
            "Epoch 00038: val_accuracy did not improve from 0.79643\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3456 - accuracy: 0.8321 - val_loss: 0.4469 - val_accuracy: 0.7859\n",
            "Epoch 39/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3465 - accuracy: 0.8299\n",
            "Epoch 00039: val_accuracy did not improve from 0.79643\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3469 - accuracy: 0.8296 - val_loss: 0.4391 - val_accuracy: 0.7961\n",
            "Epoch 40/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3406 - accuracy: 0.8339\n",
            "Epoch 00040: val_accuracy did not improve from 0.79643\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3407 - accuracy: 0.8340 - val_loss: 0.4327 - val_accuracy: 0.7961\n",
            "Epoch 41/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3370 - accuracy: 0.8369\n",
            "Epoch 00041: val_accuracy did not improve from 0.79643\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3373 - accuracy: 0.8367 - val_loss: 0.4369 - val_accuracy: 0.7935\n",
            "Epoch 42/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.3347 - accuracy: 0.8376\n",
            "Epoch 00042: val_accuracy improved from 0.79643 to 0.80550, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3353 - accuracy: 0.8374 - val_loss: 0.4255 - val_accuracy: 0.8055\n",
            "Epoch 43/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3300 - accuracy: 0.8419\n",
            "Epoch 00043: val_accuracy did not improve from 0.80550\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3299 - accuracy: 0.8420 - val_loss: 0.4288 - val_accuracy: 0.8048\n",
            "Epoch 44/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8444\n",
            "Epoch 00044: val_accuracy did not improve from 0.80550\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3234 - accuracy: 0.8444 - val_loss: 0.4359 - val_accuracy: 0.8039\n",
            "Epoch 45/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3243 - accuracy: 0.8446\n",
            "Epoch 00045: val_accuracy did not improve from 0.80550\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3244 - accuracy: 0.8445 - val_loss: 0.4362 - val_accuracy: 0.7973\n",
            "Epoch 46/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.3142 - accuracy: 0.8491\n",
            "Epoch 00046: val_accuracy did not improve from 0.80550\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.3153 - accuracy: 0.8486 - val_loss: 0.4354 - val_accuracy: 0.8007\n",
            "Epoch 47/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3083 - accuracy: 0.8524\n",
            "Epoch 00047: val_accuracy did not improve from 0.80550\n",
            "\n",
            "Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.3083 - accuracy: 0.8524 - val_loss: 0.4242 - val_accuracy: 0.8035\n",
            "Epoch 48/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2767 - accuracy: 0.8718\n",
            "Epoch 00048: val_accuracy improved from 0.80550 to 0.83017, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2759 - accuracy: 0.8724 - val_loss: 0.3992 - val_accuracy: 0.8302\n",
            "Epoch 49/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.8757\n",
            "Epoch 00049: val_accuracy did not improve from 0.83017\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2686 - accuracy: 0.8757 - val_loss: 0.4009 - val_accuracy: 0.8290\n",
            "Epoch 50/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2631 - accuracy: 0.8793\n",
            "Epoch 00050: val_accuracy did not improve from 0.83017\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2646 - accuracy: 0.8786 - val_loss: 0.4011 - val_accuracy: 0.8255\n",
            "Epoch 51/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.8799\n",
            "Epoch 00051: val_accuracy improved from 0.83017 to 0.83144, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2612 - accuracy: 0.8799 - val_loss: 0.4043 - val_accuracy: 0.8314\n",
            "Epoch 52/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.8803\n",
            "Epoch 00052: val_accuracy did not improve from 0.83144\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2596 - accuracy: 0.8803 - val_loss: 0.4047 - val_accuracy: 0.8313\n",
            "Epoch 53/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.8849\n",
            "Epoch 00053: val_accuracy improved from 0.83144 to 0.83314, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.2529 - accuracy: 0.8847 - val_loss: 0.4048 - val_accuracy: 0.8331\n",
            "Epoch 54/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.8847\n",
            "Epoch 00054: val_accuracy improved from 0.83314 to 0.83357, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.2523 - accuracy: 0.8847 - val_loss: 0.4019 - val_accuracy: 0.8336\n",
            "Epoch 55/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2482 - accuracy: 0.8867\n",
            "Epoch 00055: val_accuracy did not improve from 0.83357\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2480 - accuracy: 0.8869 - val_loss: 0.4095 - val_accuracy: 0.8310\n",
            "Epoch 56/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2449 - accuracy: 0.8891\n",
            "Epoch 00056: val_accuracy improved from 0.83357 to 0.83640, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2446 - accuracy: 0.8894 - val_loss: 0.3953 - val_accuracy: 0.8364\n",
            "Epoch 57/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2416 - accuracy: 0.8889\n",
            "Epoch 00057: val_accuracy improved from 0.83640 to 0.84094, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2413 - accuracy: 0.8891 - val_loss: 0.3954 - val_accuracy: 0.8409\n",
            "Epoch 58/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2399 - accuracy: 0.8909\n",
            "Epoch 00058: val_accuracy did not improve from 0.84094\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2409 - accuracy: 0.8905 - val_loss: 0.4029 - val_accuracy: 0.8329\n",
            "Epoch 59/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2394 - accuracy: 0.8902\n",
            "Epoch 00059: val_accuracy did not improve from 0.84094\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2392 - accuracy: 0.8904 - val_loss: 0.4121 - val_accuracy: 0.8344\n",
            "Epoch 60/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.8907\n",
            "Epoch 00060: val_accuracy improved from 0.84094 to 0.84420, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2384 - accuracy: 0.8908 - val_loss: 0.3964 - val_accuracy: 0.8442\n",
            "Epoch 61/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2314 - accuracy: 0.8946\n",
            "Epoch 00061: val_accuracy did not improve from 0.84420\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2323 - accuracy: 0.8940 - val_loss: 0.4184 - val_accuracy: 0.8327\n",
            "Epoch 62/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2336 - accuracy: 0.8938\n",
            "Epoch 00062: val_accuracy improved from 0.84420 to 0.84519, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.2343 - accuracy: 0.8934 - val_loss: 0.3951 - val_accuracy: 0.8452\n",
            "Epoch 63/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2339 - accuracy: 0.8941\n",
            "Epoch 00063: val_accuracy did not improve from 0.84519\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2339 - accuracy: 0.8941 - val_loss: 0.4012 - val_accuracy: 0.8432\n",
            "Epoch 64/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.8968\n",
            "Epoch 00064: val_accuracy did not improve from 0.84519\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2272 - accuracy: 0.8968 - val_loss: 0.4074 - val_accuracy: 0.8448\n",
            "Epoch 65/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.8986\n",
            "Epoch 00065: val_accuracy improved from 0.84519 to 0.84548, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2252 - accuracy: 0.8986 - val_loss: 0.3948 - val_accuracy: 0.8455\n",
            "Epoch 66/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2286 - accuracy: 0.8967\n",
            "Epoch 00066: val_accuracy improved from 0.84548 to 0.85001, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2289 - accuracy: 0.8965 - val_loss: 0.3907 - val_accuracy: 0.8500\n",
            "Epoch 67/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9003\n",
            "Epoch 00067: val_accuracy did not improve from 0.85001\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2197 - accuracy: 0.9001 - val_loss: 0.3998 - val_accuracy: 0.8452\n",
            "Epoch 68/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.9000\n",
            "Epoch 00068: val_accuracy improved from 0.85001 to 0.85413, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.2223 - accuracy: 0.8999 - val_loss: 0.3905 - val_accuracy: 0.8541\n",
            "Epoch 69/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.2159 - accuracy: 0.9033\n",
            "Epoch 00069: val_accuracy did not improve from 0.85413\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2162 - accuracy: 0.9030 - val_loss: 0.3973 - val_accuracy: 0.8521\n",
            "Epoch 70/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2191 - accuracy: 0.9008\n",
            "Epoch 00070: val_accuracy did not improve from 0.85413\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2194 - accuracy: 0.9007 - val_loss: 0.4027 - val_accuracy: 0.8483\n",
            "Epoch 71/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2137 - accuracy: 0.9046\n",
            "Epoch 00071: val_accuracy improved from 0.85413 to 0.85455, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2137 - accuracy: 0.9046 - val_loss: 0.3943 - val_accuracy: 0.8546\n",
            "Epoch 72/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2139 - accuracy: 0.9039\n",
            "Epoch 00072: val_accuracy did not improve from 0.85455\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2143 - accuracy: 0.9036 - val_loss: 0.4003 - val_accuracy: 0.8451\n",
            "Epoch 73/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2083 - accuracy: 0.9074\n",
            "Epoch 00073: val_accuracy did not improve from 0.85455\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2093 - accuracy: 0.9068 - val_loss: 0.4102 - val_accuracy: 0.8455\n",
            "Epoch 74/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9057\n",
            "Epoch 00074: val_accuracy improved from 0.85455 to 0.85781, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2085 - accuracy: 0.9059 - val_loss: 0.3948 - val_accuracy: 0.8578\n",
            "Epoch 75/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2058 - accuracy: 0.9086\n",
            "Epoch 00075: val_accuracy improved from 0.85781 to 0.86136, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2051 - accuracy: 0.9089 - val_loss: 0.3878 - val_accuracy: 0.8614\n",
            "Epoch 76/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2066 - accuracy: 0.9083\n",
            "Epoch 00076: val_accuracy did not improve from 0.86136\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2068 - accuracy: 0.9082 - val_loss: 0.4048 - val_accuracy: 0.8517\n",
            "Epoch 77/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2064 - accuracy: 0.9083\n",
            "Epoch 00077: val_accuracy did not improve from 0.86136\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2071 - accuracy: 0.9079 - val_loss: 0.3993 - val_accuracy: 0.8554\n",
            "Epoch 78/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9092\n",
            "Epoch 00078: val_accuracy did not improve from 0.86136\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.2044 - accuracy: 0.9092 - val_loss: 0.3934 - val_accuracy: 0.8530\n",
            "Epoch 79/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2042 - accuracy: 0.9084\n",
            "Epoch 00079: val_accuracy did not improve from 0.86136\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2040 - accuracy: 0.9085 - val_loss: 0.3966 - val_accuracy: 0.8563\n",
            "Epoch 80/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2022 - accuracy: 0.9098\n",
            "Epoch 00080: val_accuracy did not improve from 0.86136\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.2024 - accuracy: 0.9097 - val_loss: 0.3928 - val_accuracy: 0.8568\n",
            "Epoch 81/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1848 - accuracy: 0.9188\n",
            "Epoch 00081: val_accuracy improved from 0.86136 to 0.86788, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1850 - accuracy: 0.9188 - val_loss: 0.3875 - val_accuracy: 0.8679\n",
            "Epoch 82/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1748 - accuracy: 0.9246\n",
            "Epoch 00082: val_accuracy improved from 0.86788 to 0.86844, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1741 - accuracy: 0.9250 - val_loss: 0.3863 - val_accuracy: 0.8684\n",
            "Epoch 83/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1731 - accuracy: 0.9259\n",
            "Epoch 00083: val_accuracy did not improve from 0.86844\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1732 - accuracy: 0.9257 - val_loss: 0.3840 - val_accuracy: 0.8673\n",
            "Epoch 84/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1743 - accuracy: 0.9246\n",
            "Epoch 00084: val_accuracy improved from 0.86844 to 0.86873, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1741 - accuracy: 0.9247 - val_loss: 0.3912 - val_accuracy: 0.8687\n",
            "Epoch 85/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1702 - accuracy: 0.9263\n",
            "Epoch 00085: val_accuracy did not improve from 0.86873\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1700 - accuracy: 0.9264 - val_loss: 0.3924 - val_accuracy: 0.8680\n",
            "Epoch 86/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9267\n",
            "Epoch 00086: val_accuracy did not improve from 0.86873\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1686 - accuracy: 0.9267 - val_loss: 0.4007 - val_accuracy: 0.8673\n",
            "Epoch 87/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1664 - accuracy: 0.9276\n",
            "Epoch 00087: val_accuracy improved from 0.86873 to 0.86944, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1664 - accuracy: 0.9276 - val_loss: 0.3935 - val_accuracy: 0.8694\n",
            "Epoch 88/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1677 - accuracy: 0.9274\n",
            "Epoch 00088: val_accuracy did not improve from 0.86944\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1697 - accuracy: 0.9261 - val_loss: 0.3938 - val_accuracy: 0.8655\n",
            "Epoch 89/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1645 - accuracy: 0.9288\n",
            "Epoch 00089: val_accuracy improved from 0.86944 to 0.87454, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1644 - accuracy: 0.9289 - val_loss: 0.3887 - val_accuracy: 0.8745\n",
            "Epoch 90/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9286\n",
            "Epoch 00090: val_accuracy did not improve from 0.87454\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1659 - accuracy: 0.9286 - val_loss: 0.3920 - val_accuracy: 0.8721\n",
            "Epoch 91/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1671 - accuracy: 0.9274\n",
            "Epoch 00091: val_accuracy improved from 0.87454 to 0.87624, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1677 - accuracy: 0.9269 - val_loss: 0.3908 - val_accuracy: 0.8762\n",
            "Epoch 92/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9297\n",
            "Epoch 00092: val_accuracy did not improve from 0.87624\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1634 - accuracy: 0.9295 - val_loss: 0.3996 - val_accuracy: 0.8675\n",
            "Epoch 93/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9311\n",
            "Epoch 00093: val_accuracy did not improve from 0.87624\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1601 - accuracy: 0.9311 - val_loss: 0.3944 - val_accuracy: 0.8717\n",
            "Epoch 94/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1640 - accuracy: 0.9294\n",
            "Epoch 00094: val_accuracy did not improve from 0.87624\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1641 - accuracy: 0.9293 - val_loss: 0.3926 - val_accuracy: 0.8684\n",
            "Epoch 95/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.9285\n",
            "Epoch 00095: val_accuracy improved from 0.87624 to 0.87638, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1640 - accuracy: 0.9285 - val_loss: 0.3936 - val_accuracy: 0.8764\n",
            "Epoch 96/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9307\n",
            "Epoch 00096: val_accuracy did not improve from 0.87638\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1635 - accuracy: 0.9304 - val_loss: 0.3946 - val_accuracy: 0.8744\n",
            "Epoch 97/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1644 - accuracy: 0.9288\n",
            "Epoch 00097: val_accuracy did not improve from 0.87638\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1638 - accuracy: 0.9293 - val_loss: 0.3961 - val_accuracy: 0.8717\n",
            "Epoch 98/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1596 - accuracy: 0.9313\n",
            "Epoch 00098: val_accuracy did not improve from 0.87638\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1602 - accuracy: 0.9311 - val_loss: 0.4085 - val_accuracy: 0.8704\n",
            "Epoch 99/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1589 - accuracy: 0.9314\n",
            "Epoch 00099: val_accuracy did not improve from 0.87638\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1585 - accuracy: 0.9314 - val_loss: 0.4019 - val_accuracy: 0.8703\n",
            "Epoch 100/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9301\n",
            "Epoch 00100: val_accuracy did not improve from 0.87638\n",
            "\n",
            "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1621 - accuracy: 0.9300 - val_loss: 0.4036 - val_accuracy: 0.8694\n",
            "Epoch 101/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1521 - accuracy: 0.9354\n",
            "Epoch 00101: val_accuracy improved from 0.87638 to 0.88021, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1520 - accuracy: 0.9355 - val_loss: 0.3954 - val_accuracy: 0.8802\n",
            "Epoch 102/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1552 - accuracy: 0.9337\n",
            "Epoch 00102: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1545 - accuracy: 0.9341 - val_loss: 0.3947 - val_accuracy: 0.8798\n",
            "Epoch 103/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1448 - accuracy: 0.9388\n",
            "Epoch 00103: val_accuracy improved from 0.88021 to 0.88035, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1448 - accuracy: 0.9388 - val_loss: 0.3942 - val_accuracy: 0.8804\n",
            "Epoch 104/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1462 - accuracy: 0.9387\n",
            "Epoch 00104: val_accuracy improved from 0.88035 to 0.88064, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1464 - accuracy: 0.9386 - val_loss: 0.3966 - val_accuracy: 0.8806\n",
            "Epoch 105/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1427 - accuracy: 0.9413\n",
            "Epoch 00105: val_accuracy did not improve from 0.88064\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1434 - accuracy: 0.9407 - val_loss: 0.3994 - val_accuracy: 0.8785\n",
            "Epoch 106/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1445 - accuracy: 0.9398\n",
            "Epoch 00106: val_accuracy improved from 0.88064 to 0.88205, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1439 - accuracy: 0.9399 - val_loss: 0.3944 - val_accuracy: 0.8821\n",
            "Epoch 107/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.9396\n",
            "Epoch 00107: val_accuracy did not improve from 0.88205\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1432 - accuracy: 0.9398 - val_loss: 0.3986 - val_accuracy: 0.8781\n",
            "Epoch 108/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1461 - accuracy: 0.9378\n",
            "Epoch 00108: val_accuracy improved from 0.88205 to 0.88234, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1460 - accuracy: 0.9379 - val_loss: 0.3996 - val_accuracy: 0.8823\n",
            "Epoch 109/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1478 - accuracy: 0.9376\n",
            "Epoch 00109: val_accuracy improved from 0.88234 to 0.88319, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1473 - accuracy: 0.9377 - val_loss: 0.3978 - val_accuracy: 0.8832\n",
            "Epoch 110/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1394 - accuracy: 0.9421\n",
            "Epoch 00110: val_accuracy did not improve from 0.88319\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1391 - accuracy: 0.9422 - val_loss: 0.4014 - val_accuracy: 0.8811\n",
            "Epoch 111/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.9395\n",
            "Epoch 00111: val_accuracy did not improve from 0.88319\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1430 - accuracy: 0.9395 - val_loss: 0.4000 - val_accuracy: 0.8822\n",
            "Epoch 112/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1452 - accuracy: 0.9377\n",
            "Epoch 00112: val_accuracy did not improve from 0.88319\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1452 - accuracy: 0.9376 - val_loss: 0.3996 - val_accuracy: 0.8825\n",
            "Epoch 113/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1463 - accuracy: 0.9383\n",
            "Epoch 00113: val_accuracy improved from 0.88319 to 0.88347, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.1469 - accuracy: 0.9380 - val_loss: 0.4012 - val_accuracy: 0.8835\n",
            "Epoch 114/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9406\n",
            "Epoch 00114: val_accuracy improved from 0.88347 to 0.88546, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1418 - accuracy: 0.9408 - val_loss: 0.3998 - val_accuracy: 0.8855\n",
            "Epoch 115/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1406 - accuracy: 0.9408\n",
            "Epoch 00115: val_accuracy did not improve from 0.88546\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1410 - accuracy: 0.9406 - val_loss: 0.4032 - val_accuracy: 0.8825\n",
            "Epoch 116/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1430 - accuracy: 0.9392\n",
            "Epoch 00116: val_accuracy did not improve from 0.88546\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1431 - accuracy: 0.9393 - val_loss: 0.4016 - val_accuracy: 0.8840\n",
            "Epoch 117/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9435\n",
            "Epoch 00117: val_accuracy did not improve from 0.88546\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1361 - accuracy: 0.9435 - val_loss: 0.3994 - val_accuracy: 0.8852\n",
            "Epoch 118/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1378 - accuracy: 0.9425\n",
            "Epoch 00118: val_accuracy did not improve from 0.88546\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1377 - accuracy: 0.9423 - val_loss: 0.3984 - val_accuracy: 0.8835\n",
            "Epoch 119/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.9422\n",
            "Epoch 00119: val_accuracy improved from 0.88546 to 0.88588, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1380 - accuracy: 0.9422 - val_loss: 0.4032 - val_accuracy: 0.8859\n",
            "Epoch 120/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1410 - accuracy: 0.9408\n",
            "Epoch 00120: val_accuracy did not improve from 0.88588\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1417 - accuracy: 0.9405 - val_loss: 0.4028 - val_accuracy: 0.8849\n",
            "Epoch 121/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1364 - accuracy: 0.9438\n",
            "Epoch 00121: val_accuracy did not improve from 0.88588\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1360 - accuracy: 0.9440 - val_loss: 0.4028 - val_accuracy: 0.8840\n",
            "Epoch 122/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1446 - accuracy: 0.9384\n",
            "Epoch 00122: val_accuracy did not improve from 0.88588\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1444 - accuracy: 0.9382 - val_loss: 0.4092 - val_accuracy: 0.8826\n",
            "Epoch 123/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1401 - accuracy: 0.9416\n",
            "Epoch 00123: val_accuracy did not improve from 0.88588\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1404 - accuracy: 0.9413 - val_loss: 0.4097 - val_accuracy: 0.8840\n",
            "Epoch 124/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1406 - accuracy: 0.9410\n",
            "Epoch 00124: val_accuracy did not improve from 0.88588\n",
            "\n",
            "Epoch 00124: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1402 - accuracy: 0.9411 - val_loss: 0.4037 - val_accuracy: 0.8843\n",
            "Epoch 125/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9444\n",
            "Epoch 00125: val_accuracy did not improve from 0.88588\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1341 - accuracy: 0.9443 - val_loss: 0.4016 - val_accuracy: 0.8856\n",
            "Epoch 126/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9494\n",
            "Epoch 00126: val_accuracy did not improve from 0.88588\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1263 - accuracy: 0.9495 - val_loss: 0.4033 - val_accuracy: 0.8849\n",
            "Epoch 127/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1319 - accuracy: 0.9466\n",
            "Epoch 00127: val_accuracy improved from 0.88588 to 0.88843, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1319 - accuracy: 0.9464 - val_loss: 0.4021 - val_accuracy: 0.8884\n",
            "Epoch 128/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.9475\n",
            "Epoch 00128: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1312 - accuracy: 0.9469 - val_loss: 0.4040 - val_accuracy: 0.8845\n",
            "Epoch 129/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9488\n",
            "Epoch 00129: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1262 - accuracy: 0.9488 - val_loss: 0.4047 - val_accuracy: 0.8874\n",
            "Epoch 130/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9441\n",
            "Epoch 00130: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1349 - accuracy: 0.9440 - val_loss: 0.4032 - val_accuracy: 0.8876\n",
            "Epoch 131/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1286 - accuracy: 0.9470\n",
            "Epoch 00131: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1288 - accuracy: 0.9467 - val_loss: 0.4012 - val_accuracy: 0.8876\n",
            "Epoch 132/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1340 - accuracy: 0.9447\n",
            "Epoch 00132: val_accuracy did not improve from 0.88843\n",
            "\n",
            "Epoch 00132: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1327 - accuracy: 0.9452 - val_loss: 0.4031 - val_accuracy: 0.8869\n",
            "Epoch 133/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9468\n",
            "Epoch 00133: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1261 - accuracy: 0.9476 - val_loss: 0.4041 - val_accuracy: 0.8866\n",
            "Epoch 134/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9469\n",
            "Epoch 00134: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1296 - accuracy: 0.9465 - val_loss: 0.4018 - val_accuracy: 0.8860\n",
            "Epoch 135/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1259 - accuracy: 0.9491\n",
            "Epoch 00135: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1259 - accuracy: 0.9491 - val_loss: 0.4022 - val_accuracy: 0.8876\n",
            "Epoch 136/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1290 - accuracy: 0.9466\n",
            "Epoch 00136: val_accuracy did not improve from 0.88843\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1288 - accuracy: 0.9467 - val_loss: 0.4045 - val_accuracy: 0.8880\n",
            "Epoch 137/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9489\n",
            "Epoch 00137: val_accuracy improved from 0.88843 to 0.88900, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1242 - accuracy: 0.9489 - val_loss: 0.4039 - val_accuracy: 0.8890\n",
            "Epoch 138/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1275 - accuracy: 0.9479\n",
            "Epoch 00138: val_accuracy did not improve from 0.88900\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1275 - accuracy: 0.9479 - val_loss: 0.4049 - val_accuracy: 0.8883\n",
            "Epoch 139/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9485\n",
            "Epoch 00139: val_accuracy did not improve from 0.88900\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1262 - accuracy: 0.9488 - val_loss: 0.4055 - val_accuracy: 0.8876\n",
            "Epoch 140/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9477\n",
            "Epoch 00140: val_accuracy did not improve from 0.88900\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1276 - accuracy: 0.9479 - val_loss: 0.4048 - val_accuracy: 0.8889\n",
            "Epoch 141/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9490\n",
            "Epoch 00141: val_accuracy did not improve from 0.88900\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1251 - accuracy: 0.9490 - val_loss: 0.4053 - val_accuracy: 0.8877\n",
            "Epoch 142/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1294 - accuracy: 0.9469\n",
            "Epoch 00142: val_accuracy did not improve from 0.88900\n",
            "\n",
            "Epoch 00142: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1290 - accuracy: 0.9471 - val_loss: 0.4049 - val_accuracy: 0.8864\n",
            "Epoch 143/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1241 - accuracy: 0.9495\n",
            "Epoch 00143: val_accuracy did not improve from 0.88900\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1245 - accuracy: 0.9492 - val_loss: 0.4039 - val_accuracy: 0.8886\n",
            "Epoch 144/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9501\n",
            "Epoch 00144: val_accuracy did not improve from 0.88900\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1225 - accuracy: 0.9503 - val_loss: 0.4041 - val_accuracy: 0.8884\n",
            "Epoch 145/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9511\n",
            "Epoch 00145: val_accuracy improved from 0.88900 to 0.88942, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1217 - accuracy: 0.9511 - val_loss: 0.4049 - val_accuracy: 0.8894\n",
            "Epoch 146/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9497\n",
            "Epoch 00146: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1242 - accuracy: 0.9499 - val_loss: 0.4048 - val_accuracy: 0.8890\n",
            "Epoch 147/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9474\n",
            "Epoch 00147: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1290 - accuracy: 0.9471 - val_loss: 0.4047 - val_accuracy: 0.8883\n",
            "Epoch 148/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9487\n",
            "Epoch 00148: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1254 - accuracy: 0.9487 - val_loss: 0.4045 - val_accuracy: 0.8887\n",
            "Epoch 149/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1213 - accuracy: 0.9514\n",
            "Epoch 00149: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1217 - accuracy: 0.9513 - val_loss: 0.4045 - val_accuracy: 0.8884\n",
            "Epoch 150/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1202 - accuracy: 0.9519\n",
            "Epoch 00150: val_accuracy did not improve from 0.88942\n",
            "\n",
            "Epoch 00150: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1199 - accuracy: 0.9522 - val_loss: 0.4048 - val_accuracy: 0.8881\n",
            "Epoch 151/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9522\n",
            "Epoch 00151: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1208 - accuracy: 0.9517 - val_loss: 0.4050 - val_accuracy: 0.8893\n",
            "Epoch 152/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1204 - accuracy: 0.9513\n",
            "Epoch 00152: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1208 - accuracy: 0.9510 - val_loss: 0.4053 - val_accuracy: 0.8886\n",
            "Epoch 153/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9496\n",
            "Epoch 00153: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1235 - accuracy: 0.9494 - val_loss: 0.4053 - val_accuracy: 0.8889\n",
            "Epoch 154/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9489\n",
            "Epoch 00154: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1243 - accuracy: 0.9489 - val_loss: 0.4054 - val_accuracy: 0.8880\n",
            "Epoch 155/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9506\n",
            "Epoch 00155: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1232 - accuracy: 0.9505 - val_loss: 0.4048 - val_accuracy: 0.8890\n",
            "Epoch 156/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9512\n",
            "Epoch 00156: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1221 - accuracy: 0.9514 - val_loss: 0.4055 - val_accuracy: 0.8890\n",
            "Epoch 157/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9505\n",
            "Epoch 00157: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1225 - accuracy: 0.9508 - val_loss: 0.4045 - val_accuracy: 0.8883\n",
            "Epoch 158/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9495\n",
            "Epoch 00158: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1236 - accuracy: 0.9495 - val_loss: 0.4051 - val_accuracy: 0.8890\n",
            "Epoch 159/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9510\n",
            "Epoch 00159: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1230 - accuracy: 0.9508 - val_loss: 0.4052 - val_accuracy: 0.8884\n",
            "Epoch 160/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9509\n",
            "Epoch 00160: val_accuracy did not improve from 0.88942\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1219 - accuracy: 0.9509 - val_loss: 0.4056 - val_accuracy: 0.8889\n",
            "Epoch 161/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1233 - accuracy: 0.9500\n",
            "Epoch 00161: val_accuracy improved from 0.88942 to 0.88971, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1226 - accuracy: 0.9505 - val_loss: 0.4056 - val_accuracy: 0.8897\n",
            "Epoch 162/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9487\n",
            "Epoch 00162: val_accuracy improved from 0.88971 to 0.89013, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1252 - accuracy: 0.9489 - val_loss: 0.4053 - val_accuracy: 0.8901\n",
            "Epoch 163/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9518\n",
            "Epoch 00163: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1200 - accuracy: 0.9520 - val_loss: 0.4058 - val_accuracy: 0.8884\n",
            "Epoch 164/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1231 - accuracy: 0.9508\n",
            "Epoch 00164: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1221 - accuracy: 0.9515 - val_loss: 0.4049 - val_accuracy: 0.8890\n",
            "Epoch 165/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9509\n",
            "Epoch 00165: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1219 - accuracy: 0.9507 - val_loss: 0.4047 - val_accuracy: 0.8889\n",
            "Epoch 166/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9493\n",
            "Epoch 00166: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1247 - accuracy: 0.9493 - val_loss: 0.4048 - val_accuracy: 0.8884\n",
            "Epoch 167/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1225 - accuracy: 0.9499\n",
            "Epoch 00167: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1224 - accuracy: 0.9500 - val_loss: 0.4056 - val_accuracy: 0.8887\n",
            "Epoch 168/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9503\n",
            "Epoch 00168: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1229 - accuracy: 0.9504 - val_loss: 0.4057 - val_accuracy: 0.8890\n",
            "Epoch 169/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9467\n",
            "Epoch 00169: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1284 - accuracy: 0.9467 - val_loss: 0.4069 - val_accuracy: 0.8879\n",
            "Epoch 170/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.9526\n",
            "Epoch 00170: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1192 - accuracy: 0.9525 - val_loss: 0.4054 - val_accuracy: 0.8887\n",
            "Epoch 171/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9490\n",
            "Epoch 00171: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1242 - accuracy: 0.9490 - val_loss: 0.4057 - val_accuracy: 0.8884\n",
            "Epoch 172/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1254 - accuracy: 0.9490\n",
            "Epoch 00172: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1274 - accuracy: 0.9480 - val_loss: 0.4051 - val_accuracy: 0.8893\n",
            "Epoch 173/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.9490\n",
            "Epoch 00173: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1248 - accuracy: 0.9487 - val_loss: 0.4055 - val_accuracy: 0.8884\n",
            "Epoch 174/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1248 - accuracy: 0.9490\n",
            "Epoch 00174: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1249 - accuracy: 0.9489 - val_loss: 0.4057 - val_accuracy: 0.8893\n",
            "Epoch 175/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9512\n",
            "Epoch 00175: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1203 - accuracy: 0.9514 - val_loss: 0.4055 - val_accuracy: 0.8886\n",
            "Epoch 176/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1199 - accuracy: 0.9514\n",
            "Epoch 00176: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1195 - accuracy: 0.9516 - val_loss: 0.4060 - val_accuracy: 0.8890\n",
            "Epoch 177/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1193 - accuracy: 0.9524\n",
            "Epoch 00177: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1195 - accuracy: 0.9522 - val_loss: 0.4060 - val_accuracy: 0.8891\n",
            "Epoch 178/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1261 - accuracy: 0.9481\n",
            "Epoch 00178: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1261 - accuracy: 0.9481 - val_loss: 0.4061 - val_accuracy: 0.8893\n",
            "Epoch 179/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9494\n",
            "Epoch 00179: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1249 - accuracy: 0.9495 - val_loss: 0.4068 - val_accuracy: 0.8900\n",
            "Epoch 180/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9512\n",
            "Epoch 00180: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1216 - accuracy: 0.9512 - val_loss: 0.4072 - val_accuracy: 0.8896\n",
            "Epoch 181/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1215 - accuracy: 0.9507\n",
            "Epoch 00181: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1218 - accuracy: 0.9505 - val_loss: 0.4072 - val_accuracy: 0.8896\n",
            "Epoch 182/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1197 - accuracy: 0.9518\n",
            "Epoch 00182: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1197 - accuracy: 0.9518 - val_loss: 0.4068 - val_accuracy: 0.8897\n",
            "Epoch 183/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9478\n",
            "Epoch 00183: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1281 - accuracy: 0.9476 - val_loss: 0.4066 - val_accuracy: 0.8880\n",
            "Epoch 184/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9506\n",
            "Epoch 00184: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1234 - accuracy: 0.9502 - val_loss: 0.4068 - val_accuracy: 0.8877\n",
            "Epoch 185/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9491\n",
            "Epoch 00185: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1225 - accuracy: 0.9491 - val_loss: 0.4068 - val_accuracy: 0.8896\n",
            "Epoch 186/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1257 - accuracy: 0.9480\n",
            "Epoch 00186: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1247 - accuracy: 0.9486 - val_loss: 0.4074 - val_accuracy: 0.8891\n",
            "Epoch 187/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9501\n",
            "Epoch 00187: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1220 - accuracy: 0.9499 - val_loss: 0.4069 - val_accuracy: 0.8891\n",
            "Epoch 188/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9511\n",
            "Epoch 00188: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1206 - accuracy: 0.9509 - val_loss: 0.4068 - val_accuracy: 0.8887\n",
            "Epoch 189/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9540\n",
            "Epoch 00189: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1169 - accuracy: 0.9540 - val_loss: 0.4065 - val_accuracy: 0.8889\n",
            "Epoch 190/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9525\n",
            "Epoch 00190: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1170 - accuracy: 0.9526 - val_loss: 0.4068 - val_accuracy: 0.8893\n",
            "Epoch 191/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1174 - accuracy: 0.9526\n",
            "Epoch 00191: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1171 - accuracy: 0.9529 - val_loss: 0.4070 - val_accuracy: 0.8886\n",
            "Epoch 192/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9486\n",
            "Epoch 00192: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1250 - accuracy: 0.9489 - val_loss: 0.4069 - val_accuracy: 0.8889\n",
            "Epoch 193/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9495\n",
            "Epoch 00193: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1253 - accuracy: 0.9487 - val_loss: 0.4068 - val_accuracy: 0.8896\n",
            "Epoch 194/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9529\n",
            "Epoch 00194: val_accuracy did not improve from 0.89013\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1183 - accuracy: 0.9529 - val_loss: 0.4071 - val_accuracy: 0.8887\n",
            "Epoch 195/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9510\n",
            "Epoch 00195: val_accuracy improved from 0.89013 to 0.89028, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1202 - accuracy: 0.9510 - val_loss: 0.4072 - val_accuracy: 0.8903\n",
            "Epoch 196/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9533\n",
            "Epoch 00196: val_accuracy did not improve from 0.89028\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1167 - accuracy: 0.9533 - val_loss: 0.4074 - val_accuracy: 0.8890\n",
            "Epoch 197/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9534\n",
            "Epoch 00197: val_accuracy did not improve from 0.89028\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1172 - accuracy: 0.9534 - val_loss: 0.4071 - val_accuracy: 0.8898\n",
            "Epoch 198/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9496\n",
            "Epoch 00198: val_accuracy did not improve from 0.89028\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1247 - accuracy: 0.9494 - val_loss: 0.4073 - val_accuracy: 0.8901\n",
            "Epoch 199/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9505\n",
            "Epoch 00199: val_accuracy did not improve from 0.89028\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1230 - accuracy: 0.9502 - val_loss: 0.4067 - val_accuracy: 0.8891\n",
            "Epoch 200/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9507\n",
            "Epoch 00200: val_accuracy improved from 0.89028 to 0.89042, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1207 - accuracy: 0.9504 - val_loss: 0.4067 - val_accuracy: 0.8904\n",
            "Epoch 201/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9477\n",
            "Epoch 00201: val_accuracy did not improve from 0.89042\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1279 - accuracy: 0.9475 - val_loss: 0.4070 - val_accuracy: 0.8897\n",
            "Epoch 202/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9508\n",
            "Epoch 00202: val_accuracy did not improve from 0.89042\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1216 - accuracy: 0.9504 - val_loss: 0.4068 - val_accuracy: 0.8903\n",
            "Epoch 203/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9533\n",
            "Epoch 00203: val_accuracy improved from 0.89042 to 0.89056, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1189 - accuracy: 0.9528 - val_loss: 0.4071 - val_accuracy: 0.8906\n",
            "Epoch 204/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9512\n",
            "Epoch 00204: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1202 - accuracy: 0.9512 - val_loss: 0.4067 - val_accuracy: 0.8898\n",
            "Epoch 205/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1255 - accuracy: 0.9485\n",
            "Epoch 00205: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1247 - accuracy: 0.9488 - val_loss: 0.4064 - val_accuracy: 0.8900\n",
            "Epoch 206/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1208 - accuracy: 0.9513\n",
            "Epoch 00206: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1210 - accuracy: 0.9511 - val_loss: 0.4063 - val_accuracy: 0.8896\n",
            "Epoch 207/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9515\n",
            "Epoch 00207: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1209 - accuracy: 0.9515 - val_loss: 0.4068 - val_accuracy: 0.8894\n",
            "Epoch 208/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9500\n",
            "Epoch 00208: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1244 - accuracy: 0.9494 - val_loss: 0.4067 - val_accuracy: 0.8893\n",
            "Epoch 209/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9495\n",
            "Epoch 00209: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1233 - accuracy: 0.9495 - val_loss: 0.4070 - val_accuracy: 0.8898\n",
            "Epoch 210/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9507\n",
            "Epoch 00210: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1231 - accuracy: 0.9506 - val_loss: 0.4076 - val_accuracy: 0.8889\n",
            "Epoch 211/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9518\n",
            "Epoch 00211: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1213 - accuracy: 0.9511 - val_loss: 0.4075 - val_accuracy: 0.8896\n",
            "Epoch 212/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9500\n",
            "Epoch 00212: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1230 - accuracy: 0.9498 - val_loss: 0.4077 - val_accuracy: 0.8897\n",
            "Epoch 213/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9477\n",
            "Epoch 00213: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1279 - accuracy: 0.9478 - val_loss: 0.4082 - val_accuracy: 0.8897\n",
            "Epoch 214/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1226 - accuracy: 0.9499\n",
            "Epoch 00214: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1224 - accuracy: 0.9499 - val_loss: 0.4081 - val_accuracy: 0.8894\n",
            "Epoch 215/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9516\n",
            "Epoch 00215: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1210 - accuracy: 0.9512 - val_loss: 0.4080 - val_accuracy: 0.8901\n",
            "Epoch 216/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9514\n",
            "Epoch 00216: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1203 - accuracy: 0.9511 - val_loss: 0.4080 - val_accuracy: 0.8898\n",
            "Epoch 217/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9518\n",
            "Epoch 00217: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1179 - accuracy: 0.9521 - val_loss: 0.4075 - val_accuracy: 0.8898\n",
            "Epoch 218/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9519\n",
            "Epoch 00218: val_accuracy improved from 0.89056 to 0.89098, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1201 - accuracy: 0.9515 - val_loss: 0.4072 - val_accuracy: 0.8910\n",
            "Epoch 219/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9500\n",
            "Epoch 00219: val_accuracy did not improve from 0.89098\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1228 - accuracy: 0.9502 - val_loss: 0.4063 - val_accuracy: 0.8904\n",
            "Epoch 220/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9515\n",
            "Epoch 00220: val_accuracy did not improve from 0.89098\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1194 - accuracy: 0.9515 - val_loss: 0.4066 - val_accuracy: 0.8910\n",
            "Epoch 221/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1243 - accuracy: 0.9485\n",
            "Epoch 00221: val_accuracy improved from 0.89098 to 0.89113, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1249 - accuracy: 0.9484 - val_loss: 0.4062 - val_accuracy: 0.8911\n",
            "Epoch 222/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9521\n",
            "Epoch 00222: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1188 - accuracy: 0.9523 - val_loss: 0.4071 - val_accuracy: 0.8903\n",
            "Epoch 223/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9521\n",
            "Epoch 00223: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1192 - accuracy: 0.9521 - val_loss: 0.4074 - val_accuracy: 0.8901\n",
            "Epoch 224/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9514\n",
            "Epoch 00224: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1214 - accuracy: 0.9515 - val_loss: 0.4075 - val_accuracy: 0.8900\n",
            "Epoch 225/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9515\n",
            "Epoch 00225: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1207 - accuracy: 0.9515 - val_loss: 0.4071 - val_accuracy: 0.8911\n",
            "Epoch 226/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9521\n",
            "Epoch 00226: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1208 - accuracy: 0.9517 - val_loss: 0.4074 - val_accuracy: 0.8906\n",
            "Epoch 227/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9512\n",
            "Epoch 00227: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1202 - accuracy: 0.9512 - val_loss: 0.4069 - val_accuracy: 0.8896\n",
            "Epoch 228/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9507\n",
            "Epoch 00228: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1204 - accuracy: 0.9509 - val_loss: 0.4066 - val_accuracy: 0.8904\n",
            "Epoch 229/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9514\n",
            "Epoch 00229: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1207 - accuracy: 0.9511 - val_loss: 0.4075 - val_accuracy: 0.8900\n",
            "Epoch 230/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9537\n",
            "Epoch 00230: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1152 - accuracy: 0.9537 - val_loss: 0.4075 - val_accuracy: 0.8904\n",
            "Epoch 231/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9492\n",
            "Epoch 00231: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1250 - accuracy: 0.9488 - val_loss: 0.4069 - val_accuracy: 0.8897\n",
            "Epoch 232/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1184 - accuracy: 0.9524\n",
            "Epoch 00232: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1180 - accuracy: 0.9525 - val_loss: 0.4079 - val_accuracy: 0.8901\n",
            "Epoch 233/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1175 - accuracy: 0.9527\n",
            "Epoch 00233: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1183 - accuracy: 0.9523 - val_loss: 0.4074 - val_accuracy: 0.8897\n",
            "Epoch 234/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9506\n",
            "Epoch 00234: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1218 - accuracy: 0.9504 - val_loss: 0.4077 - val_accuracy: 0.8907\n",
            "Epoch 235/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9516\n",
            "Epoch 00235: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1202 - accuracy: 0.9520 - val_loss: 0.4085 - val_accuracy: 0.8907\n",
            "Epoch 236/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1193 - accuracy: 0.9515\n",
            "Epoch 00236: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1195 - accuracy: 0.9515 - val_loss: 0.4083 - val_accuracy: 0.8900\n",
            "Epoch 237/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9536\n",
            "Epoch 00237: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1173 - accuracy: 0.9534 - val_loss: 0.4085 - val_accuracy: 0.8900\n",
            "Epoch 238/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9504\n",
            "Epoch 00238: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1206 - accuracy: 0.9504 - val_loss: 0.4089 - val_accuracy: 0.8900\n",
            "Epoch 239/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9509\n",
            "Epoch 00239: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1236 - accuracy: 0.9500 - val_loss: 0.4090 - val_accuracy: 0.8897\n",
            "Epoch 240/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9519\n",
            "Epoch 00240: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1190 - accuracy: 0.9519 - val_loss: 0.4093 - val_accuracy: 0.8907\n",
            "Epoch 241/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9509\n",
            "Epoch 00241: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1224 - accuracy: 0.9507 - val_loss: 0.4091 - val_accuracy: 0.8906\n",
            "Epoch 242/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9522\n",
            "Epoch 00242: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1184 - accuracy: 0.9518 - val_loss: 0.4087 - val_accuracy: 0.8910\n",
            "Epoch 243/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9524\n",
            "Epoch 00243: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1172 - accuracy: 0.9524 - val_loss: 0.4096 - val_accuracy: 0.8904\n",
            "Epoch 244/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9486\n",
            "Epoch 00244: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1250 - accuracy: 0.9489 - val_loss: 0.4089 - val_accuracy: 0.8911\n",
            "Epoch 245/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9516\n",
            "Epoch 00245: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1183 - accuracy: 0.9517 - val_loss: 0.4090 - val_accuracy: 0.8897\n",
            "Epoch 246/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9518\n",
            "Epoch 00246: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1209 - accuracy: 0.9516 - val_loss: 0.4090 - val_accuracy: 0.8904\n",
            "Epoch 247/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9502\n",
            "Epoch 00247: val_accuracy did not improve from 0.89113\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.1228 - accuracy: 0.9503 - val_loss: 0.4094 - val_accuracy: 0.8911\n",
            "Epoch 248/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9507\n",
            "Epoch 00248: val_accuracy improved from 0.89113 to 0.89198, saving model to best_cnn1d_model.h5\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1208 - accuracy: 0.9507 - val_loss: 0.4091 - val_accuracy: 0.8920\n",
            "Epoch 249/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9515\n",
            "Epoch 00249: val_accuracy did not improve from 0.89198\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.1201 - accuracy: 0.9514 - val_loss: 0.4096 - val_accuracy: 0.8918\n",
            "Epoch 250/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1189 - accuracy: 0.9514\n",
            "Epoch 00250: val_accuracy did not improve from 0.89198\n",
            "127/127 [==============================] - 1s 8ms/step - loss: 0.1195 - accuracy: 0.9512 - val_loss: 0.4098 - val_accuracy: 0.8911\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 0.3843 - accuracy: 0.8935\n",
            "Accuracy(on Test-data): 0.8934677243232727\n",
            "time of training (s) 244.17793679237366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf2Wv3brUACu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "32af6d35-04a2-4cd5-a38d-d911e4e3b78a"
      },
      "source": [
        "# load a saved model\n",
        "from keras.models import load_model\n",
        "saved_model = load_model('best_cnn1d_model.h5')\n",
        "# evaluate the model\n",
        "_, train_acc = saved_model.evaluate(X_train, y_train)\n",
        "_, test_acc = saved_model.evaluate(X_test, y_test)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2205/2205 [==============================] - 6s 3ms/step - loss: 0.1103 - accuracy: 0.9683\n",
            "245/245 [==============================] - 1s 2ms/step - loss: 0.3841 - accuracy: 0.8956\n",
            "Train: 0.968, Test: 0.896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkfhJ8SJO_BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viz(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        " \n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px20O6T2PDYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "8ab36ccb-4a55-411d-c3c0-08fb61865e73"
      },
      "source": [
        "viz(fit1)\n",
        "#fit1.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvp4i_hqJFYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "b01713ad-0364-46e0-d219-37846bfdb397"
      },
      "source": [
        " \n",
        "import time\n",
        " \n",
        "Xnew=[['70.39932429', '127673.0908', '-49.57230843', '127648.0176', '-169.5783186', '127723.2374', '65.68961121', '605.91099', '-57.00357104', '626.78553', '-173.5890232', '602.4319', '70.4222426', '127673.0908', '0', '0', '0', '0', '65.00779144', '611.5874', '118.5678861', '13.18392', '-100.8692198', '13.91636', '59.999', '0.01', '6.391383458', '0.076290455', '0', '60.65826798', '124631.8125', '-59.29595943', '124484.3594', '-179.3380777', '124715.0703', '-119.5504813', '612.7967529', '117.7267525', '632.5321045', '0.859680212', '610.1417236', '60.6802407', '124611.9844', '0', '0', '0', '0', '-120.3414991', '618.3013916', '-64.05304275', '12.7658844', '69.39789118', '12.8288269', '59.99900055', '0.02', '6.130100104', '3.135101005', '0', '60.66477135', '124187.9063', '-59.31259095', '124162.833', '-179.3014124', '124212.9796', '-119.7539088', '610.12252', '117.6855311', '628.25041', '0.658901464', '606.82654', '60.68768966', '124187.9063', '0', '0', '0', '0', '-120.4872947', '614.88338', '-64.81298579', '12.08526', '70.38786513', '11.90215', '59.999', '0.02', '6.111439531', '3.140520023', '0', '70.45089049', '127723.2374', '-49.53793097', '127096.4056', '-169.532482', '127773.3839', '65.64377459', '604.44611', '-56.87179074', '621.84156', '-173.8697725', '599.86836', '70.46234965', '127522.6512', '0', '0', '0', '0', '64.95049566', '608.47453', '119.3012721', '12.26837', '-102.060972', '11.71904', '59.999', '0.01', '6.341831592', '0.077897157', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#normal    \n",
        " \n",
        "Q=[['8.508423258', '130832.3229', '-111.4632095', '130782.1763', '128.5258926', '130907.5427', '3.729955246', '500.80585', '-116.6026409', '500.62274', '123.632833', '501.35518', '8.519882414', '130832.3229', '0', '0', '0', '0', '3.586715798', '500.98896', '0', '0', '0', '0', '60', '0', '7.438322484', '0.077833008', '0', '0.920105028', '128836.9766', '-119.0066616', '128699.4609', '120.9347557', '128913.4688', '176.7782594', '505.7258606', '56.64825709', '507.2078857', '-63.31146179', '506.9961548', '0.94757082', '128814.1563', '0', '0', '0', '0', '176.7068566', '506.6356812', '0', '0', '0', '0', '60', '0', '7.257351968', '-3.071651355', '0', '0.928191628', '128375.1424', '-119.0319819', '128350.0691', '120.9743089', '128400.2157', '176.7173728', '502.63695', '56.61968931', '503.36939', '-63.3290251', '503.00317', '0.95110994', '128375.1424', '0', '0', '0', '0', '176.6715361', '503.00317', '0', '0', '0', '0', '60', '0', '7.268424496', '-3.0691986', '0', '8.559989459', '130857.3961', '-111.428832', '130230.5644', '128.5831884', '130932.6159', '3.707036934', '497.50987', '-116.3619986', '498.24231', '123.741695', '496.59432', '8.571448615', '130681.8832', '0', '0', '0', '0', '3.689848201', '497.50987', '0', '0', '0', '0', '60', '0', '7.489104114', '0.086553421', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        " \n",
        "N=[['70.97801166', '130957.6892', '-48.99362106', '130932.6159', '-169.0053608', '131032.909', '66.42872677', '482.31174', '-53.82938485', '483.22729', '-173.646319', '483.77662', '70.99520039', '130982.7625', '0', '0', '0', '0', '66.31986479', '483.04418', '0', '0', '0', '0', '59.999', '0', '7.742501568', '0.074426263', '0', '63.67401507', '129107.1016', '-56.26922599', '128963.9531', '-176.3168392', '129194.8594', '-120.8908103', '488.6340942', '119.1714468', '489.3894043', '-0.826721188', '489.4866943', '63.70147926', '129088.1484', '0', '0', '0', '0', '-120.8468648', '489.1662598', '0', '0', '0', '0', '60', '0', '7.521482155', '-3.061590456', '0', '63.67279977', '128650.9484', '-56.28737379', '128625.8751', '-176.2819248', '128676.0216', '-120.8826356', '485.79083', '119.1981397', '485.42461', '-0.916732472', '485.42461', '63.70144766', '128650.9484', '0', '0', '0', '0', '-120.8711765', '485.42461', '0', '0', '0', '0', '59.999', '0', '7.510130448', '-3.061004926', '0', '71.03530744', '131007.8358', '-48.97070275', '130381.004', '-168.9423355', '131083.0556', '66.42872677', '480.11442', '-53.53717638', '480.11442', '-173.5890232', '479.38198', '71.0467666', '130832.3229', '0', '0', '0', '0', '66.44018592', '479.93131', '0', '0', '0', '0', '60', '0', '7.746914069', '0.08213434', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        "A=[['174.2765725', '98337.36494', '59.20372897', '128550.6553', '-72.71407378', '127347.1383', '100.4796085', '1111.29459', '-4.566473627', '241.33898', '-85.22174245', '621.84156', '173.5546457', '117618.7096', '-9.786119141', '17325.62957', '-13.22959549', '1980.78833', '118.854365', '599.68525', '82.17933656', '420.23745', '78.74731936', '154.36173', '60.016', '-0.04', '1.907633306', '1.477346313', '0', '173.1802387', '47452.82422', '50.77606342', '120554.2734', '-77.66510434', '132296.1563', '-80.08758802', '1147.247314', '170.1754746', '297.0142212', '89.53582968', '636.0683594', '167.4398882', '99778.42969', '-10.8901979', '31715.72461', '-27.77069198', '21617.71094', '-63.9047252', '642.6830444', '-97.7590974', '408.9145508', '-101.4559988', '150.7930756', '60.01599884', '-0.050000001', '0.903115513', '-1.682575032', '0', '173.1822232', '47288.18722', '50.75260149', '120226.3297', '-77.62432208', '131760.0339', '85.92075096', '1062.40422', '-112.9070631', '589.06487', '123.4151091', '150.88264', '167.4526452', '99440.58882', '-10.94922346', '31567.24693', '-27.58791784', '21588.08547', '54.61433703', '466.74739', '103.9746511', '479.01576', '110.2084319', '228.33817', '60.016', '-0.04', '0.793198105', '1.484402105', '0', '169.0225496', '41797.14109', '63.34048425', '121254.3337', '-75.34967964', '121881.1655', '122.4983766', '1748.15117', '72.4218653', '600.23458', '-38.82934978', '177.06737', '173.2395189', '93849.24961', '-3.907572163', '37534.68519', '-2.056918485', '14743.08276', '142.8441079', '707.17082', '112.8039307', '436.16802', '107.7561725', '677.507', '60.014', '-0.04', '0.341509407', '1.111764282', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#'Attack'\n",
        "transformer= Normalizer() \n",
        "Z=transformer.transform(N)\n",
        "V = ss.transform(Z)\n",
        " \n",
        "V= np.reshape(V, (V.shape[0], 1, V.shape[1]))\n",
        "start_time = time.time()\n",
        " \n",
        "# make a prediction\n",
        "ynew = saved_model.predict_classes(V)\n",
        "# show the inputs and predicted outputs\n",
        "#for i in range(len(Xnew)):\n",
        "    #print(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
        " \n",
        "duration = time.time() - start_time\n",
        "print(\"time of test (s)\", duration)\n",
        " \n",
        "print(ynew)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time of test (s) 0.037695884704589844\n",
            "[0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:1829: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
            "  X = check_array(X, accept_sparse='csr')\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}