{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN2D.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5cs8SI-lnmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#إستدعاء المكتبيات\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nKs4eFGohUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install patool\n",
        "!pip install pyunpack\n",
        "from pyunpack import Archive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjqR3iJGokan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Archive(\"/data/binaryAllNaturalPlusNormalVsAttacks.7z\").extractall(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg71dcLVmEqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.read_csv(\"/data/data1.csv\")\n",
        "df2=pd.read_csv(\"/data/data2.csv\")\n",
        "df3=pd.read_csv(\"/data/data3.csv\")\n",
        "df4=pd.read_csv(\"/data/data4.csv\")\n",
        "df5=pd.read_csv(\"/data/data5.csv\")\n",
        "df6=pd.read_csv(\"/data/data6.csv\")\n",
        "df7=pd.read_csv(\"/data/data7.csv\")\n",
        "df8=pd.read_csv(\"/data/data8.csv\")\n",
        "df9=pd.read_csv(\"/data/data9.csv\")\n",
        "df10=pd.read_csv(\"/data/data10.csv\")\n",
        "df11=pd.read_csv(\"/data/data11.csv\")\n",
        "df12=pd.read_csv(\"/data/data12.csv\")\n",
        "df13=pd.read_csv(\"/data/data13.csv\")\n",
        "df14=pd.read_csv(\"/data/data14.csv\")\n",
        "df15=pd.read_csv(\"/data/data15.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apenq3XamKp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#دمج مجموعات البيانات\n",
        "df = pd.concat([df1, df2,df3,df4, df5,df6,df7, df8,df9,df10, df11,df12,df13, df14,df15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN_Ys98wmM2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "418f17c3-03da-4b89-8ffd-1e62c2fe69ac"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvNPkdzmOca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "629e46bd-8376-4d55-8326-48b30a804972"
      },
      "source": [
        "df.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvheO1H9mQGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "19d2d58b-44ba-423b-a387-37032d13b603"
      },
      "source": [
        "df.describe(include=\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-PA1:VH</th>\n",
              "      <th>R1-PM1:V</th>\n",
              "      <th>R1-PA2:VH</th>\n",
              "      <th>R1-PM2:V</th>\n",
              "      <th>R1-PA3:VH</th>\n",
              "      <th>R1-PM3:V</th>\n",
              "      <th>R1-PA4:IH</th>\n",
              "      <th>R1-PM4:I</th>\n",
              "      <th>R1-PA5:IH</th>\n",
              "      <th>R1-PM5:I</th>\n",
              "      <th>R1-PA6:IH</th>\n",
              "      <th>R1-PM6:I</th>\n",
              "      <th>R1-PA7:VH</th>\n",
              "      <th>R1-PM7:V</th>\n",
              "      <th>R1-PA8:VH</th>\n",
              "      <th>R1-PM8:V</th>\n",
              "      <th>R1-PA9:VH</th>\n",
              "      <th>R1-PM9:V</th>\n",
              "      <th>R1-PA10:IH</th>\n",
              "      <th>R1-PM10:I</th>\n",
              "      <th>R1-PA11:IH</th>\n",
              "      <th>R1-PM11:I</th>\n",
              "      <th>R1-PA12:IH</th>\n",
              "      <th>R1-PM12:I</th>\n",
              "      <th>R1:F</th>\n",
              "      <th>R1:DF</th>\n",
              "      <th>R1-PA:Z</th>\n",
              "      <th>R1-PA:ZH</th>\n",
              "      <th>R1:S</th>\n",
              "      <th>R2-PA1:VH</th>\n",
              "      <th>R2-PM1:V</th>\n",
              "      <th>R2-PA2:VH</th>\n",
              "      <th>R2-PM2:V</th>\n",
              "      <th>R2-PA3:VH</th>\n",
              "      <th>R2-PM3:V</th>\n",
              "      <th>R2-PA4:IH</th>\n",
              "      <th>R2-PM4:I</th>\n",
              "      <th>R2-PA5:IH</th>\n",
              "      <th>R2-PM5:I</th>\n",
              "      <th>R2-PA6:IH</th>\n",
              "      <th>...</th>\n",
              "      <th>R4-PA2:VH</th>\n",
              "      <th>R4-PM2:V</th>\n",
              "      <th>R4-PA3:VH</th>\n",
              "      <th>R4-PM3:V</th>\n",
              "      <th>R4-PA4:IH</th>\n",
              "      <th>R4-PM4:I</th>\n",
              "      <th>R4-PA5:IH</th>\n",
              "      <th>R4-PM5:I</th>\n",
              "      <th>R4-PA6:IH</th>\n",
              "      <th>R4-PM6:I</th>\n",
              "      <th>R4-PA7:VH</th>\n",
              "      <th>R4-PM7:V</th>\n",
              "      <th>R4-PA8:VH</th>\n",
              "      <th>R4-PM8:V</th>\n",
              "      <th>R4-PA9:VH</th>\n",
              "      <th>R4-PM9:V</th>\n",
              "      <th>R4-PA10:IH</th>\n",
              "      <th>R4-PM10:I</th>\n",
              "      <th>R4-PA11:IH</th>\n",
              "      <th>R4-PM11:I</th>\n",
              "      <th>R4-PA12:IH</th>\n",
              "      <th>R4-PM12:I</th>\n",
              "      <th>R4:F</th>\n",
              "      <th>R4:DF</th>\n",
              "      <th>R4-PA:Z</th>\n",
              "      <th>R4-PA:ZH</th>\n",
              "      <th>R4:S</th>\n",
              "      <th>control_panel_log1</th>\n",
              "      <th>control_panel_log2</th>\n",
              "      <th>control_panel_log3</th>\n",
              "      <th>control_panel_log4</th>\n",
              "      <th>relay1_log</th>\n",
              "      <th>relay2_log</th>\n",
              "      <th>relay3_log</th>\n",
              "      <th>relay4_log</th>\n",
              "      <th>snort_log1</th>\n",
              "      <th>snort_log2</th>\n",
              "      <th>snort_log3</th>\n",
              "      <th>snort_log4</th>\n",
              "      <th>marker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>55663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-15.802424</td>\n",
              "      <td>130764.039577</td>\n",
              "      <td>2.175196</td>\n",
              "      <td>131035.528095</td>\n",
              "      <td>6.834315</td>\n",
              "      <td>131395.717581</td>\n",
              "      <td>-14.334996</td>\n",
              "      <td>393.949321</td>\n",
              "      <td>3.538540</td>\n",
              "      <td>387.438133</td>\n",
              "      <td>6.129781</td>\n",
              "      <td>381.912845</td>\n",
              "      <td>-15.798835</td>\n",
              "      <td>131056.980030</td>\n",
              "      <td>0.207857</td>\n",
              "      <td>297.083556</td>\n",
              "      <td>0.227606</td>\n",
              "      <td>87.397031</td>\n",
              "      <td>-14.504282</td>\n",
              "      <td>386.557188</td>\n",
              "      <td>-1.734936</td>\n",
              "      <td>9.979982</td>\n",
              "      <td>6.123374</td>\n",
              "      <td>9.494176</td>\n",
              "      <td>59.992801</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.018428</td>\n",
              "      <td>788.868750</td>\n",
              "      <td>-15.216491</td>\n",
              "      <td>127033.389923</td>\n",
              "      <td>4.751134</td>\n",
              "      <td>128015.428015</td>\n",
              "      <td>5.510410</td>\n",
              "      <td>128362.246185</td>\n",
              "      <td>15.836436</td>\n",
              "      <td>395.109497</td>\n",
              "      <td>-6.961603</td>\n",
              "      <td>392.508845</td>\n",
              "      <td>-6.437082</td>\n",
              "      <td>...</td>\n",
              "      <td>2.278991</td>\n",
              "      <td>131355.212680</td>\n",
              "      <td>7.065760</td>\n",
              "      <td>131745.074472</td>\n",
              "      <td>-13.931742</td>\n",
              "      <td>391.330912</td>\n",
              "      <td>3.446031</td>\n",
              "      <td>384.399819</td>\n",
              "      <td>6.096400</td>\n",
              "      <td>379.952713</td>\n",
              "      <td>-15.563852</td>\n",
              "      <td>131397.999652</td>\n",
              "      <td>0.257084</td>\n",
              "      <td>292.112647</td>\n",
              "      <td>0.207103</td>\n",
              "      <td>82.439295</td>\n",
              "      <td>-14.144585</td>\n",
              "      <td>384.036050</td>\n",
              "      <td>-1.859917</td>\n",
              "      <td>9.834635</td>\n",
              "      <td>5.989009</td>\n",
              "      <td>9.073233</td>\n",
              "      <td>59.992750</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.016616</td>\n",
              "      <td>749.014459</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.026436</td>\n",
              "      <td>0.026500</td>\n",
              "      <td>0.035597</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>100.876750</td>\n",
              "      <td>8546.118477</td>\n",
              "      <td>111.743169</td>\n",
              "      <td>5393.135370</td>\n",
              "      <td>97.065063</td>\n",
              "      <td>5443.752388</td>\n",
              "      <td>99.601107</td>\n",
              "      <td>190.966011</td>\n",
              "      <td>109.504977</td>\n",
              "      <td>151.277183</td>\n",
              "      <td>95.294904</td>\n",
              "      <td>153.326452</td>\n",
              "      <td>100.877118</td>\n",
              "      <td>6152.379663</td>\n",
              "      <td>13.075863</td>\n",
              "      <td>2687.617199</td>\n",
              "      <td>12.488596</td>\n",
              "      <td>897.541412</td>\n",
              "      <td>99.605025</td>\n",
              "      <td>154.484403</td>\n",
              "      <td>68.383257</td>\n",
              "      <td>47.241783</td>\n",
              "      <td>73.059209</td>\n",
              "      <td>47.875569</td>\n",
              "      <td>0.610045</td>\n",
              "      <td>0.087799</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.242813</td>\n",
              "      <td>14048.448459</td>\n",
              "      <td>101.837622</td>\n",
              "      <td>16155.767175</td>\n",
              "      <td>111.043204</td>\n",
              "      <td>12106.876201</td>\n",
              "      <td>96.270117</td>\n",
              "      <td>11990.863815</td>\n",
              "      <td>99.876094</td>\n",
              "      <td>171.765698</td>\n",
              "      <td>94.996062</td>\n",
              "      <td>152.357765</td>\n",
              "      <td>108.896267</td>\n",
              "      <td>...</td>\n",
              "      <td>111.828597</td>\n",
              "      <td>4733.901358</td>\n",
              "      <td>97.085981</td>\n",
              "      <td>4777.648212</td>\n",
              "      <td>99.653296</td>\n",
              "      <td>187.094100</td>\n",
              "      <td>109.561785</td>\n",
              "      <td>148.882516</td>\n",
              "      <td>95.495537</td>\n",
              "      <td>150.929876</td>\n",
              "      <td>100.882320</td>\n",
              "      <td>5536.542517</td>\n",
              "      <td>13.150046</td>\n",
              "      <td>2621.155809</td>\n",
              "      <td>12.523032</td>\n",
              "      <td>850.696972</td>\n",
              "      <td>99.627784</td>\n",
              "      <td>151.746973</td>\n",
              "      <td>67.783975</td>\n",
              "      <td>47.562328</td>\n",
              "      <td>72.087423</td>\n",
              "      <td>45.998572</td>\n",
              "      <td>0.609958</td>\n",
              "      <td>0.087273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.248023</td>\n",
              "      <td>14041.170907</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.186082</td>\n",
              "      <td>0.160430</td>\n",
              "      <td>0.160618</td>\n",
              "      <td>0.185285</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.009450</td>\n",
              "      <td>0.008749</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.501948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.903018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.010000</td>\n",
              "      <td>1.852102e-01</td>\n",
              "      <td>-3.140569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>...</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-169.984571</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>-178.875387</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-142.790610</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-165.686647</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>-155.458725</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-145.203989</td>\n",
              "      <td>-179.719672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.612733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.460000</td>\n",
              "      <td>6.781427e-03</td>\n",
              "      <td>-3.093717</td>\n",
              "      <td>-45.998332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-100.416583</td>\n",
              "      <td>131057.982300</td>\n",
              "      <td>-102.129727</td>\n",
              "      <td>130732.029800</td>\n",
              "      <td>-69.459673</td>\n",
              "      <td>131133.202100</td>\n",
              "      <td>-98.159129</td>\n",
              "      <td>305.793700</td>\n",
              "      <td>-94.790138</td>\n",
              "      <td>311.836330</td>\n",
              "      <td>-66.279758</td>\n",
              "      <td>303.962600</td>\n",
              "      <td>-100.399394</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-98.227884</td>\n",
              "      <td>307.807910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.291006e+00</td>\n",
              "      <td>-0.028589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-101.562499</td>\n",
              "      <td>128826.461300</td>\n",
              "      <td>-96.490178</td>\n",
              "      <td>128769.000000</td>\n",
              "      <td>-69.052873</td>\n",
              "      <td>128901.681100</td>\n",
              "      <td>-65.091252</td>\n",
              "      <td>310.554560</td>\n",
              "      <td>-83.182013</td>\n",
              "      <td>316.446289</td>\n",
              "      <td>-105.295719</td>\n",
              "      <td>...</td>\n",
              "      <td>-101.992217</td>\n",
              "      <td>131018.156300</td>\n",
              "      <td>-69.430847</td>\n",
              "      <td>131435.093800</td>\n",
              "      <td>-97.643467</td>\n",
              "      <td>304.511930</td>\n",
              "      <td>-94.919132</td>\n",
              "      <td>309.494019</td>\n",
              "      <td>-67.173162</td>\n",
              "      <td>302.598938</td>\n",
              "      <td>-100.009783</td>\n",
              "      <td>131272.515600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-97.723682</td>\n",
              "      <td>306.249634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.347653e+00</td>\n",
              "      <td>-0.028625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-28.865614</td>\n",
              "      <td>131684.814000</td>\n",
              "      <td>8.118812</td>\n",
              "      <td>131358.861500</td>\n",
              "      <td>13.401483</td>\n",
              "      <td>131760.033900</td>\n",
              "      <td>-23.514188</td>\n",
              "      <td>378.671480</td>\n",
              "      <td>1.885031</td>\n",
              "      <td>383.249230</td>\n",
              "      <td>6.881223</td>\n",
              "      <td>376.474160</td>\n",
              "      <td>-28.842695</td>\n",
              "      <td>131609.594200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.749101</td>\n",
              "      <td>380.319470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.011317e+01</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-29.665834</td>\n",
              "      <td>130020.265600</td>\n",
              "      <td>12.330052</td>\n",
              "      <td>129954.758400</td>\n",
              "      <td>10.077210</td>\n",
              "      <td>130080.124800</td>\n",
              "      <td>24.534053</td>\n",
              "      <td>383.615450</td>\n",
              "      <td>-5.729578</td>\n",
              "      <td>388.010090</td>\n",
              "      <td>-6.222322</td>\n",
              "      <td>...</td>\n",
              "      <td>7.969843</td>\n",
              "      <td>131634.667500</td>\n",
              "      <td>13.883972</td>\n",
              "      <td>132060.913100</td>\n",
              "      <td>-23.124577</td>\n",
              "      <td>376.997009</td>\n",
              "      <td>1.512609</td>\n",
              "      <td>380.868800</td>\n",
              "      <td>7.116394</td>\n",
              "      <td>375.009280</td>\n",
              "      <td>-28.742983</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.359489</td>\n",
              "      <td>378.353119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.017212e+01</td>\n",
              "      <td>0.015089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>68.096034</td>\n",
              "      <td>132186.279400</td>\n",
              "      <td>104.897113</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>85.324875</td>\n",
              "      <td>132261.499300</td>\n",
              "      <td>66.274028</td>\n",
              "      <td>456.676340</td>\n",
              "      <td>102.674037</td>\n",
              "      <td>460.338540</td>\n",
              "      <td>82.053286</td>\n",
              "      <td>454.295910</td>\n",
              "      <td>68.096034</td>\n",
              "      <td>132085.986400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.027656</td>\n",
              "      <td>457.775000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>14.667720</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.269372e+01</td>\n",
              "      <td>0.059942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69.035338</td>\n",
              "      <td>130932.615900</td>\n",
              "      <td>104.674992</td>\n",
              "      <td>130857.396100</td>\n",
              "      <td>81.577731</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>102.450263</td>\n",
              "      <td>461.986530</td>\n",
              "      <td>65.091252</td>\n",
              "      <td>465.751648</td>\n",
              "      <td>93.506712</td>\n",
              "      <td>...</td>\n",
              "      <td>105.040353</td>\n",
              "      <td>132325.828100</td>\n",
              "      <td>85.565517</td>\n",
              "      <td>132584.640600</td>\n",
              "      <td>66.849063</td>\n",
              "      <td>454.050049</td>\n",
              "      <td>102.620546</td>\n",
              "      <td>457.489014</td>\n",
              "      <td>82.167877</td>\n",
              "      <td>452.281700</td>\n",
              "      <td>68.321228</td>\n",
              "      <td>132467.359400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.531859</td>\n",
              "      <td>455.348968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.780624</td>\n",
              "      <td>7.992761</td>\n",
              "      <td>6.775070</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.277497e+01</td>\n",
              "      <td>0.057622</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>179.994691</td>\n",
              "      <td>151592.990400</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151567.917200</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151693.283500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1779.462980</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1265.656320</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151618.063700</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>46987.307980</td>\n",
              "      <td>179.467570</td>\n",
              "      <td>17501.142460</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>611.404290</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>723.467610</td>\n",
              "      <td>66.035000</td>\n",
              "      <td>3.720000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.125937</td>\n",
              "      <td>272394.000000</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>145336.656300</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>145167.796900</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>155526.781300</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1416.722070</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1275.910480</td>\n",
              "      <td>179.986276</td>\n",
              "      <td>...</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151749.687500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151924.062500</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1781.324341</td>\n",
              "      <td>179.991768</td>\n",
              "      <td>1266.205650</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1262.726560</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151859.328100</td>\n",
              "      <td>179.964297</td>\n",
              "      <td>45946.320310</td>\n",
              "      <td>179.914860</td>\n",
              "      <td>17351.042970</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1264.740770</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>610.038757</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>715.827942</td>\n",
              "      <td>62.226000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.106656</td>\n",
              "      <td>270336.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R1-PA1:VH       R1-PM1:V  ...    snort_log4  marker\n",
              "count   78377.000000   78377.000000  ...  78377.000000   78377\n",
              "unique           NaN            NaN  ...           NaN       2\n",
              "top              NaN            NaN  ...           NaN  Attack\n",
              "freq             NaN            NaN  ...           NaN   55663\n",
              "mean      -15.802424  130764.039577  ...      0.000077     NaN\n",
              "std       100.876750    8546.118477  ...      0.008749     NaN\n",
              "min      -179.988962       0.000000  ...      0.000000     NaN\n",
              "25%      -100.416583  131057.982300  ...      0.000000     NaN\n",
              "50%       -28.865614  131684.814000  ...      0.000000     NaN\n",
              "75%        68.096034  132186.279400  ...      0.000000     NaN\n",
              "max       179.994691  151592.990400  ...      1.000000     NaN\n",
              "\n",
              "[11 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4rnzbBKmYDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6876decd-25b9-4b81-ea2e-79abd0eea6fd"
      },
      "source": [
        "# Check if missing values\n",
        "df.isnull().values.any()\n",
        " \n",
        "# Check number of NaNs\n",
        "df.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAplIOhZmaSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#تحويل الكلمات إلى أرقام للعمود marker\n",
        "#target equal 0 is natural\n",
        "#target equal 1 is Attack\n",
        "df.loc[df[\"marker\"] == \"Natural\", \"marker\"] = 0\n",
        "df.loc[df[\"marker\"] ==\"Attack\", \"marker\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfO3UI65mdzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fe2078bd-00ae-40b7-feb8-2d5aef6a27cf"
      },
      "source": [
        "# إحصاء القيم\n",
        "df[\"marker\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    55663\n",
              "0    22714\n",
              "Name: marker, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_OLlaFKmf5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "217b4115-b0fb-4c1e-8273-30195b75a0ad"
      },
      "source": [
        "#سيعطي هذا مجموعة من الأماكن حيث توجد قيم NA.\n",
        " \n",
        "df[df==np.inf]=np.nan\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#ذا كانت بياناتك تحتوي على Nan ، فجرّب ما يلي:\n",
        "np.isnan(df.values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59MEg1Qfmjf7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert type float and string\n",
        "X= df.drop(\"marker\", axis = 1)\n",
        "X=X.astype(\"float\")\n",
        " \n",
        " \n",
        "from sklearn.preprocessing import Normalizer\n",
        "transformer= Normalizer().fit(X) \n",
        "transformer\n",
        "S=transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGCoeRfXmnny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "ce832e2d-6c74-4be5-df01-c126d2bb4f12"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        " \n",
        "ss = StandardScaler()\n",
        " \n",
        "x = pd.DataFrame(ss.fit_transform(S))\n",
        " \n",
        "y = df[\"marker\"]\n",
        " \n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.874304</td>\n",
              "      <td>0.099036</td>\n",
              "      <td>-0.474731</td>\n",
              "      <td>0.061580</td>\n",
              "      <td>-1.867958</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.824335</td>\n",
              "      <td>1.018163</td>\n",
              "      <td>-0.569834</td>\n",
              "      <td>1.564347</td>\n",
              "      <td>-1.946451</td>\n",
              "      <td>1.417422</td>\n",
              "      <td>0.874513</td>\n",
              "      <td>0.069631</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.819024</td>\n",
              "      <td>1.415316</td>\n",
              "      <td>1.806088</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-1.503010</td>\n",
              "      <td>-0.004960</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100647</td>\n",
              "      <td>-0.023363</td>\n",
              "      <td>0.205864</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.763126</td>\n",
              "      <td>0.129576</td>\n",
              "      <td>-0.593351</td>\n",
              "      <td>0.052249</td>\n",
              "      <td>-1.979122</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>-1.394714</td>\n",
              "      <td>1.193198</td>\n",
              "      <td>1.353806</td>\n",
              "      <td>1.556328</td>\n",
              "      <td>0.066997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063134</td>\n",
              "      <td>-0.412118</td>\n",
              "      <td>-0.061167</td>\n",
              "      <td>-1.434133</td>\n",
              "      <td>-0.019137</td>\n",
              "      <td>0.620453</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>-0.567535</td>\n",
              "      <td>0.131541</td>\n",
              "      <td>-1.941687</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.869845</td>\n",
              "      <td>-0.002144</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.814642</td>\n",
              "      <td>1.448899</td>\n",
              "      <td>1.834972</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>-1.538149</td>\n",
              "      <td>0.036196</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100102</td>\n",
              "      <td>-0.022051</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.889684</td>\n",
              "      <td>0.036331</td>\n",
              "      <td>-0.434350</td>\n",
              "      <td>-0.014128</td>\n",
              "      <td>-1.790412</td>\n",
              "      <td>-0.049095</td>\n",
              "      <td>0.869987</td>\n",
              "      <td>0.398801</td>\n",
              "      <td>-0.501111</td>\n",
              "      <td>0.701235</td>\n",
              "      <td>-1.836419</td>\n",
              "      <td>0.602633</td>\n",
              "      <td>0.889830</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864449</td>\n",
              "      <td>0.601895</td>\n",
              "      <td>1.868931</td>\n",
              "      <td>-0.007345</td>\n",
              "      <td>-1.400215</td>\n",
              "      <td>-0.008872</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020642</td>\n",
              "      <td>0.008042</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.801732</td>\n",
              "      <td>0.166731</td>\n",
              "      <td>-0.531031</td>\n",
              "      <td>0.101154</td>\n",
              "      <td>-1.876163</td>\n",
              "      <td>0.083426</td>\n",
              "      <td>-1.318168</td>\n",
              "      <td>0.481710</td>\n",
              "      <td>1.369793</td>\n",
              "      <td>0.693499</td>\n",
              "      <td>0.109440</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.377415</td>\n",
              "      <td>-0.134235</td>\n",
              "      <td>-1.374928</td>\n",
              "      <td>-0.094753</td>\n",
              "      <td>0.655781</td>\n",
              "      <td>0.075937</td>\n",
              "      <td>-0.498681</td>\n",
              "      <td>0.058612</td>\n",
              "      <td>-1.830469</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>0.885227</td>\n",
              "      <td>-0.079381</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860780</td>\n",
              "      <td>0.614187</td>\n",
              "      <td>1.864821</td>\n",
              "      <td>-0.011410</td>\n",
              "      <td>-1.411007</td>\n",
              "      <td>-0.005342</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019336</td>\n",
              "      <td>0.008870</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.890043</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.433884</td>\n",
              "      <td>-0.013045</td>\n",
              "      <td>-1.789712</td>\n",
              "      <td>-0.048005</td>\n",
              "      <td>0.869660</td>\n",
              "      <td>0.398513</td>\n",
              "      <td>-0.500733</td>\n",
              "      <td>0.700832</td>\n",
              "      <td>-1.835701</td>\n",
              "      <td>0.600059</td>\n",
              "      <td>0.890189</td>\n",
              "      <td>-0.002797</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864586</td>\n",
              "      <td>0.601516</td>\n",
              "      <td>1.857189</td>\n",
              "      <td>-0.007370</td>\n",
              "      <td>-1.381310</td>\n",
              "      <td>-0.008874</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020636</td>\n",
              "      <td>0.019097</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.802080</td>\n",
              "      <td>0.166042</td>\n",
              "      <td>-0.530669</td>\n",
              "      <td>0.101267</td>\n",
              "      <td>-1.875407</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>-1.317960</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>1.370322</td>\n",
              "      <td>0.691744</td>\n",
              "      <td>0.109383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>-0.376972</td>\n",
              "      <td>-0.133153</td>\n",
              "      <td>-1.374439</td>\n",
              "      <td>-0.097088</td>\n",
              "      <td>0.655930</td>\n",
              "      <td>0.075577</td>\n",
              "      <td>-0.498514</td>\n",
              "      <td>0.058768</td>\n",
              "      <td>-1.829935</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.885529</td>\n",
              "      <td>-0.078267</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860917</td>\n",
              "      <td>0.614905</td>\n",
              "      <td>1.879398</td>\n",
              "      <td>-0.011434</td>\n",
              "      <td>-1.418009</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019302</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.891902</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.429791</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-1.782334</td>\n",
              "      <td>-0.049518</td>\n",
              "      <td>0.871534</td>\n",
              "      <td>0.390431</td>\n",
              "      <td>-0.495338</td>\n",
              "      <td>0.682959</td>\n",
              "      <td>-1.830180</td>\n",
              "      <td>0.595921</td>\n",
              "      <td>0.892105</td>\n",
              "      <td>-0.007788</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.866303</td>\n",
              "      <td>0.590921</td>\n",
              "      <td>1.891025</td>\n",
              "      <td>-0.023903</td>\n",
              "      <td>-1.407753</td>\n",
              "      <td>-0.010136</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>0.010396</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.804544</td>\n",
              "      <td>0.167318</td>\n",
              "      <td>-0.525924</td>\n",
              "      <td>0.102684</td>\n",
              "      <td>-1.867290</td>\n",
              "      <td>0.085049</td>\n",
              "      <td>-1.314364</td>\n",
              "      <td>0.478504</td>\n",
              "      <td>1.372069</td>\n",
              "      <td>0.677139</td>\n",
              "      <td>0.111401</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>-0.373513</td>\n",
              "      <td>-0.137835</td>\n",
              "      <td>-1.368761</td>\n",
              "      <td>-0.095179</td>\n",
              "      <td>0.657141</td>\n",
              "      <td>0.074537</td>\n",
              "      <td>-0.493178</td>\n",
              "      <td>0.057356</td>\n",
              "      <td>-1.824012</td>\n",
              "      <td>0.047841</td>\n",
              "      <td>0.887439</td>\n",
              "      <td>-0.079738</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.862749</td>\n",
              "      <td>0.605183</td>\n",
              "      <td>1.892994</td>\n",
              "      <td>-0.027845</td>\n",
              "      <td>-1.432902</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019377</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893597</td>\n",
              "      <td>0.033239</td>\n",
              "      <td>-0.423901</td>\n",
              "      <td>-0.017844</td>\n",
              "      <td>-1.770758</td>\n",
              "      <td>-0.052923</td>\n",
              "      <td>0.868269</td>\n",
              "      <td>0.389099</td>\n",
              "      <td>-0.489603</td>\n",
              "      <td>0.661865</td>\n",
              "      <td>-1.824963</td>\n",
              "      <td>0.604001</td>\n",
              "      <td>0.893742</td>\n",
              "      <td>-0.007717</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864216</td>\n",
              "      <td>0.586934</td>\n",
              "      <td>1.884543</td>\n",
              "      <td>-0.063127</td>\n",
              "      <td>-1.447021</td>\n",
              "      <td>-0.012640</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020903</td>\n",
              "      <td>0.037047</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.807090</td>\n",
              "      <td>0.167550</td>\n",
              "      <td>-0.519189</td>\n",
              "      <td>0.103949</td>\n",
              "      <td>-1.854435</td>\n",
              "      <td>0.085472</td>\n",
              "      <td>-1.310773</td>\n",
              "      <td>0.475735</td>\n",
              "      <td>1.371320</td>\n",
              "      <td>0.654512</td>\n",
              "      <td>0.108615</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003178</td>\n",
              "      <td>-0.368471</td>\n",
              "      <td>-0.137439</td>\n",
              "      <td>-1.359878</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>0.654775</td>\n",
              "      <td>0.074295</td>\n",
              "      <td>-0.487400</td>\n",
              "      <td>0.055487</td>\n",
              "      <td>-1.819112</td>\n",
              "      <td>0.048351</td>\n",
              "      <td>0.889183</td>\n",
              "      <td>-0.083081</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860604</td>\n",
              "      <td>0.598972</td>\n",
              "      <td>1.887278</td>\n",
              "      <td>-0.063596</td>\n",
              "      <td>-1.489143</td>\n",
              "      <td>-0.052365</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019594</td>\n",
              "      <td>0.009072</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       125      126       127\n",
              "0  0.874304  0.099036 -0.474731  ... -0.007144 -0.00943 -0.008728\n",
              "1  0.889684  0.036331 -0.434350  ... -0.007144 -0.00943 -0.008728\n",
              "2  0.890043  0.037227 -0.433884  ... -0.007144 -0.00943 -0.008728\n",
              "3  0.891902  0.036003 -0.429791  ... -0.007144 -0.00943 -0.008728\n",
              "4  0.893597  0.033239 -0.423901  ... -0.007144 -0.00943 -0.008728\n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glBtUvjOmqfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "x=np.asarray(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE7tb-Q0ms98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert x to 3d\n",
        "d=np.reshape(x, x.shape + (1,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkFViKhCm07q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c1493223-4bf1-4d84-de38-80acc6759c29"
      },
      "source": [
        "d.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntWj_X1mm25b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = d.reshape(-1,8,16,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5Ac4QYvm_Ph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ee82e9ad-9f1f-483b-d410-8e90033d3b7f"
      },
      "source": [
        "# categorical target y to array \n",
        "from keras.utils.np_utils import to_categorical\n",
        " \n",
        "y_cat = to_categorical(y,2)\n",
        "y_cat[:10]\n",
        "y_cat.astype(\"int\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       ...,\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP4dgjolnBoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#divide datasets into to part training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(k, y_cat,test_size=0.1,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3lGmQSTrXWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e4159f9-d01a-4867-a7c0-828742378bb9"
      },
      "source": [
        "print(x_test.shape)\n",
        " \n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7838, 8, 16, 1)\n",
            "(70539, 8, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52sinxJfzb1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        " \n",
        "from keras.layers import Conv2D, MaxPooling2D # convolution layers\n",
        "from keras.layers import Dense, Dropout, Flatten    # core layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkFM9C3DyztB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "df05d0a9-b8eb-4531-ff57-e851d2cb911d"
      },
      "source": [
        " \n",
        "model = Sequential()\n",
        " \n",
        "model.add(Conv2D(filters = 64,padding=\"same\", kernel_size = (5, 5), activation='tanh',\n",
        "                 input_shape = (8, 16, 1)))\n",
        "model.add(Conv2D(filters = 64,padding=\"same\", kernel_size = (7, 7), activation='tanh'))\n",
        "model.add(Conv2D(filters = 128,padding=\"same\", kernel_size = (7, 7),strides=(2,2),  activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(3,3),padding=\"same\",strides=(2,2)  ))\n",
        "model.add(Dropout(0.1))\n",
        " \n",
        "model.add(Conv2D(filters = 64,padding=\"same\", kernel_size = (5, 5), activation='tanh'))\n",
        "model.add(Conv2D(filters = 64,padding=\"same\", kernel_size = (5, 5),strides=(2,2) , activation='tanh'))\n",
        "model.add(MaxPooling2D(pool_size=(3,3),padding=\"same\",strides=(2,2)))\n",
        " \n",
        " \n",
        " \n",
        " \n",
        " \n",
        "model.add(Conv2D(filters = 128,padding=\"same\", kernel_size = (3, 3), activation='tanh'))\n",
        " \n",
        " \n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='tanh',kernel_initializer=\"he_uniform\"))\n",
        "model.add(Dense(2, activation='sigmoid',kernel_initializer=\"he_uniform\"))\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer =tf.keras.optimizers.Adam(0.001,decay=0.000005), metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 8, 16, 64)         1664      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 16, 64)         200768    \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 4, 8, 128)         401536    \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 2, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 2, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 2, 4, 64)          204864    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 1, 2, 64)          102464    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 1, 1, 128)         73856     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 1,052,226\n",
            "Trainable params: 1,052,226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c80pyM6jn6eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "66f7ae02-5789-4799-a041-88ef28fd5cd3"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-M9s7R8n90Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau,LearningRateScheduler\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_cnn2d_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "rp = ReduceLROnPlateau(monitor='val_accuracy',patience = 5,verbose=1,factor=0.5,min_lr=0.00001)\n",
        " \n",
        " \n",
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK2DMdzzCED0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6721340-38e7-41aa-d79e-7895d9ab5ed6"
      },
      "source": [
        " \n",
        "fit1 = model.fit(\n",
        "    x_train, y_train,validation_split=0.1,batch_size=500,epochs=250, verbose=1,callbacks=[es, mc,rp])\n",
        "score=model.evaluate(x_test,y_test)\n",
        "print('Accuracy(on Test-data): ' + str(score[1]))\n",
        "duration = time.time() - start_time\n",
        "print(\"time of training (s)\", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7097\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71364, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 26ms/step - loss: 0.5927 - accuracy: 0.7097 - val_loss: 0.5847 - val_accuracy: 0.7136\n",
            "Epoch 2/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5814 - accuracy: 0.7128\n",
            "Epoch 00002: val_accuracy improved from 0.71364 to 0.71945, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5814 - accuracy: 0.7128 - val_loss: 0.5760 - val_accuracy: 0.7194\n",
            "Epoch 3/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5723 - accuracy: 0.7173\n",
            "Epoch 00003: val_accuracy did not improve from 0.71945\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5723 - accuracy: 0.7173 - val_loss: 0.5705 - val_accuracy: 0.7173\n",
            "Epoch 4/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.7226\n",
            "Epoch 00004: val_accuracy improved from 0.71945 to 0.72966, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5627 - accuracy: 0.7226 - val_loss: 0.5546 - val_accuracy: 0.7297\n",
            "Epoch 5/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5513 - accuracy: 0.7304\n",
            "Epoch 00005: val_accuracy improved from 0.72966 to 0.73150, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5513 - accuracy: 0.7304 - val_loss: 0.5569 - val_accuracy: 0.7315\n",
            "Epoch 6/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5464 - accuracy: 0.7332\n",
            "Epoch 00006: val_accuracy did not improve from 0.73150\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5464 - accuracy: 0.7332 - val_loss: 0.5519 - val_accuracy: 0.7272\n",
            "Epoch 7/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7368\n",
            "Epoch 00007: val_accuracy improved from 0.73150 to 0.73703, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5378 - accuracy: 0.7369 - val_loss: 0.5433 - val_accuracy: 0.7370\n",
            "Epoch 8/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5331 - accuracy: 0.7404\n",
            "Epoch 00008: val_accuracy did not improve from 0.73703\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5331 - accuracy: 0.7404 - val_loss: 0.5339 - val_accuracy: 0.7365\n",
            "Epoch 9/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.7455\n",
            "Epoch 00009: val_accuracy improved from 0.73703 to 0.73760, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5248 - accuracy: 0.7455 - val_loss: 0.5320 - val_accuracy: 0.7376\n",
            "Epoch 10/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.7490\n",
            "Epoch 00010: val_accuracy improved from 0.73760 to 0.74624, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5174 - accuracy: 0.7490 - val_loss: 0.5230 - val_accuracy: 0.7462\n",
            "Epoch 11/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.5112 - accuracy: 0.7528\n",
            "Epoch 00011: val_accuracy did not improve from 0.74624\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5115 - accuracy: 0.7527 - val_loss: 0.5300 - val_accuracy: 0.7428\n",
            "Epoch 12/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7563\n",
            "Epoch 00012: val_accuracy improved from 0.74624 to 0.74709, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.5050 - accuracy: 0.7563 - val_loss: 0.5186 - val_accuracy: 0.7471\n",
            "Epoch 13/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4973 - accuracy: 0.7598\n",
            "Epoch 00013: val_accuracy did not improve from 0.74709\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4973 - accuracy: 0.7598 - val_loss: 0.5177 - val_accuracy: 0.7431\n",
            "Epoch 14/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4911 - accuracy: 0.7631\n",
            "Epoch 00014: val_accuracy improved from 0.74709 to 0.74724, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4911 - accuracy: 0.7631 - val_loss: 0.5210 - val_accuracy: 0.7472\n",
            "Epoch 15/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4833 - accuracy: 0.7663\n",
            "Epoch 00015: val_accuracy improved from 0.74724 to 0.75347, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4833 - accuracy: 0.7663 - val_loss: 0.5085 - val_accuracy: 0.7535\n",
            "Epoch 16/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.7695\n",
            "Epoch 00016: val_accuracy improved from 0.75347 to 0.75617, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4752 - accuracy: 0.7695 - val_loss: 0.4993 - val_accuracy: 0.7562\n",
            "Epoch 17/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4681 - accuracy: 0.7742\n",
            "Epoch 00017: val_accuracy improved from 0.75617 to 0.76056, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4681 - accuracy: 0.7742 - val_loss: 0.4996 - val_accuracy: 0.7606\n",
            "Epoch 18/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.7773\n",
            "Epoch 00018: val_accuracy did not improve from 0.76056\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4618 - accuracy: 0.7773 - val_loss: 0.5033 - val_accuracy: 0.7574\n",
            "Epoch 19/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4508 - accuracy: 0.7827\n",
            "Epoch 00019: val_accuracy improved from 0.76056 to 0.76113, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4508 - accuracy: 0.7827 - val_loss: 0.4981 - val_accuracy: 0.7611\n",
            "Epoch 20/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4454 - accuracy: 0.7855\n",
            "Epoch 00020: val_accuracy improved from 0.76113 to 0.76297, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4454 - accuracy: 0.7855 - val_loss: 0.4910 - val_accuracy: 0.7630\n",
            "Epoch 21/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.7903\n",
            "Epoch 00021: val_accuracy improved from 0.76297 to 0.76793, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4365 - accuracy: 0.7901 - val_loss: 0.4803 - val_accuracy: 0.7679\n",
            "Epoch 22/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4307 - accuracy: 0.7928\n",
            "Epoch 00022: val_accuracy improved from 0.76793 to 0.76822, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4308 - accuracy: 0.7926 - val_loss: 0.4828 - val_accuracy: 0.7682\n",
            "Epoch 23/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.7960\n",
            "Epoch 00023: val_accuracy did not improve from 0.76822\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4265 - accuracy: 0.7960 - val_loss: 0.4815 - val_accuracy: 0.7658\n",
            "Epoch 24/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.8000\n",
            "Epoch 00024: val_accuracy improved from 0.76822 to 0.77048, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4183 - accuracy: 0.8000 - val_loss: 0.4698 - val_accuracy: 0.7705\n",
            "Epoch 25/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4102 - accuracy: 0.8033\n",
            "Epoch 00025: val_accuracy improved from 0.77048 to 0.77375, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4102 - accuracy: 0.8033 - val_loss: 0.4680 - val_accuracy: 0.7737\n",
            "Epoch 26/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.8079\n",
            "Epoch 00026: val_accuracy did not improve from 0.77375\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.4042 - accuracy: 0.8079 - val_loss: 0.4791 - val_accuracy: 0.7715\n",
            "Epoch 27/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8086\n",
            "Epoch 00027: val_accuracy improved from 0.77375 to 0.78225, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3985 - accuracy: 0.8086 - val_loss: 0.4648 - val_accuracy: 0.7823\n",
            "Epoch 28/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3910 - accuracy: 0.8134\n",
            "Epoch 00028: val_accuracy did not improve from 0.78225\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3910 - accuracy: 0.8134 - val_loss: 0.4687 - val_accuracy: 0.7797\n",
            "Epoch 29/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3867 - accuracy: 0.8158\n",
            "Epoch 00029: val_accuracy did not improve from 0.78225\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3867 - accuracy: 0.8158 - val_loss: 0.4747 - val_accuracy: 0.7784\n",
            "Epoch 30/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8200\n",
            "Epoch 00030: val_accuracy improved from 0.78225 to 0.78792, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3777 - accuracy: 0.8200 - val_loss: 0.4625 - val_accuracy: 0.7879\n",
            "Epoch 31/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8231\n",
            "Epoch 00031: val_accuracy improved from 0.78792 to 0.79402, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3735 - accuracy: 0.8231 - val_loss: 0.4506 - val_accuracy: 0.7940\n",
            "Epoch 32/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8269\n",
            "Epoch 00032: val_accuracy improved from 0.79402 to 0.79458, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3629 - accuracy: 0.8269 - val_loss: 0.4549 - val_accuracy: 0.7946\n",
            "Epoch 33/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8294\n",
            "Epoch 00033: val_accuracy improved from 0.79458 to 0.79699, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3613 - accuracy: 0.8294 - val_loss: 0.4497 - val_accuracy: 0.7970\n",
            "Epoch 34/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3534 - accuracy: 0.8340\n",
            "Epoch 00034: val_accuracy did not improve from 0.79699\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3534 - accuracy: 0.8340 - val_loss: 0.4491 - val_accuracy: 0.7961\n",
            "Epoch 35/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3464 - accuracy: 0.8364\n",
            "Epoch 00035: val_accuracy did not improve from 0.79699\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3464 - accuracy: 0.8364 - val_loss: 0.4495 - val_accuracy: 0.7964\n",
            "Epoch 36/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.8379\n",
            "Epoch 00036: val_accuracy improved from 0.79699 to 0.80224, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3405 - accuracy: 0.8379 - val_loss: 0.4503 - val_accuracy: 0.8022\n",
            "Epoch 37/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8420\n",
            "Epoch 00037: val_accuracy did not improve from 0.80224\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3364 - accuracy: 0.8420 - val_loss: 0.4445 - val_accuracy: 0.7961\n",
            "Epoch 38/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.8461\n",
            "Epoch 00038: val_accuracy improved from 0.80224 to 0.81131, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3297 - accuracy: 0.8461 - val_loss: 0.4286 - val_accuracy: 0.8113\n",
            "Epoch 39/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8449\n",
            "Epoch 00039: val_accuracy did not improve from 0.81131\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3289 - accuracy: 0.8449 - val_loss: 0.4301 - val_accuracy: 0.8022\n",
            "Epoch 40/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3245 - accuracy: 0.8456\n",
            "Epoch 00040: val_accuracy did not improve from 0.81131\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3245 - accuracy: 0.8456 - val_loss: 0.4426 - val_accuracy: 0.8014\n",
            "Epoch 41/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.8499\n",
            "Epoch 00041: val_accuracy did not improve from 0.81131\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3184 - accuracy: 0.8499 - val_loss: 0.4431 - val_accuracy: 0.7994\n",
            "Epoch 42/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3164 - accuracy: 0.8523\n",
            "Epoch 00042: val_accuracy did not improve from 0.81131\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3164 - accuracy: 0.8523 - val_loss: 0.4409 - val_accuracy: 0.8017\n",
            "Epoch 43/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8550\n",
            "Epoch 00043: val_accuracy improved from 0.81131 to 0.81216, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3101 - accuracy: 0.8550 - val_loss: 0.4231 - val_accuracy: 0.8122\n",
            "Epoch 44/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3080 - accuracy: 0.8554\n",
            "Epoch 00044: val_accuracy improved from 0.81216 to 0.81571, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3080 - accuracy: 0.8554 - val_loss: 0.4086 - val_accuracy: 0.8157\n",
            "Epoch 45/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8603\n",
            "Epoch 00045: val_accuracy did not improve from 0.81571\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3010 - accuracy: 0.8603 - val_loss: 0.4172 - val_accuracy: 0.8106\n",
            "Epoch 46/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8587\n",
            "Epoch 00046: val_accuracy did not improve from 0.81571\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.3008 - accuracy: 0.8587 - val_loss: 0.4361 - val_accuracy: 0.8137\n",
            "Epoch 47/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2988 - accuracy: 0.8608\n",
            "Epoch 00047: val_accuracy did not improve from 0.81571\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2988 - accuracy: 0.8608 - val_loss: 0.4293 - val_accuracy: 0.8151\n",
            "Epoch 48/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.8644\n",
            "Epoch 00048: val_accuracy did not improve from 0.81571\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2935 - accuracy: 0.8644 - val_loss: 0.4276 - val_accuracy: 0.8132\n",
            "Epoch 49/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8668\n",
            "Epoch 00049: val_accuracy improved from 0.81571 to 0.82195, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2872 - accuracy: 0.8668 - val_loss: 0.4268 - val_accuracy: 0.8219\n",
            "Epoch 50/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2885 - accuracy: 0.8660\n",
            "Epoch 00050: val_accuracy improved from 0.82195 to 0.82209, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2885 - accuracy: 0.8660 - val_loss: 0.4072 - val_accuracy: 0.8221\n",
            "Epoch 51/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2830 - accuracy: 0.8685\n",
            "Epoch 00051: val_accuracy improved from 0.82209 to 0.82379, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2830 - accuracy: 0.8685 - val_loss: 0.4148 - val_accuracy: 0.8238\n",
            "Epoch 52/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.8702\n",
            "Epoch 00052: val_accuracy improved from 0.82379 to 0.82606, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2805 - accuracy: 0.8699 - val_loss: 0.4098 - val_accuracy: 0.8261\n",
            "Epoch 53/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.8727\n",
            "Epoch 00053: val_accuracy improved from 0.82606 to 0.82620, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2787 - accuracy: 0.8727 - val_loss: 0.4080 - val_accuracy: 0.8262\n",
            "Epoch 54/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2755 - accuracy: 0.8720\n",
            "Epoch 00054: val_accuracy did not improve from 0.82620\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2755 - accuracy: 0.8720 - val_loss: 0.4123 - val_accuracy: 0.8222\n",
            "Epoch 55/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.8753\n",
            "Epoch 00055: val_accuracy did not improve from 0.82620\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2694 - accuracy: 0.8753 - val_loss: 0.4240 - val_accuracy: 0.8249\n",
            "Epoch 56/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.8770\n",
            "Epoch 00056: val_accuracy did not improve from 0.82620\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2684 - accuracy: 0.8770 - val_loss: 0.4116 - val_accuracy: 0.8239\n",
            "Epoch 57/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2665 - accuracy: 0.8772\n",
            "Epoch 00057: val_accuracy did not improve from 0.82620\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.2665 - accuracy: 0.8772 - val_loss: 0.4063 - val_accuracy: 0.8225\n",
            "Epoch 58/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2594 - accuracy: 0.8806\n",
            "Epoch 00058: val_accuracy improved from 0.82620 to 0.83017, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2594 - accuracy: 0.8806 - val_loss: 0.4118 - val_accuracy: 0.8302\n",
            "Epoch 59/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.8807\n",
            "Epoch 00059: val_accuracy improved from 0.83017 to 0.83343, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2578 - accuracy: 0.8807 - val_loss: 0.4053 - val_accuracy: 0.8334\n",
            "Epoch 60/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2558 - accuracy: 0.8818\n",
            "Epoch 00060: val_accuracy improved from 0.83343 to 0.83357, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2558 - accuracy: 0.8818 - val_loss: 0.4087 - val_accuracy: 0.8336\n",
            "Epoch 61/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2539 - accuracy: 0.8826\n",
            "Epoch 00061: val_accuracy did not improve from 0.83357\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2539 - accuracy: 0.8826 - val_loss: 0.4089 - val_accuracy: 0.8317\n",
            "Epoch 62/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2557 - accuracy: 0.8818\n",
            "Epoch 00062: val_accuracy did not improve from 0.83357\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2557 - accuracy: 0.8818 - val_loss: 0.4159 - val_accuracy: 0.8303\n",
            "Epoch 63/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.8831\n",
            "Epoch 00063: val_accuracy did not improve from 0.83357\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2581 - accuracy: 0.8831 - val_loss: 0.4244 - val_accuracy: 0.8290\n",
            "Epoch 64/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2522 - accuracy: 0.8840\n",
            "Epoch 00064: val_accuracy improved from 0.83357 to 0.83428, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2522 - accuracy: 0.8840 - val_loss: 0.4205 - val_accuracy: 0.8343\n",
            "Epoch 65/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.8858\n",
            "Epoch 00065: val_accuracy improved from 0.83428 to 0.84250, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2490 - accuracy: 0.8858 - val_loss: 0.4000 - val_accuracy: 0.8425\n",
            "Epoch 66/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.8860\n",
            "Epoch 00066: val_accuracy did not improve from 0.84250\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2490 - accuracy: 0.8860 - val_loss: 0.3998 - val_accuracy: 0.8351\n",
            "Epoch 67/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2443 - accuracy: 0.8893\n",
            "Epoch 00067: val_accuracy did not improve from 0.84250\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2443 - accuracy: 0.8893 - val_loss: 0.4129 - val_accuracy: 0.8339\n",
            "Epoch 68/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.8880\n",
            "Epoch 00068: val_accuracy did not improve from 0.84250\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2449 - accuracy: 0.8880 - val_loss: 0.4098 - val_accuracy: 0.8374\n",
            "Epoch 69/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.8891\n",
            "Epoch 00069: val_accuracy did not improve from 0.84250\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2431 - accuracy: 0.8891 - val_loss: 0.4180 - val_accuracy: 0.8310\n",
            "Epoch 70/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2389 - accuracy: 0.8901\n",
            "Epoch 00070: val_accuracy improved from 0.84250 to 0.84562, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2389 - accuracy: 0.8901 - val_loss: 0.4019 - val_accuracy: 0.8456\n",
            "Epoch 71/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2373 - accuracy: 0.8906\n",
            "Epoch 00071: val_accuracy did not improve from 0.84562\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2373 - accuracy: 0.8906 - val_loss: 0.4103 - val_accuracy: 0.8346\n",
            "Epoch 72/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2375 - accuracy: 0.8920\n",
            "Epoch 00072: val_accuracy did not improve from 0.84562\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2375 - accuracy: 0.8920 - val_loss: 0.4007 - val_accuracy: 0.8411\n",
            "Epoch 73/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2360 - accuracy: 0.8916\n",
            "Epoch 00073: val_accuracy did not improve from 0.84562\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.2360 - accuracy: 0.8916 - val_loss: 0.4129 - val_accuracy: 0.8331\n",
            "Epoch 74/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2324 - accuracy: 0.8951\n",
            "Epoch 00074: val_accuracy improved from 0.84562 to 0.84803, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2324 - accuracy: 0.8951 - val_loss: 0.3915 - val_accuracy: 0.8480\n",
            "Epoch 75/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2294 - accuracy: 0.8961\n",
            "Epoch 00075: val_accuracy did not improve from 0.84803\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2294 - accuracy: 0.8961 - val_loss: 0.4044 - val_accuracy: 0.8439\n",
            "Epoch 76/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.8936\n",
            "Epoch 00076: val_accuracy did not improve from 0.84803\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2317 - accuracy: 0.8936 - val_loss: 0.4122 - val_accuracy: 0.8424\n",
            "Epoch 77/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.8987\n",
            "Epoch 00077: val_accuracy did not improve from 0.84803\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2255 - accuracy: 0.8987 - val_loss: 0.4027 - val_accuracy: 0.8435\n",
            "Epoch 78/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2286 - accuracy: 0.8956\n",
            "Epoch 00078: val_accuracy did not improve from 0.84803\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2286 - accuracy: 0.8956 - val_loss: 0.4071 - val_accuracy: 0.8473\n",
            "Epoch 79/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2246 - accuracy: 0.8979\n",
            "Epoch 00079: val_accuracy did not improve from 0.84803\n",
            "\n",
            "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.2246 - accuracy: 0.8979 - val_loss: 0.4118 - val_accuracy: 0.8363\n",
            "Epoch 80/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1909 - accuracy: 0.9134\n",
            "Epoch 00080: val_accuracy improved from 0.84803 to 0.86150, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1909 - accuracy: 0.9134 - val_loss: 0.4054 - val_accuracy: 0.8615\n",
            "Epoch 81/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9224\n",
            "Epoch 00081: val_accuracy improved from 0.86150 to 0.86646, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1711 - accuracy: 0.9224 - val_loss: 0.3982 - val_accuracy: 0.8665\n",
            "Epoch 82/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9244\n",
            "Epoch 00082: val_accuracy did not improve from 0.86646\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1687 - accuracy: 0.9244 - val_loss: 0.4075 - val_accuracy: 0.8635\n",
            "Epoch 83/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9256\n",
            "Epoch 00083: val_accuracy did not improve from 0.86646\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1657 - accuracy: 0.9256 - val_loss: 0.4041 - val_accuracy: 0.8653\n",
            "Epoch 84/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9263\n",
            "Epoch 00084: val_accuracy did not improve from 0.86646\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1657 - accuracy: 0.9263 - val_loss: 0.4120 - val_accuracy: 0.8618\n",
            "Epoch 85/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9296\n",
            "Epoch 00085: val_accuracy improved from 0.86646 to 0.86887, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1563 - accuracy: 0.9296 - val_loss: 0.4161 - val_accuracy: 0.8689\n",
            "Epoch 86/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9302\n",
            "Epoch 00086: val_accuracy did not improve from 0.86887\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1556 - accuracy: 0.9302 - val_loss: 0.4083 - val_accuracy: 0.8672\n",
            "Epoch 87/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9301\n",
            "Epoch 00087: val_accuracy did not improve from 0.86887\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1581 - accuracy: 0.9301 - val_loss: 0.4144 - val_accuracy: 0.8665\n",
            "Epoch 88/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9312\n",
            "Epoch 00088: val_accuracy improved from 0.86887 to 0.87397, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1529 - accuracy: 0.9312 - val_loss: 0.4131 - val_accuracy: 0.8740\n",
            "Epoch 89/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9331\n",
            "Epoch 00089: val_accuracy did not improve from 0.87397\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1499 - accuracy: 0.9331 - val_loss: 0.4182 - val_accuracy: 0.8720\n",
            "Epoch 90/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9311\n",
            "Epoch 00090: val_accuracy did not improve from 0.87397\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1518 - accuracy: 0.9311 - val_loss: 0.4113 - val_accuracy: 0.8724\n",
            "Epoch 91/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9331\n",
            "Epoch 00091: val_accuracy did not improve from 0.87397\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1492 - accuracy: 0.9331 - val_loss: 0.4198 - val_accuracy: 0.8740\n",
            "Epoch 92/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9340\n",
            "Epoch 00092: val_accuracy did not improve from 0.87397\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1475 - accuracy: 0.9340 - val_loss: 0.4260 - val_accuracy: 0.8730\n",
            "Epoch 93/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9315\n",
            "Epoch 00093: val_accuracy improved from 0.87397 to 0.87567, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1512 - accuracy: 0.9315 - val_loss: 0.4128 - val_accuracy: 0.8757\n",
            "Epoch 94/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9345\n",
            "Epoch 00094: val_accuracy did not improve from 0.87567\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1444 - accuracy: 0.9345 - val_loss: 0.4231 - val_accuracy: 0.8741\n",
            "Epoch 95/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9369\n",
            "Epoch 00095: val_accuracy improved from 0.87567 to 0.87709, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1414 - accuracy: 0.9369 - val_loss: 0.4352 - val_accuracy: 0.8771\n",
            "Epoch 96/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9365\n",
            "Epoch 00096: val_accuracy did not improve from 0.87709\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1422 - accuracy: 0.9365 - val_loss: 0.4301 - val_accuracy: 0.8760\n",
            "Epoch 97/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9381\n",
            "Epoch 00097: val_accuracy did not improve from 0.87709\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1392 - accuracy: 0.9381 - val_loss: 0.4228 - val_accuracy: 0.8750\n",
            "Epoch 98/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9384\n",
            "Epoch 00098: val_accuracy did not improve from 0.87709\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1399 - accuracy: 0.9384 - val_loss: 0.4226 - val_accuracy: 0.8767\n",
            "Epoch 99/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9380\n",
            "Epoch 00099: val_accuracy improved from 0.87709 to 0.87851, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1399 - accuracy: 0.9380 - val_loss: 0.4285 - val_accuracy: 0.8785\n",
            "Epoch 100/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9395\n",
            "Epoch 00100: val_accuracy improved from 0.87851 to 0.88021, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1357 - accuracy: 0.9395 - val_loss: 0.4236 - val_accuracy: 0.8802\n",
            "Epoch 101/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9391\n",
            "Epoch 00101: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1357 - accuracy: 0.9391 - val_loss: 0.4334 - val_accuracy: 0.8769\n",
            "Epoch 102/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9412\n",
            "Epoch 00102: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1330 - accuracy: 0.9412 - val_loss: 0.4355 - val_accuracy: 0.8777\n",
            "Epoch 103/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9411\n",
            "Epoch 00103: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1338 - accuracy: 0.9411 - val_loss: 0.4241 - val_accuracy: 0.8751\n",
            "Epoch 104/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9406\n",
            "Epoch 00104: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.1354 - accuracy: 0.9406 - val_loss: 0.4246 - val_accuracy: 0.8799\n",
            "Epoch 105/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9412\n",
            "Epoch 00105: val_accuracy did not improve from 0.88021\n",
            "\n",
            "Epoch 00105: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1317 - accuracy: 0.9412 - val_loss: 0.4296 - val_accuracy: 0.8796\n",
            "Epoch 106/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9493\n",
            "Epoch 00106: val_accuracy improved from 0.88021 to 0.88843, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1141 - accuracy: 0.9493 - val_loss: 0.4447 - val_accuracy: 0.8884\n",
            "Epoch 107/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9522\n",
            "Epoch 00107: val_accuracy improved from 0.88843 to 0.88857, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.1058 - accuracy: 0.9522 - val_loss: 0.4509 - val_accuracy: 0.8886\n",
            "Epoch 108/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0996 - accuracy: 0.9555\n",
            "Epoch 00108: val_accuracy improved from 0.88857 to 0.89183, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0996 - accuracy: 0.9555 - val_loss: 0.4565 - val_accuracy: 0.8918\n",
            "Epoch 109/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9567\n",
            "Epoch 00109: val_accuracy did not improve from 0.89183\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.0992 - accuracy: 0.9567 - val_loss: 0.4563 - val_accuracy: 0.8907\n",
            "Epoch 110/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9559\n",
            "Epoch 00110: val_accuracy improved from 0.89183 to 0.89311, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0997 - accuracy: 0.9559 - val_loss: 0.4626 - val_accuracy: 0.8931\n",
            "Epoch 111/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9576\n",
            "Epoch 00111: val_accuracy did not improve from 0.89311\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0963 - accuracy: 0.9576 - val_loss: 0.4642 - val_accuracy: 0.8925\n",
            "Epoch 112/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9593\n",
            "Epoch 00112: val_accuracy improved from 0.89311 to 0.89339, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0937 - accuracy: 0.9593 - val_loss: 0.4771 - val_accuracy: 0.8934\n",
            "Epoch 113/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0967 - accuracy: 0.9586\n",
            "Epoch 00113: val_accuracy did not improve from 0.89339\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0967 - accuracy: 0.9586 - val_loss: 0.4612 - val_accuracy: 0.8927\n",
            "Epoch 114/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9581\n",
            "Epoch 00114: val_accuracy improved from 0.89339 to 0.89495, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0946 - accuracy: 0.9581 - val_loss: 0.4637 - val_accuracy: 0.8950\n",
            "Epoch 115/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0935 - accuracy: 0.9589\n",
            "Epoch 00115: val_accuracy did not improve from 0.89495\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0935 - accuracy: 0.9589 - val_loss: 0.4736 - val_accuracy: 0.8924\n",
            "Epoch 116/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0930 - accuracy: 0.9598\n",
            "Epoch 00116: val_accuracy did not improve from 0.89495\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0931 - accuracy: 0.9598 - val_loss: 0.4710 - val_accuracy: 0.8940\n",
            "Epoch 117/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9597\n",
            "Epoch 00117: val_accuracy improved from 0.89495 to 0.89722, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0914 - accuracy: 0.9597 - val_loss: 0.4853 - val_accuracy: 0.8972\n",
            "Epoch 118/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9584\n",
            "Epoch 00118: val_accuracy did not improve from 0.89722\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0934 - accuracy: 0.9584 - val_loss: 0.4831 - val_accuracy: 0.8927\n",
            "Epoch 119/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9600\n",
            "Epoch 00119: val_accuracy did not improve from 0.89722\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0904 - accuracy: 0.9600 - val_loss: 0.4892 - val_accuracy: 0.8933\n",
            "Epoch 120/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9601\n",
            "Epoch 00120: val_accuracy did not improve from 0.89722\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0915 - accuracy: 0.9601 - val_loss: 0.4761 - val_accuracy: 0.8911\n",
            "Epoch 121/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9610\n",
            "Epoch 00121: val_accuracy did not improve from 0.89722\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0903 - accuracy: 0.9610 - val_loss: 0.4759 - val_accuracy: 0.8928\n",
            "Epoch 122/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9615\n",
            "Epoch 00122: val_accuracy did not improve from 0.89722\n",
            "\n",
            "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0876 - accuracy: 0.9615 - val_loss: 0.4864 - val_accuracy: 0.8948\n",
            "Epoch 123/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9644\n",
            "Epoch 00123: val_accuracy improved from 0.89722 to 0.89779, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0797 - accuracy: 0.9644 - val_loss: 0.4841 - val_accuracy: 0.8978\n",
            "Epoch 124/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9664\n",
            "Epoch 00124: val_accuracy did not improve from 0.89779\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0755 - accuracy: 0.9664 - val_loss: 0.4961 - val_accuracy: 0.8976\n",
            "Epoch 125/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9679\n",
            "Epoch 00125: val_accuracy improved from 0.89779 to 0.89991, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0725 - accuracy: 0.9679 - val_loss: 0.5063 - val_accuracy: 0.8999\n",
            "Epoch 126/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9670\n",
            "Epoch 00126: val_accuracy improved from 0.89991 to 0.90020, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0740 - accuracy: 0.9670 - val_loss: 0.5028 - val_accuracy: 0.9002\n",
            "Epoch 127/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9681\n",
            "Epoch 00127: val_accuracy did not improve from 0.90020\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0724 - accuracy: 0.9681 - val_loss: 0.5046 - val_accuracy: 0.8965\n",
            "Epoch 128/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9684\n",
            "Epoch 00128: val_accuracy improved from 0.90020 to 0.90091, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0719 - accuracy: 0.9684 - val_loss: 0.5085 - val_accuracy: 0.9009\n",
            "Epoch 129/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9689\n",
            "Epoch 00129: val_accuracy improved from 0.90091 to 0.90261, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0712 - accuracy: 0.9689 - val_loss: 0.5138 - val_accuracy: 0.9026\n",
            "Epoch 130/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9684\n",
            "Epoch 00130: val_accuracy improved from 0.90261 to 0.90275, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0729 - accuracy: 0.9684 - val_loss: 0.5046 - val_accuracy: 0.9028\n",
            "Epoch 131/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9687\n",
            "Epoch 00131: val_accuracy did not improve from 0.90275\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0702 - accuracy: 0.9687 - val_loss: 0.5164 - val_accuracy: 0.8998\n",
            "Epoch 132/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9694\n",
            "Epoch 00132: val_accuracy did not improve from 0.90275\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0700 - accuracy: 0.9694 - val_loss: 0.5178 - val_accuracy: 0.9016\n",
            "Epoch 133/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9692\n",
            "Epoch 00133: val_accuracy did not improve from 0.90275\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0703 - accuracy: 0.9692 - val_loss: 0.5134 - val_accuracy: 0.9015\n",
            "Epoch 134/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0693 - accuracy: 0.9699\n",
            "Epoch 00134: val_accuracy did not improve from 0.90275\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0692 - accuracy: 0.9700 - val_loss: 0.5069 - val_accuracy: 0.9020\n",
            "Epoch 135/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9689\n",
            "Epoch 00135: val_accuracy did not improve from 0.90275\n",
            "\n",
            "Epoch 00135: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0717 - accuracy: 0.9689 - val_loss: 0.5066 - val_accuracy: 0.8989\n",
            "Epoch 136/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9714\n",
            "Epoch 00136: val_accuracy improved from 0.90275 to 0.90346, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0650 - accuracy: 0.9714 - val_loss: 0.5174 - val_accuracy: 0.9035\n",
            "Epoch 137/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9728\n",
            "Epoch 00137: val_accuracy did not improve from 0.90346\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0639 - accuracy: 0.9728 - val_loss: 0.5197 - val_accuracy: 0.9020\n",
            "Epoch 138/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9732\n",
            "Epoch 00138: val_accuracy did not improve from 0.90346\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0627 - accuracy: 0.9732 - val_loss: 0.5272 - val_accuracy: 0.9030\n",
            "Epoch 139/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9735\n",
            "Epoch 00139: val_accuracy improved from 0.90346 to 0.90459, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0621 - accuracy: 0.9735 - val_loss: 0.5294 - val_accuracy: 0.9046\n",
            "Epoch 140/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9726\n",
            "Epoch 00140: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0628 - accuracy: 0.9726 - val_loss: 0.5289 - val_accuracy: 0.9033\n",
            "Epoch 141/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9738\n",
            "Epoch 00141: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0601 - accuracy: 0.9738 - val_loss: 0.5371 - val_accuracy: 0.9022\n",
            "Epoch 142/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9734\n",
            "Epoch 00142: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0616 - accuracy: 0.9734 - val_loss: 0.5358 - val_accuracy: 0.9035\n",
            "Epoch 143/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9736\n",
            "Epoch 00143: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0600 - accuracy: 0.9736 - val_loss: 0.5386 - val_accuracy: 0.9030\n",
            "Epoch 144/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0606 - accuracy: 0.9740\n",
            "Epoch 00144: val_accuracy did not improve from 0.90459\n",
            "\n",
            "Epoch 00144: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0606 - accuracy: 0.9740 - val_loss: 0.5410 - val_accuracy: 0.9037\n",
            "Epoch 145/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9750\n",
            "Epoch 00145: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 0.5414 - val_accuracy: 0.9036\n",
            "Epoch 146/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9747\n",
            "Epoch 00146: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0580 - accuracy: 0.9747 - val_loss: 0.5456 - val_accuracy: 0.9042\n",
            "Epoch 147/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9753\n",
            "Epoch 00147: val_accuracy did not improve from 0.90459\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0577 - accuracy: 0.9753 - val_loss: 0.5459 - val_accuracy: 0.9036\n",
            "Epoch 148/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9746\n",
            "Epoch 00148: val_accuracy improved from 0.90459 to 0.90488, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0574 - accuracy: 0.9746 - val_loss: 0.5449 - val_accuracy: 0.9049\n",
            "Epoch 149/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9759\n",
            "Epoch 00149: val_accuracy did not improve from 0.90488\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0566 - accuracy: 0.9759 - val_loss: 0.5479 - val_accuracy: 0.9036\n",
            "Epoch 150/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9754\n",
            "Epoch 00150: val_accuracy improved from 0.90488 to 0.90544, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0574 - accuracy: 0.9754 - val_loss: 0.5470 - val_accuracy: 0.9054\n",
            "Epoch 151/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9764\n",
            "Epoch 00151: val_accuracy improved from 0.90544 to 0.90686, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0564 - accuracy: 0.9764 - val_loss: 0.5498 - val_accuracy: 0.9069\n",
            "Epoch 152/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9765\n",
            "Epoch 00152: val_accuracy did not improve from 0.90686\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0563 - accuracy: 0.9765 - val_loss: 0.5475 - val_accuracy: 0.9047\n",
            "Epoch 153/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9759\n",
            "Epoch 00153: val_accuracy did not improve from 0.90686\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0555 - accuracy: 0.9759 - val_loss: 0.5481 - val_accuracy: 0.9066\n",
            "Epoch 154/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9741\n",
            "Epoch 00154: val_accuracy did not improve from 0.90686\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0588 - accuracy: 0.9741 - val_loss: 0.5438 - val_accuracy: 0.9067\n",
            "Epoch 155/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9755\n",
            "Epoch 00155: val_accuracy did not improve from 0.90686\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0565 - accuracy: 0.9755 - val_loss: 0.5454 - val_accuracy: 0.9062\n",
            "Epoch 156/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9764\n",
            "Epoch 00156: val_accuracy did not improve from 0.90686\n",
            "\n",
            "Epoch 00156: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0551 - accuracy: 0.9764 - val_loss: 0.5480 - val_accuracy: 0.9060\n",
            "Epoch 157/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9759\n",
            "Epoch 00157: val_accuracy did not improve from 0.90686\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0545 - accuracy: 0.9759 - val_loss: 0.5502 - val_accuracy: 0.9057\n",
            "Epoch 158/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9764\n",
            "Epoch 00158: val_accuracy did not improve from 0.90686\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.0550 - accuracy: 0.9764 - val_loss: 0.5515 - val_accuracy: 0.9063\n",
            "Epoch 159/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9762\n",
            "Epoch 00159: val_accuracy improved from 0.90686 to 0.90714, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0541 - accuracy: 0.9762 - val_loss: 0.5513 - val_accuracy: 0.9071\n",
            "Epoch 160/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9764\n",
            "Epoch 00160: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0542 - accuracy: 0.9764 - val_loss: 0.5525 - val_accuracy: 0.9067\n",
            "Epoch 161/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9777\n",
            "Epoch 00161: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0530 - accuracy: 0.9777 - val_loss: 0.5542 - val_accuracy: 0.9067\n",
            "Epoch 162/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9772\n",
            "Epoch 00162: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0536 - accuracy: 0.9772 - val_loss: 0.5561 - val_accuracy: 0.9052\n",
            "Epoch 163/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9769\n",
            "Epoch 00163: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0531 - accuracy: 0.9769 - val_loss: 0.5564 - val_accuracy: 0.9071\n",
            "Epoch 164/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9772\n",
            "Epoch 00164: val_accuracy did not improve from 0.90714\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.0540 - accuracy: 0.9772 - val_loss: 0.5566 - val_accuracy: 0.9071\n",
            "Epoch 165/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9766\n",
            "Epoch 00165: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0535 - accuracy: 0.9766 - val_loss: 0.5556 - val_accuracy: 0.9069\n",
            "Epoch 166/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9766\n",
            "Epoch 00166: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0527 - accuracy: 0.9766 - val_loss: 0.5569 - val_accuracy: 0.9070\n",
            "Epoch 167/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9779\n",
            "Epoch 00167: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0531 - accuracy: 0.9779 - val_loss: 0.5570 - val_accuracy: 0.9069\n",
            "Epoch 168/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9770\n",
            "Epoch 00168: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0534 - accuracy: 0.9770 - val_loss: 0.5559 - val_accuracy: 0.9069\n",
            "Epoch 169/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9773\n",
            "Epoch 00169: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0535 - accuracy: 0.9773 - val_loss: 0.5566 - val_accuracy: 0.9063\n",
            "Epoch 170/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9770\n",
            "Epoch 00170: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0535 - accuracy: 0.9770 - val_loss: 0.5575 - val_accuracy: 0.9067\n",
            "Epoch 171/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9781\n",
            "Epoch 00171: val_accuracy did not improve from 0.90714\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0515 - accuracy: 0.9781 - val_loss: 0.5590 - val_accuracy: 0.9062\n",
            "Epoch 172/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9768\n",
            "Epoch 00172: val_accuracy improved from 0.90714 to 0.90743, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0541 - accuracy: 0.9768 - val_loss: 0.5589 - val_accuracy: 0.9074\n",
            "Epoch 173/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9767\n",
            "Epoch 00173: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0539 - accuracy: 0.9767 - val_loss: 0.5596 - val_accuracy: 0.9067\n",
            "Epoch 174/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9778\n",
            "Epoch 00174: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0526 - accuracy: 0.9778 - val_loss: 0.5620 - val_accuracy: 0.9059\n",
            "Epoch 175/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9767\n",
            "Epoch 00175: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0527 - accuracy: 0.9767 - val_loss: 0.5619 - val_accuracy: 0.9057\n",
            "Epoch 176/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9779\n",
            "Epoch 00176: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0518 - accuracy: 0.9779 - val_loss: 0.5616 - val_accuracy: 0.9059\n",
            "Epoch 177/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9776\n",
            "Epoch 00177: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0513 - accuracy: 0.9776 - val_loss: 0.5635 - val_accuracy: 0.9060\n",
            "Epoch 178/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9772\n",
            "Epoch 00178: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0544 - accuracy: 0.9772 - val_loss: 0.5620 - val_accuracy: 0.9060\n",
            "Epoch 179/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9776\n",
            "Epoch 00179: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0513 - accuracy: 0.9776 - val_loss: 0.5631 - val_accuracy: 0.9064\n",
            "Epoch 180/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9769\n",
            "Epoch 00180: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0530 - accuracy: 0.9769 - val_loss: 0.5635 - val_accuracy: 0.9066\n",
            "Epoch 181/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9768\n",
            "Epoch 00181: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0535 - accuracy: 0.9768 - val_loss: 0.5639 - val_accuracy: 0.9071\n",
            "Epoch 182/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9770\n",
            "Epoch 00182: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0541 - accuracy: 0.9770 - val_loss: 0.5622 - val_accuracy: 0.9069\n",
            "Epoch 183/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9770\n",
            "Epoch 00183: val_accuracy did not improve from 0.90743\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0530 - accuracy: 0.9770 - val_loss: 0.5622 - val_accuracy: 0.9066\n",
            "Epoch 184/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9779\n",
            "Epoch 00184: val_accuracy improved from 0.90743 to 0.90785, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0518 - accuracy: 0.9779 - val_loss: 0.5626 - val_accuracy: 0.9079\n",
            "Epoch 185/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9775\n",
            "Epoch 00185: val_accuracy improved from 0.90785 to 0.90800, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0519 - accuracy: 0.9775 - val_loss: 0.5629 - val_accuracy: 0.9080\n",
            "Epoch 186/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9769\n",
            "Epoch 00186: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0526 - accuracy: 0.9769 - val_loss: 0.5629 - val_accuracy: 0.9076\n",
            "Epoch 187/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9776\n",
            "Epoch 00187: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0531 - accuracy: 0.9776 - val_loss: 0.5629 - val_accuracy: 0.9074\n",
            "Epoch 188/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9767\n",
            "Epoch 00188: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0539 - accuracy: 0.9767 - val_loss: 0.5629 - val_accuracy: 0.9070\n",
            "Epoch 189/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9771\n",
            "Epoch 00189: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0524 - accuracy: 0.9771 - val_loss: 0.5638 - val_accuracy: 0.9080\n",
            "Epoch 190/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9770\n",
            "Epoch 00190: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0521 - accuracy: 0.9770 - val_loss: 0.5651 - val_accuracy: 0.9077\n",
            "Epoch 191/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9789\n",
            "Epoch 00191: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0497 - accuracy: 0.9789 - val_loss: 0.5676 - val_accuracy: 0.9069\n",
            "Epoch 192/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9779\n",
            "Epoch 00192: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0520 - accuracy: 0.9779 - val_loss: 0.5684 - val_accuracy: 0.9071\n",
            "Epoch 193/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0525 - accuracy: 0.9774\n",
            "Epoch 00193: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0523 - accuracy: 0.9775 - val_loss: 0.5693 - val_accuracy: 0.9077\n",
            "Epoch 194/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9779\n",
            "Epoch 00194: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0507 - accuracy: 0.9779 - val_loss: 0.5699 - val_accuracy: 0.9076\n",
            "Epoch 195/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9781\n",
            "Epoch 00195: val_accuracy did not improve from 0.90800\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0522 - accuracy: 0.9781 - val_loss: 0.5685 - val_accuracy: 0.9079\n",
            "Epoch 196/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9770\n",
            "Epoch 00196: val_accuracy improved from 0.90800 to 0.90814, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0522 - accuracy: 0.9771 - val_loss: 0.5691 - val_accuracy: 0.9081\n",
            "Epoch 197/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9774\n",
            "Epoch 00197: val_accuracy improved from 0.90814 to 0.90828, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0526 - accuracy: 0.9774 - val_loss: 0.5692 - val_accuracy: 0.9083\n",
            "Epoch 198/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9776\n",
            "Epoch 00198: val_accuracy did not improve from 0.90828\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0512 - accuracy: 0.9776 - val_loss: 0.5691 - val_accuracy: 0.9070\n",
            "Epoch 199/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9778\n",
            "Epoch 00199: val_accuracy did not improve from 0.90828\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0516 - accuracy: 0.9778 - val_loss: 0.5674 - val_accuracy: 0.9083\n",
            "Epoch 200/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9781\n",
            "Epoch 00200: val_accuracy did not improve from 0.90828\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0512 - accuracy: 0.9781 - val_loss: 0.5684 - val_accuracy: 0.9079\n",
            "Epoch 201/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9771\n",
            "Epoch 00201: val_accuracy did not improve from 0.90828\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0527 - accuracy: 0.9771 - val_loss: 0.5688 - val_accuracy: 0.9080\n",
            "Epoch 202/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9784\n",
            "Epoch 00202: val_accuracy did not improve from 0.90828\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0513 - accuracy: 0.9784 - val_loss: 0.5692 - val_accuracy: 0.9083\n",
            "Epoch 203/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9778\n",
            "Epoch 00203: val_accuracy improved from 0.90828 to 0.90870, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0510 - accuracy: 0.9778 - val_loss: 0.5693 - val_accuracy: 0.9087\n",
            "Epoch 204/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9787\n",
            "Epoch 00204: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0506 - accuracy: 0.9787 - val_loss: 0.5707 - val_accuracy: 0.9086\n",
            "Epoch 205/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9778\n",
            "Epoch 00205: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0524 - accuracy: 0.9778 - val_loss: 0.5701 - val_accuracy: 0.9074\n",
            "Epoch 206/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9777\n",
            "Epoch 00206: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0516 - accuracy: 0.9777 - val_loss: 0.5699 - val_accuracy: 0.9077\n",
            "Epoch 207/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9781\n",
            "Epoch 00207: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0517 - accuracy: 0.9781 - val_loss: 0.5692 - val_accuracy: 0.9074\n",
            "Epoch 208/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9781\n",
            "Epoch 00208: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0517 - accuracy: 0.9781 - val_loss: 0.5684 - val_accuracy: 0.9071\n",
            "Epoch 209/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9777\n",
            "Epoch 00209: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0522 - accuracy: 0.9777 - val_loss: 0.5691 - val_accuracy: 0.9070\n",
            "Epoch 210/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9784\n",
            "Epoch 00210: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0516 - accuracy: 0.9784 - val_loss: 0.5688 - val_accuracy: 0.9067\n",
            "Epoch 211/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9784\n",
            "Epoch 00211: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0521 - accuracy: 0.9784 - val_loss: 0.5693 - val_accuracy: 0.9069\n",
            "Epoch 212/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9782\n",
            "Epoch 00212: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0517 - accuracy: 0.9782 - val_loss: 0.5689 - val_accuracy: 0.9073\n",
            "Epoch 213/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9782\n",
            "Epoch 00213: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0518 - accuracy: 0.9782 - val_loss: 0.5691 - val_accuracy: 0.9084\n",
            "Epoch 214/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9779\n",
            "Epoch 00214: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0509 - accuracy: 0.9779 - val_loss: 0.5704 - val_accuracy: 0.9079\n",
            "Epoch 215/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9783\n",
            "Epoch 00215: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0505 - accuracy: 0.9783 - val_loss: 0.5712 - val_accuracy: 0.9080\n",
            "Epoch 216/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9782\n",
            "Epoch 00216: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0502 - accuracy: 0.9782 - val_loss: 0.5737 - val_accuracy: 0.9076\n",
            "Epoch 217/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9792\n",
            "Epoch 00217: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0498 - accuracy: 0.9792 - val_loss: 0.5740 - val_accuracy: 0.9080\n",
            "Epoch 218/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9779\n",
            "Epoch 00218: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0516 - accuracy: 0.9779 - val_loss: 0.5749 - val_accuracy: 0.9080\n",
            "Epoch 219/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9777\n",
            "Epoch 00219: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0510 - accuracy: 0.9777 - val_loss: 0.5738 - val_accuracy: 0.9074\n",
            "Epoch 220/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9781\n",
            "Epoch 00220: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0518 - accuracy: 0.9781 - val_loss: 0.5742 - val_accuracy: 0.9074\n",
            "Epoch 221/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9778\n",
            "Epoch 00221: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0514 - accuracy: 0.9778 - val_loss: 0.5742 - val_accuracy: 0.9079\n",
            "Epoch 222/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9781\n",
            "Epoch 00222: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0508 - accuracy: 0.9781 - val_loss: 0.5741 - val_accuracy: 0.9076\n",
            "Epoch 223/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9782\n",
            "Epoch 00223: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0507 - accuracy: 0.9782 - val_loss: 0.5742 - val_accuracy: 0.9081\n",
            "Epoch 224/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9779\n",
            "Epoch 00224: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0509 - accuracy: 0.9779 - val_loss: 0.5742 - val_accuracy: 0.9083\n",
            "Epoch 225/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9778\n",
            "Epoch 00225: val_accuracy did not improve from 0.90870\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0519 - accuracy: 0.9778 - val_loss: 0.5739 - val_accuracy: 0.9083\n",
            "Epoch 226/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9780\n",
            "Epoch 00226: val_accuracy improved from 0.90870 to 0.90899, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0502 - accuracy: 0.9780 - val_loss: 0.5752 - val_accuracy: 0.9090\n",
            "Epoch 227/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9783\n",
            "Epoch 00227: val_accuracy improved from 0.90899 to 0.90941, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0513 - accuracy: 0.9783 - val_loss: 0.5748 - val_accuracy: 0.9094\n",
            "Epoch 228/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9777\n",
            "Epoch 00228: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0509 - accuracy: 0.9777 - val_loss: 0.5753 - val_accuracy: 0.9086\n",
            "Epoch 229/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9781\n",
            "Epoch 00229: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0509 - accuracy: 0.9781 - val_loss: 0.5747 - val_accuracy: 0.9094\n",
            "Epoch 230/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9781\n",
            "Epoch 00230: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0515 - accuracy: 0.9781 - val_loss: 0.5751 - val_accuracy: 0.9088\n",
            "Epoch 231/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9783\n",
            "Epoch 00231: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0505 - accuracy: 0.9783 - val_loss: 0.5755 - val_accuracy: 0.9084\n",
            "Epoch 232/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9789\n",
            "Epoch 00232: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0491 - accuracy: 0.9789 - val_loss: 0.5773 - val_accuracy: 0.9084\n",
            "Epoch 233/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9784\n",
            "Epoch 00233: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0506 - accuracy: 0.9784 - val_loss: 0.5791 - val_accuracy: 0.9091\n",
            "Epoch 234/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9788\n",
            "Epoch 00234: val_accuracy did not improve from 0.90941\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0503 - accuracy: 0.9788 - val_loss: 0.5803 - val_accuracy: 0.9087\n",
            "Epoch 235/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0495 - accuracy: 0.9792\n",
            "Epoch 00235: val_accuracy improved from 0.90941 to 0.90955, saving model to best_cnn2d_model.h5\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0495 - accuracy: 0.9792 - val_loss: 0.5798 - val_accuracy: 0.9096\n",
            "Epoch 236/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9787\n",
            "Epoch 00236: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0504 - accuracy: 0.9787 - val_loss: 0.5797 - val_accuracy: 0.9087\n",
            "Epoch 237/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9788\n",
            "Epoch 00237: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0513 - accuracy: 0.9788 - val_loss: 0.5781 - val_accuracy: 0.9086\n",
            "Epoch 238/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0508 - accuracy: 0.9781\n",
            "Epoch 00238: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0508 - accuracy: 0.9782 - val_loss: 0.5787 - val_accuracy: 0.9093\n",
            "Epoch 239/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9782\n",
            "Epoch 00239: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0507 - accuracy: 0.9782 - val_loss: 0.5779 - val_accuracy: 0.9087\n",
            "Epoch 240/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9786\n",
            "Epoch 00240: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0496 - accuracy: 0.9786 - val_loss: 0.5793 - val_accuracy: 0.9090\n",
            "Epoch 241/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9782\n",
            "Epoch 00241: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0509 - accuracy: 0.9782 - val_loss: 0.5790 - val_accuracy: 0.9087\n",
            "Epoch 242/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9776\n",
            "Epoch 00242: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0506 - accuracy: 0.9776 - val_loss: 0.5771 - val_accuracy: 0.9081\n",
            "Epoch 243/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9780\n",
            "Epoch 00243: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0507 - accuracy: 0.9780 - val_loss: 0.5776 - val_accuracy: 0.9088\n",
            "Epoch 244/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9787\n",
            "Epoch 00244: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0502 - accuracy: 0.9787 - val_loss: 0.5782 - val_accuracy: 0.9091\n",
            "Epoch 245/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9788\n",
            "Epoch 00245: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0503 - accuracy: 0.9788 - val_loss: 0.5788 - val_accuracy: 0.9076\n",
            "Epoch 246/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9783\n",
            "Epoch 00246: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0500 - accuracy: 0.9783 - val_loss: 0.5779 - val_accuracy: 0.9086\n",
            "Epoch 247/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9788\n",
            "Epoch 00247: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0505 - accuracy: 0.9788 - val_loss: 0.5790 - val_accuracy: 0.9084\n",
            "Epoch 248/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9783\n",
            "Epoch 00248: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0501 - accuracy: 0.9783 - val_loss: 0.5789 - val_accuracy: 0.9087\n",
            "Epoch 249/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9787\n",
            "Epoch 00249: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0503 - accuracy: 0.9787 - val_loss: 0.5784 - val_accuracy: 0.9088\n",
            "Epoch 250/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9796\n",
            "Epoch 00250: val_accuracy did not improve from 0.90955\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 0.5792 - val_accuracy: 0.9083\n",
            "245/245 [==============================] - 1s 3ms/step - loss: 0.5467 - accuracy: 0.9101\n",
            "Accuracy(on Test-data): 0.9100536108016968\n",
            "time of training (s) 739.9511046409607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY-9otupoKMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viz(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        " \n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp5baxEkoLdp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "f36135e0-146d-4efb-8fae-ca75df7b5223"
      },
      "source": [
        "viz(fit1)\n",
        "#fit1.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi5cF2QioPcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ccef3b4-b832-4c7e-9ee4-627e28bb79b2"
      },
      "source": [
        "# load a saved model\n",
        "from keras.models import load_model\n",
        "saved_model = load_model('best_cnn2d_model.h5')\n",
        "# evaluate the model\n",
        "_, train_acc = saved_model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = saved_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.983, Test: 0.910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMbUtjRV8eD0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "ded3c8a3-0f0b-4bf4-bc85-9308b45a00c2"
      },
      "source": [
        "import time\n",
        " \n",
        "Xnew=[['70.39932429', '127673.0908', '-49.57230843', '127648.0176', '-169.5783186', '127723.2374', '65.68961121', '605.91099', '-57.00357104', '626.78553', '-173.5890232', '602.4319', '70.4222426', '127673.0908', '0', '0', '0', '0', '65.00779144', '611.5874', '118.5678861', '13.18392', '-100.8692198', '13.91636', '59.999', '0.01', '6.391383458', '0.076290455', '0', '60.65826798', '124631.8125', '-59.29595943', '124484.3594', '-179.3380777', '124715.0703', '-119.5504813', '612.7967529', '117.7267525', '632.5321045', '0.859680212', '610.1417236', '60.6802407', '124611.9844', '0', '0', '0', '0', '-120.3414991', '618.3013916', '-64.05304275', '12.7658844', '69.39789118', '12.8288269', '59.99900055', '0.02', '6.130100104', '3.135101005', '0', '60.66477135', '124187.9063', '-59.31259095', '124162.833', '-179.3014124', '124212.9796', '-119.7539088', '610.12252', '117.6855311', '628.25041', '0.658901464', '606.82654', '60.68768966', '124187.9063', '0', '0', '0', '0', '-120.4872947', '614.88338', '-64.81298579', '12.08526', '70.38786513', '11.90215', '59.999', '0.02', '6.111439531', '3.140520023', '0', '70.45089049', '127723.2374', '-49.53793097', '127096.4056', '-169.532482', '127773.3839', '65.64377459', '604.44611', '-56.87179074', '621.84156', '-173.8697725', '599.86836', '70.46234965', '127522.6512', '0', '0', '0', '0', '64.95049566', '608.47453', '119.3012721', '12.26837', '-102.060972', '11.71904', '59.999', '0.01', '6.341831592', '0.077897157', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#normal    \n",
        " \n",
        "Q=[['8.508423258', '130832.3229', '-111.4632095', '130782.1763', '128.5258926', '130907.5427', '3.729955246', '500.80585', '-116.6026409', '500.62274', '123.632833', '501.35518', '8.519882414', '130832.3229', '0', '0', '0', '0', '3.586715798', '500.98896', '0', '0', '0', '0', '60', '0', '7.438322484', '0.077833008', '0', '0.920105028', '128836.9766', '-119.0066616', '128699.4609', '120.9347557', '128913.4688', '176.7782594', '505.7258606', '56.64825709', '507.2078857', '-63.31146179', '506.9961548', '0.94757082', '128814.1563', '0', '0', '0', '0', '176.7068566', '506.6356812', '0', '0', '0', '0', '60', '0', '7.257351968', '-3.071651355', '0', '0.928191628', '128375.1424', '-119.0319819', '128350.0691', '120.9743089', '128400.2157', '176.7173728', '502.63695', '56.61968931', '503.36939', '-63.3290251', '503.00317', '0.95110994', '128375.1424', '0', '0', '0', '0', '176.6715361', '503.00317', '0', '0', '0', '0', '60', '0', '7.268424496', '-3.0691986', '0', '8.559989459', '130857.3961', '-111.428832', '130230.5644', '128.5831884', '130932.6159', '3.707036934', '497.50987', '-116.3619986', '498.24231', '123.741695', '496.59432', '8.571448615', '130681.8832', '0', '0', '0', '0', '3.689848201', '497.50987', '0', '0', '0', '0', '60', '0', '7.489104114', '0.086553421', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        " \n",
        "N=[['70.97801166', '130957.6892', '-48.99362106', '130932.6159', '-169.0053608', '131032.909', '66.42872677', '482.31174', '-53.82938485', '483.22729', '-173.646319', '483.77662', '70.99520039', '130982.7625', '0', '0', '0', '0', '66.31986479', '483.04418', '0', '0', '0', '0', '59.999', '0', '7.742501568', '0.074426263', '0', '63.67401507', '129107.1016', '-56.26922599', '128963.9531', '-176.3168392', '129194.8594', '-120.8908103', '488.6340942', '119.1714468', '489.3894043', '-0.826721188', '489.4866943', '63.70147926', '129088.1484', '0', '0', '0', '0', '-120.8468648', '489.1662598', '0', '0', '0', '0', '60', '0', '7.521482155', '-3.061590456', '0', '63.67279977', '128650.9484', '-56.28737379', '128625.8751', '-176.2819248', '128676.0216', '-120.8826356', '485.79083', '119.1981397', '485.42461', '-0.916732472', '485.42461', '63.70144766', '128650.9484', '0', '0', '0', '0', '-120.8711765', '485.42461', '0', '0', '0', '0', '59.999', '0', '7.510130448', '-3.061004926', '0', '71.03530744', '131007.8358', '-48.97070275', '130381.004', '-168.9423355', '131083.0556', '66.42872677', '480.11442', '-53.53717638', '480.11442', '-173.5890232', '479.38198', '71.0467666', '130832.3229', '0', '0', '0', '0', '66.44018592', '479.93131', '0', '0', '0', '0', '60', '0', '7.746914069', '0.08213434', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        "A=[['174.2765725', '98337.36494', '59.20372897', '128550.6553', '-72.71407378', '127347.1383', '100.4796085', '1111.29459', '-4.566473627', '241.33898', '-85.22174245', '621.84156', '173.5546457', '117618.7096', '-9.786119141', '17325.62957', '-13.22959549', '1980.78833', '118.854365', '599.68525', '82.17933656', '420.23745', '78.74731936', '154.36173', '60.016', '-0.04', '1.907633306', '1.477346313', '0', '173.1802387', '47452.82422', '50.77606342', '120554.2734', '-77.66510434', '132296.1563', '-80.08758802', '1147.247314', '170.1754746', '297.0142212', '89.53582968', '636.0683594', '167.4398882', '99778.42969', '-10.8901979', '31715.72461', '-27.77069198', '21617.71094', '-63.9047252', '642.6830444', '-97.7590974', '408.9145508', '-101.4559988', '150.7930756', '60.01599884', '-0.050000001', '0.903115513', '-1.682575032', '0', '173.1822232', '47288.18722', '50.75260149', '120226.3297', '-77.62432208', '131760.0339', '85.92075096', '1062.40422', '-112.9070631', '589.06487', '123.4151091', '150.88264', '167.4526452', '99440.58882', '-10.94922346', '31567.24693', '-27.58791784', '21588.08547', '54.61433703', '466.74739', '103.9746511', '479.01576', '110.2084319', '228.33817', '60.016', '-0.04', '0.793198105', '1.484402105', '0', '169.0225496', '41797.14109', '63.34048425', '121254.3337', '-75.34967964', '121881.1655', '122.4983766', '1748.15117', '72.4218653', '600.23458', '-38.82934978', '177.06737', '173.2395189', '93849.24961', '-3.907572163', '37534.68519', '-2.056918485', '14743.08276', '142.8441079', '707.17082', '112.8039307', '436.16802', '107.7561725', '677.507', '60.014', '-0.04', '0.341509407', '1.111764282', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#'Attack'\n",
        "transformer= Normalizer() \n",
        "Z=transformer.transform(A)\n",
        "V = ss.transform(Z)\n",
        "V=np.reshape(V, V.shape + (1,))\n",
        "V = V.reshape(-1,8,16,1)\n",
        "start_time = time.time()\n",
        " \n",
        "# make a prediction\n",
        "ynew = saved_model.predict_classes(V)\n",
        "# show the inputs and predicted outputs\n",
        "#for i in range(len(Xnew)):\n",
        "    #print(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
        " \n",
        "duration = time.time() - start_time\n",
        "print(\"time of test (s)\", duration)\n",
        " \n",
        "print(ynew)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time of test (s) 0.03198075294494629\n",
            "[1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:1829: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
            "  X = check_array(X, accept_sparse='csr')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttr8m7O-_OzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}