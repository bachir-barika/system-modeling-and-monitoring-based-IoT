{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEy4m0rqBW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#إستدعاء المكتبيات\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Z_VYJXpDgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install patool\n",
        "!pip install pyunpack\n",
        "from pyunpack import Archive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng-9KDwwpEaj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Archive(\"/data/binaryAllNaturalPlusNormalVsAttacks.7z\").extractall(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_6_bI0RBt3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.read_csv(\"/data/data1.csv\")\n",
        "df2=pd.read_csv(\"/data/data2.csv\")\n",
        "df3=pd.read_csv(\"/data/data3.csv\")\n",
        "df4=pd.read_csv(\"/data/data4.csv\")\n",
        "df5=pd.read_csv(\"/data/data5.csv\")\n",
        "df6=pd.read_csv(\"/data/data6.csv\")\n",
        "df7=pd.read_csv(\"/data/data7.csv\")\n",
        "df8=pd.read_csv(\"/data/data8.csv\")\n",
        "df9=pd.read_csv(\"/data/data9.csv\")\n",
        "df10=pd.read_csv(\"/data/data10.csv\")\n",
        "df11=pd.read_csv(\"/data/data11.csv\")\n",
        "df12=pd.read_csv(\"/data/data12.csv\")\n",
        "df13=pd.read_csv(\"/data/data13.csv\")\n",
        "df14=pd.read_csv(\"/data/data14.csv\")\n",
        "df15=pd.read_csv(\"/data/data15.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JPkDl68B0AH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#دمج مجموعات البيانات\n",
        "df = pd.concat([df1, df2,df3,df4, df5,df6,df7, df8,df9,df10, df11,df12,df13, df14,df15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-27NGnhB2PP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "03c6f46c-22c9-4f7b-ddaf-ba342270f80a"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsNPl2s1B4im",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "f0ae2d62-d07f-4853-8902-be4c06d407ee"
      },
      "source": [
        "df.describe(include=\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-PA1:VH</th>\n",
              "      <th>R1-PM1:V</th>\n",
              "      <th>R1-PA2:VH</th>\n",
              "      <th>R1-PM2:V</th>\n",
              "      <th>R1-PA3:VH</th>\n",
              "      <th>R1-PM3:V</th>\n",
              "      <th>R1-PA4:IH</th>\n",
              "      <th>R1-PM4:I</th>\n",
              "      <th>R1-PA5:IH</th>\n",
              "      <th>R1-PM5:I</th>\n",
              "      <th>R1-PA6:IH</th>\n",
              "      <th>R1-PM6:I</th>\n",
              "      <th>R1-PA7:VH</th>\n",
              "      <th>R1-PM7:V</th>\n",
              "      <th>R1-PA8:VH</th>\n",
              "      <th>R1-PM8:V</th>\n",
              "      <th>R1-PA9:VH</th>\n",
              "      <th>R1-PM9:V</th>\n",
              "      <th>R1-PA10:IH</th>\n",
              "      <th>R1-PM10:I</th>\n",
              "      <th>R1-PA11:IH</th>\n",
              "      <th>R1-PM11:I</th>\n",
              "      <th>R1-PA12:IH</th>\n",
              "      <th>R1-PM12:I</th>\n",
              "      <th>R1:F</th>\n",
              "      <th>R1:DF</th>\n",
              "      <th>R1-PA:Z</th>\n",
              "      <th>R1-PA:ZH</th>\n",
              "      <th>R1:S</th>\n",
              "      <th>R2-PA1:VH</th>\n",
              "      <th>R2-PM1:V</th>\n",
              "      <th>R2-PA2:VH</th>\n",
              "      <th>R2-PM2:V</th>\n",
              "      <th>R2-PA3:VH</th>\n",
              "      <th>R2-PM3:V</th>\n",
              "      <th>R2-PA4:IH</th>\n",
              "      <th>R2-PM4:I</th>\n",
              "      <th>R2-PA5:IH</th>\n",
              "      <th>R2-PM5:I</th>\n",
              "      <th>R2-PA6:IH</th>\n",
              "      <th>...</th>\n",
              "      <th>R4-PA2:VH</th>\n",
              "      <th>R4-PM2:V</th>\n",
              "      <th>R4-PA3:VH</th>\n",
              "      <th>R4-PM3:V</th>\n",
              "      <th>R4-PA4:IH</th>\n",
              "      <th>R4-PM4:I</th>\n",
              "      <th>R4-PA5:IH</th>\n",
              "      <th>R4-PM5:I</th>\n",
              "      <th>R4-PA6:IH</th>\n",
              "      <th>R4-PM6:I</th>\n",
              "      <th>R4-PA7:VH</th>\n",
              "      <th>R4-PM7:V</th>\n",
              "      <th>R4-PA8:VH</th>\n",
              "      <th>R4-PM8:V</th>\n",
              "      <th>R4-PA9:VH</th>\n",
              "      <th>R4-PM9:V</th>\n",
              "      <th>R4-PA10:IH</th>\n",
              "      <th>R4-PM10:I</th>\n",
              "      <th>R4-PA11:IH</th>\n",
              "      <th>R4-PM11:I</th>\n",
              "      <th>R4-PA12:IH</th>\n",
              "      <th>R4-PM12:I</th>\n",
              "      <th>R4:F</th>\n",
              "      <th>R4:DF</th>\n",
              "      <th>R4-PA:Z</th>\n",
              "      <th>R4-PA:ZH</th>\n",
              "      <th>R4:S</th>\n",
              "      <th>control_panel_log1</th>\n",
              "      <th>control_panel_log2</th>\n",
              "      <th>control_panel_log3</th>\n",
              "      <th>control_panel_log4</th>\n",
              "      <th>relay1_log</th>\n",
              "      <th>relay2_log</th>\n",
              "      <th>relay3_log</th>\n",
              "      <th>relay4_log</th>\n",
              "      <th>snort_log1</th>\n",
              "      <th>snort_log2</th>\n",
              "      <th>snort_log3</th>\n",
              "      <th>snort_log4</th>\n",
              "      <th>marker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>55663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-15.802424</td>\n",
              "      <td>130764.039577</td>\n",
              "      <td>2.175196</td>\n",
              "      <td>131035.528095</td>\n",
              "      <td>6.834315</td>\n",
              "      <td>131395.717581</td>\n",
              "      <td>-14.334996</td>\n",
              "      <td>393.949321</td>\n",
              "      <td>3.538540</td>\n",
              "      <td>387.438133</td>\n",
              "      <td>6.129781</td>\n",
              "      <td>381.912845</td>\n",
              "      <td>-15.798835</td>\n",
              "      <td>131056.980030</td>\n",
              "      <td>0.207857</td>\n",
              "      <td>297.083556</td>\n",
              "      <td>0.227606</td>\n",
              "      <td>87.397031</td>\n",
              "      <td>-14.504282</td>\n",
              "      <td>386.557188</td>\n",
              "      <td>-1.734936</td>\n",
              "      <td>9.979982</td>\n",
              "      <td>6.123374</td>\n",
              "      <td>9.494176</td>\n",
              "      <td>59.992801</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.018428</td>\n",
              "      <td>788.868750</td>\n",
              "      <td>-15.216491</td>\n",
              "      <td>127033.389923</td>\n",
              "      <td>4.751134</td>\n",
              "      <td>128015.428015</td>\n",
              "      <td>5.510410</td>\n",
              "      <td>128362.246185</td>\n",
              "      <td>15.836436</td>\n",
              "      <td>395.109497</td>\n",
              "      <td>-6.961603</td>\n",
              "      <td>392.508845</td>\n",
              "      <td>-6.437082</td>\n",
              "      <td>...</td>\n",
              "      <td>2.278991</td>\n",
              "      <td>131355.212680</td>\n",
              "      <td>7.065760</td>\n",
              "      <td>131745.074472</td>\n",
              "      <td>-13.931742</td>\n",
              "      <td>391.330912</td>\n",
              "      <td>3.446031</td>\n",
              "      <td>384.399819</td>\n",
              "      <td>6.096400</td>\n",
              "      <td>379.952713</td>\n",
              "      <td>-15.563852</td>\n",
              "      <td>131397.999652</td>\n",
              "      <td>0.257084</td>\n",
              "      <td>292.112647</td>\n",
              "      <td>0.207103</td>\n",
              "      <td>82.439295</td>\n",
              "      <td>-14.144585</td>\n",
              "      <td>384.036050</td>\n",
              "      <td>-1.859917</td>\n",
              "      <td>9.834635</td>\n",
              "      <td>5.989009</td>\n",
              "      <td>9.073233</td>\n",
              "      <td>59.992750</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.016616</td>\n",
              "      <td>749.014459</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.026436</td>\n",
              "      <td>0.026500</td>\n",
              "      <td>0.035597</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>100.876750</td>\n",
              "      <td>8546.118477</td>\n",
              "      <td>111.743169</td>\n",
              "      <td>5393.135370</td>\n",
              "      <td>97.065063</td>\n",
              "      <td>5443.752388</td>\n",
              "      <td>99.601107</td>\n",
              "      <td>190.966011</td>\n",
              "      <td>109.504977</td>\n",
              "      <td>151.277183</td>\n",
              "      <td>95.294904</td>\n",
              "      <td>153.326452</td>\n",
              "      <td>100.877118</td>\n",
              "      <td>6152.379663</td>\n",
              "      <td>13.075863</td>\n",
              "      <td>2687.617199</td>\n",
              "      <td>12.488596</td>\n",
              "      <td>897.541412</td>\n",
              "      <td>99.605025</td>\n",
              "      <td>154.484403</td>\n",
              "      <td>68.383257</td>\n",
              "      <td>47.241783</td>\n",
              "      <td>73.059209</td>\n",
              "      <td>47.875569</td>\n",
              "      <td>0.610045</td>\n",
              "      <td>0.087799</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.242813</td>\n",
              "      <td>14048.448459</td>\n",
              "      <td>101.837622</td>\n",
              "      <td>16155.767175</td>\n",
              "      <td>111.043204</td>\n",
              "      <td>12106.876201</td>\n",
              "      <td>96.270117</td>\n",
              "      <td>11990.863815</td>\n",
              "      <td>99.876094</td>\n",
              "      <td>171.765698</td>\n",
              "      <td>94.996062</td>\n",
              "      <td>152.357765</td>\n",
              "      <td>108.896267</td>\n",
              "      <td>...</td>\n",
              "      <td>111.828597</td>\n",
              "      <td>4733.901358</td>\n",
              "      <td>97.085981</td>\n",
              "      <td>4777.648212</td>\n",
              "      <td>99.653296</td>\n",
              "      <td>187.094100</td>\n",
              "      <td>109.561785</td>\n",
              "      <td>148.882516</td>\n",
              "      <td>95.495537</td>\n",
              "      <td>150.929876</td>\n",
              "      <td>100.882320</td>\n",
              "      <td>5536.542517</td>\n",
              "      <td>13.150046</td>\n",
              "      <td>2621.155809</td>\n",
              "      <td>12.523032</td>\n",
              "      <td>850.696972</td>\n",
              "      <td>99.627784</td>\n",
              "      <td>151.746973</td>\n",
              "      <td>67.783975</td>\n",
              "      <td>47.562328</td>\n",
              "      <td>72.087423</td>\n",
              "      <td>45.998572</td>\n",
              "      <td>0.609958</td>\n",
              "      <td>0.087273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.248023</td>\n",
              "      <td>14041.170907</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.186082</td>\n",
              "      <td>0.160430</td>\n",
              "      <td>0.160618</td>\n",
              "      <td>0.185285</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.009450</td>\n",
              "      <td>0.008749</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.501948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.903018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.010000</td>\n",
              "      <td>1.852102e-01</td>\n",
              "      <td>-3.140569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>...</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-169.984571</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>-178.875387</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-142.790610</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-165.686647</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>-155.458725</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-145.203989</td>\n",
              "      <td>-179.719672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.612733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.460000</td>\n",
              "      <td>6.781427e-03</td>\n",
              "      <td>-3.093717</td>\n",
              "      <td>-45.998332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-100.416583</td>\n",
              "      <td>131057.982300</td>\n",
              "      <td>-102.129727</td>\n",
              "      <td>130732.029800</td>\n",
              "      <td>-69.459673</td>\n",
              "      <td>131133.202100</td>\n",
              "      <td>-98.159129</td>\n",
              "      <td>305.793700</td>\n",
              "      <td>-94.790138</td>\n",
              "      <td>311.836330</td>\n",
              "      <td>-66.279758</td>\n",
              "      <td>303.962600</td>\n",
              "      <td>-100.399394</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-98.227884</td>\n",
              "      <td>307.807910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.291006e+00</td>\n",
              "      <td>-0.028589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-101.562499</td>\n",
              "      <td>128826.461300</td>\n",
              "      <td>-96.490178</td>\n",
              "      <td>128769.000000</td>\n",
              "      <td>-69.052873</td>\n",
              "      <td>128901.681100</td>\n",
              "      <td>-65.091252</td>\n",
              "      <td>310.554560</td>\n",
              "      <td>-83.182013</td>\n",
              "      <td>316.446289</td>\n",
              "      <td>-105.295719</td>\n",
              "      <td>...</td>\n",
              "      <td>-101.992217</td>\n",
              "      <td>131018.156300</td>\n",
              "      <td>-69.430847</td>\n",
              "      <td>131435.093800</td>\n",
              "      <td>-97.643467</td>\n",
              "      <td>304.511930</td>\n",
              "      <td>-94.919132</td>\n",
              "      <td>309.494019</td>\n",
              "      <td>-67.173162</td>\n",
              "      <td>302.598938</td>\n",
              "      <td>-100.009783</td>\n",
              "      <td>131272.515600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-97.723682</td>\n",
              "      <td>306.249634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.347653e+00</td>\n",
              "      <td>-0.028625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-28.865614</td>\n",
              "      <td>131684.814000</td>\n",
              "      <td>8.118812</td>\n",
              "      <td>131358.861500</td>\n",
              "      <td>13.401483</td>\n",
              "      <td>131760.033900</td>\n",
              "      <td>-23.514188</td>\n",
              "      <td>378.671480</td>\n",
              "      <td>1.885031</td>\n",
              "      <td>383.249230</td>\n",
              "      <td>6.881223</td>\n",
              "      <td>376.474160</td>\n",
              "      <td>-28.842695</td>\n",
              "      <td>131609.594200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.749101</td>\n",
              "      <td>380.319470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.011317e+01</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-29.665834</td>\n",
              "      <td>130020.265600</td>\n",
              "      <td>12.330052</td>\n",
              "      <td>129954.758400</td>\n",
              "      <td>10.077210</td>\n",
              "      <td>130080.124800</td>\n",
              "      <td>24.534053</td>\n",
              "      <td>383.615450</td>\n",
              "      <td>-5.729578</td>\n",
              "      <td>388.010090</td>\n",
              "      <td>-6.222322</td>\n",
              "      <td>...</td>\n",
              "      <td>7.969843</td>\n",
              "      <td>131634.667500</td>\n",
              "      <td>13.883972</td>\n",
              "      <td>132060.913100</td>\n",
              "      <td>-23.124577</td>\n",
              "      <td>376.997009</td>\n",
              "      <td>1.512609</td>\n",
              "      <td>380.868800</td>\n",
              "      <td>7.116394</td>\n",
              "      <td>375.009280</td>\n",
              "      <td>-28.742983</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.359489</td>\n",
              "      <td>378.353119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.017212e+01</td>\n",
              "      <td>0.015089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>68.096034</td>\n",
              "      <td>132186.279400</td>\n",
              "      <td>104.897113</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>85.324875</td>\n",
              "      <td>132261.499300</td>\n",
              "      <td>66.274028</td>\n",
              "      <td>456.676340</td>\n",
              "      <td>102.674037</td>\n",
              "      <td>460.338540</td>\n",
              "      <td>82.053286</td>\n",
              "      <td>454.295910</td>\n",
              "      <td>68.096034</td>\n",
              "      <td>132085.986400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.027656</td>\n",
              "      <td>457.775000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>14.667720</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.269372e+01</td>\n",
              "      <td>0.059942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69.035338</td>\n",
              "      <td>130932.615900</td>\n",
              "      <td>104.674992</td>\n",
              "      <td>130857.396100</td>\n",
              "      <td>81.577731</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>102.450263</td>\n",
              "      <td>461.986530</td>\n",
              "      <td>65.091252</td>\n",
              "      <td>465.751648</td>\n",
              "      <td>93.506712</td>\n",
              "      <td>...</td>\n",
              "      <td>105.040353</td>\n",
              "      <td>132325.828100</td>\n",
              "      <td>85.565517</td>\n",
              "      <td>132584.640600</td>\n",
              "      <td>66.849063</td>\n",
              "      <td>454.050049</td>\n",
              "      <td>102.620546</td>\n",
              "      <td>457.489014</td>\n",
              "      <td>82.167877</td>\n",
              "      <td>452.281700</td>\n",
              "      <td>68.321228</td>\n",
              "      <td>132467.359400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.531859</td>\n",
              "      <td>455.348968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.780624</td>\n",
              "      <td>7.992761</td>\n",
              "      <td>6.775070</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.277497e+01</td>\n",
              "      <td>0.057622</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>179.994691</td>\n",
              "      <td>151592.990400</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151567.917200</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151693.283500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1779.462980</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1265.656320</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151618.063700</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>46987.307980</td>\n",
              "      <td>179.467570</td>\n",
              "      <td>17501.142460</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>611.404290</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>723.467610</td>\n",
              "      <td>66.035000</td>\n",
              "      <td>3.720000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.125937</td>\n",
              "      <td>272394.000000</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>145336.656300</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>145167.796900</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>155526.781300</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1416.722070</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1275.910480</td>\n",
              "      <td>179.986276</td>\n",
              "      <td>...</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151749.687500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151924.062500</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1781.324341</td>\n",
              "      <td>179.991768</td>\n",
              "      <td>1266.205650</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1262.726560</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151859.328100</td>\n",
              "      <td>179.964297</td>\n",
              "      <td>45946.320310</td>\n",
              "      <td>179.914860</td>\n",
              "      <td>17351.042970</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1264.740770</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>610.038757</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>715.827942</td>\n",
              "      <td>62.226000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.106656</td>\n",
              "      <td>270336.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R1-PA1:VH       R1-PM1:V  ...    snort_log4  marker\n",
              "count   78377.000000   78377.000000  ...  78377.000000   78377\n",
              "unique           NaN            NaN  ...           NaN       2\n",
              "top              NaN            NaN  ...           NaN  Attack\n",
              "freq             NaN            NaN  ...           NaN   55663\n",
              "mean      -15.802424  130764.039577  ...      0.000077     NaN\n",
              "std       100.876750    8546.118477  ...      0.008749     NaN\n",
              "min      -179.988962       0.000000  ...      0.000000     NaN\n",
              "25%      -100.416583  131057.982300  ...      0.000000     NaN\n",
              "50%       -28.865614  131684.814000  ...      0.000000     NaN\n",
              "75%        68.096034  132186.279400  ...      0.000000     NaN\n",
              "max       179.994691  151592.990400  ...      1.000000     NaN\n",
              "\n",
              "[11 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61hfL3H4B6Zu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e2f25e5-4dc8-436d-e910-9de5094d2b42"
      },
      "source": [
        "# Check if missing values\n",
        "df.isnull().values.any()\n",
        " \n",
        "# Check number of NaNs\n",
        "df.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwCmY0JlB_f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#تحويل الكلمات إلى أرقام للعمود marker\n",
        "#target equal 0 is natural\n",
        "#target equal 1 is Attack\n",
        "df.loc[df[\"marker\"] == \"Natural\", \"marker\"] = 0\n",
        "df.loc[df[\"marker\"] ==\"Attack\", \"marker\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbI9QkYnCCFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "b402f2d0-921a-4f8c-f12e-f0eb7b0033f3"
      },
      "source": [
        "# إحصاء القيم\n",
        "df[\"marker\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    55663\n",
              "0    22714\n",
              "Name: marker, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZAvpzlzCEXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7b89596-31a9-413e-bb96-c85e82c7d3d7"
      },
      "source": [
        "#سيعطي هذا مجموعة من الأماكن حيث توجد قيم NA.\n",
        " \n",
        "df[df==np.inf]=np.nan\n",
        " \n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#ذا كانت بياناتك تحتوي على Nan ، فجرّب ما يلي:\n",
        "np.isnan(df.values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2cVG6pzCGwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert type float and string\n",
        " \n",
        "X= df.drop(\"marker\", axis = 1)\n",
        "X=X.astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz9NRzLFJseD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "transformer= Normalizer().fit(X) \n",
        "transformer\n",
        "S=transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fep-HhUCJ3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "8c11578e-0487-49b7-e5e1-6e7c672d52dc"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        " \n",
        "x = pd.DataFrame(ss.fit_transform(S))\n",
        " \n",
        "y = df[\"marker\"]\n",
        " \n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.874304</td>\n",
              "      <td>0.099036</td>\n",
              "      <td>-0.474731</td>\n",
              "      <td>0.061580</td>\n",
              "      <td>-1.867958</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.824335</td>\n",
              "      <td>1.018163</td>\n",
              "      <td>-0.569834</td>\n",
              "      <td>1.564347</td>\n",
              "      <td>-1.946451</td>\n",
              "      <td>1.417422</td>\n",
              "      <td>0.874513</td>\n",
              "      <td>0.069631</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.819024</td>\n",
              "      <td>1.415316</td>\n",
              "      <td>1.806088</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-1.503010</td>\n",
              "      <td>-0.004960</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100647</td>\n",
              "      <td>-0.023363</td>\n",
              "      <td>0.205864</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.763126</td>\n",
              "      <td>0.129576</td>\n",
              "      <td>-0.593351</td>\n",
              "      <td>0.052249</td>\n",
              "      <td>-1.979122</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>-1.394714</td>\n",
              "      <td>1.193198</td>\n",
              "      <td>1.353806</td>\n",
              "      <td>1.556328</td>\n",
              "      <td>0.066997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063134</td>\n",
              "      <td>-0.412118</td>\n",
              "      <td>-0.061167</td>\n",
              "      <td>-1.434133</td>\n",
              "      <td>-0.019137</td>\n",
              "      <td>0.620453</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>-0.567535</td>\n",
              "      <td>0.131541</td>\n",
              "      <td>-1.941687</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.869845</td>\n",
              "      <td>-0.002144</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.814642</td>\n",
              "      <td>1.448899</td>\n",
              "      <td>1.834972</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>-1.538149</td>\n",
              "      <td>0.036196</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100102</td>\n",
              "      <td>-0.022051</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.889684</td>\n",
              "      <td>0.036331</td>\n",
              "      <td>-0.434350</td>\n",
              "      <td>-0.014128</td>\n",
              "      <td>-1.790412</td>\n",
              "      <td>-0.049095</td>\n",
              "      <td>0.869987</td>\n",
              "      <td>0.398801</td>\n",
              "      <td>-0.501111</td>\n",
              "      <td>0.701235</td>\n",
              "      <td>-1.836419</td>\n",
              "      <td>0.602633</td>\n",
              "      <td>0.889830</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864449</td>\n",
              "      <td>0.601895</td>\n",
              "      <td>1.868931</td>\n",
              "      <td>-0.007345</td>\n",
              "      <td>-1.400215</td>\n",
              "      <td>-0.008872</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020642</td>\n",
              "      <td>0.008042</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.801732</td>\n",
              "      <td>0.166731</td>\n",
              "      <td>-0.531031</td>\n",
              "      <td>0.101154</td>\n",
              "      <td>-1.876163</td>\n",
              "      <td>0.083426</td>\n",
              "      <td>-1.318168</td>\n",
              "      <td>0.481710</td>\n",
              "      <td>1.369793</td>\n",
              "      <td>0.693499</td>\n",
              "      <td>0.109440</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.377415</td>\n",
              "      <td>-0.134235</td>\n",
              "      <td>-1.374928</td>\n",
              "      <td>-0.094753</td>\n",
              "      <td>0.655781</td>\n",
              "      <td>0.075937</td>\n",
              "      <td>-0.498681</td>\n",
              "      <td>0.058612</td>\n",
              "      <td>-1.830469</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>0.885227</td>\n",
              "      <td>-0.079381</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860780</td>\n",
              "      <td>0.614187</td>\n",
              "      <td>1.864821</td>\n",
              "      <td>-0.011410</td>\n",
              "      <td>-1.411007</td>\n",
              "      <td>-0.005342</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019336</td>\n",
              "      <td>0.008870</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.890043</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.433884</td>\n",
              "      <td>-0.013045</td>\n",
              "      <td>-1.789712</td>\n",
              "      <td>-0.048005</td>\n",
              "      <td>0.869660</td>\n",
              "      <td>0.398513</td>\n",
              "      <td>-0.500733</td>\n",
              "      <td>0.700832</td>\n",
              "      <td>-1.835701</td>\n",
              "      <td>0.600059</td>\n",
              "      <td>0.890189</td>\n",
              "      <td>-0.002797</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864586</td>\n",
              "      <td>0.601516</td>\n",
              "      <td>1.857189</td>\n",
              "      <td>-0.007370</td>\n",
              "      <td>-1.381310</td>\n",
              "      <td>-0.008874</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020636</td>\n",
              "      <td>0.019097</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.802080</td>\n",
              "      <td>0.166042</td>\n",
              "      <td>-0.530669</td>\n",
              "      <td>0.101267</td>\n",
              "      <td>-1.875407</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>-1.317960</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>1.370322</td>\n",
              "      <td>0.691744</td>\n",
              "      <td>0.109383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>-0.376972</td>\n",
              "      <td>-0.133153</td>\n",
              "      <td>-1.374439</td>\n",
              "      <td>-0.097088</td>\n",
              "      <td>0.655930</td>\n",
              "      <td>0.075577</td>\n",
              "      <td>-0.498514</td>\n",
              "      <td>0.058768</td>\n",
              "      <td>-1.829935</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.885529</td>\n",
              "      <td>-0.078267</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860917</td>\n",
              "      <td>0.614905</td>\n",
              "      <td>1.879398</td>\n",
              "      <td>-0.011434</td>\n",
              "      <td>-1.418009</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019302</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.891902</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.429791</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-1.782334</td>\n",
              "      <td>-0.049518</td>\n",
              "      <td>0.871534</td>\n",
              "      <td>0.390431</td>\n",
              "      <td>-0.495338</td>\n",
              "      <td>0.682959</td>\n",
              "      <td>-1.830180</td>\n",
              "      <td>0.595921</td>\n",
              "      <td>0.892105</td>\n",
              "      <td>-0.007788</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.866303</td>\n",
              "      <td>0.590921</td>\n",
              "      <td>1.891025</td>\n",
              "      <td>-0.023903</td>\n",
              "      <td>-1.407753</td>\n",
              "      <td>-0.010136</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>0.010396</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.804544</td>\n",
              "      <td>0.167318</td>\n",
              "      <td>-0.525924</td>\n",
              "      <td>0.102684</td>\n",
              "      <td>-1.867290</td>\n",
              "      <td>0.085049</td>\n",
              "      <td>-1.314364</td>\n",
              "      <td>0.478504</td>\n",
              "      <td>1.372069</td>\n",
              "      <td>0.677139</td>\n",
              "      <td>0.111401</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>-0.373513</td>\n",
              "      <td>-0.137835</td>\n",
              "      <td>-1.368761</td>\n",
              "      <td>-0.095179</td>\n",
              "      <td>0.657141</td>\n",
              "      <td>0.074537</td>\n",
              "      <td>-0.493178</td>\n",
              "      <td>0.057356</td>\n",
              "      <td>-1.824012</td>\n",
              "      <td>0.047841</td>\n",
              "      <td>0.887439</td>\n",
              "      <td>-0.079738</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.862749</td>\n",
              "      <td>0.605183</td>\n",
              "      <td>1.892994</td>\n",
              "      <td>-0.027845</td>\n",
              "      <td>-1.432902</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019377</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893597</td>\n",
              "      <td>0.033239</td>\n",
              "      <td>-0.423901</td>\n",
              "      <td>-0.017844</td>\n",
              "      <td>-1.770758</td>\n",
              "      <td>-0.052923</td>\n",
              "      <td>0.868269</td>\n",
              "      <td>0.389099</td>\n",
              "      <td>-0.489603</td>\n",
              "      <td>0.661865</td>\n",
              "      <td>-1.824963</td>\n",
              "      <td>0.604001</td>\n",
              "      <td>0.893742</td>\n",
              "      <td>-0.007717</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864216</td>\n",
              "      <td>0.586934</td>\n",
              "      <td>1.884543</td>\n",
              "      <td>-0.063127</td>\n",
              "      <td>-1.447021</td>\n",
              "      <td>-0.012640</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020903</td>\n",
              "      <td>0.037047</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.807090</td>\n",
              "      <td>0.167550</td>\n",
              "      <td>-0.519189</td>\n",
              "      <td>0.103949</td>\n",
              "      <td>-1.854435</td>\n",
              "      <td>0.085472</td>\n",
              "      <td>-1.310773</td>\n",
              "      <td>0.475735</td>\n",
              "      <td>1.371320</td>\n",
              "      <td>0.654512</td>\n",
              "      <td>0.108615</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003178</td>\n",
              "      <td>-0.368471</td>\n",
              "      <td>-0.137439</td>\n",
              "      <td>-1.359878</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>0.654775</td>\n",
              "      <td>0.074295</td>\n",
              "      <td>-0.487400</td>\n",
              "      <td>0.055487</td>\n",
              "      <td>-1.819112</td>\n",
              "      <td>0.048351</td>\n",
              "      <td>0.889183</td>\n",
              "      <td>-0.083081</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860604</td>\n",
              "      <td>0.598972</td>\n",
              "      <td>1.887278</td>\n",
              "      <td>-0.063596</td>\n",
              "      <td>-1.489143</td>\n",
              "      <td>-0.052365</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019594</td>\n",
              "      <td>0.009072</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       125      126       127\n",
              "0  0.874304  0.099036 -0.474731  ... -0.007144 -0.00943 -0.008728\n",
              "1  0.889684  0.036331 -0.434350  ... -0.007144 -0.00943 -0.008728\n",
              "2  0.890043  0.037227 -0.433884  ... -0.007144 -0.00943 -0.008728\n",
              "3  0.891902  0.036003 -0.429791  ... -0.007144 -0.00943 -0.008728\n",
              "4  0.893597  0.033239 -0.423901  ... -0.007144 -0.00943 -0.008728\n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvcfayXyCL2P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "b083d9b9-8e0f-4151-fcef-c6d66db876a3"
      },
      "source": [
        "# categorical target y to array \n",
        "from keras.utils.np_utils import to_categorical\n",
        " \n",
        "y_cat = to_categorical(y,2)\n",
        "y_cat[:10]\n",
        "y_cat.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       ...,\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgzTHegfCOd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#divide datasets into to part training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x.values, y_cat,test_size=0.1,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCXKdHA6CQzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaaB-JQ2Cq3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshape x_train,x_test,y_train and y_test\n",
        "X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
        "X_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
        "Y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1]))\n",
        "Y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COj9IQpiFqsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "007d6b9e-fd52-4016-ae6b-b6eab239e124"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIq-l4K9FuIx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "# patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_lstm_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "rp = ReduceLROnPlateau(monitor='val_accuracy',patience = 5,verbose=1,factor=0.5,min_lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WShjzDYnD1D0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "477a1baf-af63-41b6-f9b7-e2207de8d9f6"
      },
      "source": [
        "i=x_train.shape[1]\n",
        " \n",
        "model = tf.keras.Sequential()\n",
        " \n",
        "model.add(layers.LSTM(128, activation='tanh',input_shape=(1, i), return_sequences=True,kernel_initializer='he_uniform'))\n",
        "model.add(layers.LSTM(256, activation='tanh', return_sequences=True,kernel_initializer='he_uniform'))\n",
        "model.add(layers.LSTM(256, activation='tanh', return_sequences=True,kernel_initializer='he_uniform'))\n",
        "model.add(layers.LSTM(128, activation='tanh', return_sequences=False,kernel_initializer='he_uniform'))\n",
        "model.add(layers.Dense(2,activation='sigmoid',kernel_initializer='he_uniform'))\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=tf.optimizers.Adam(0.001,decay=0.000005), metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_52 (LSTM)               (None, 1, 128)            131584    \n",
            "_________________________________________________________________\n",
            "lstm_53 (LSTM)               (None, 1, 256)            394240    \n",
            "_________________________________________________________________\n",
            "lstm_54 (LSTM)               (None, 1, 256)            525312    \n",
            "_________________________________________________________________\n",
            "lstm_55 (LSTM)               (None, 128)               197120    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 1,248,514\n",
            "Trainable params: 1,248,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNrKqj-cFYST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1b41c09-3d43-4206-8027-3c367cf5a714"
      },
      "source": [
        "start_time = time.time()\n",
        "fit1 = model.fit(\n",
        "    X_train, y_train,batch_size=500,\n",
        "    validation_split=0.1,epochs=250, verbose=1,callbacks=[es, mc,rp],validation_data=(X_test, y_test))\n",
        "score=model.evaluate(X_test,y_test)\n",
        "print('Accuracy(on Test-data): ' + str(score[1]))\n",
        "duration = time.time() - start_time\n",
        "print(\"time of training (s)\", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5976 - accuracy: 0.7091\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71236, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 15s 115ms/step - loss: 0.5976 - accuracy: 0.7091 - val_loss: 0.5792 - val_accuracy: 0.7124\n",
            "Epoch 2/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5730 - accuracy: 0.7175\n",
            "Epoch 00002: val_accuracy improved from 0.71236 to 0.71661, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.5730 - accuracy: 0.7175 - val_loss: 0.5725 - val_accuracy: 0.7166\n",
            "Epoch 3/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5600 - accuracy: 0.7219\n",
            "Epoch 00003: val_accuracy improved from 0.71661 to 0.71959, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.5600 - accuracy: 0.7219 - val_loss: 0.5589 - val_accuracy: 0.7196\n",
            "Epoch 4/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.7263\n",
            "Epoch 00004: val_accuracy improved from 0.71959 to 0.72682, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.5483 - accuracy: 0.7263 - val_loss: 0.5474 - val_accuracy: 0.7268\n",
            "Epoch 5/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5357 - accuracy: 0.7313\n",
            "Epoch 00005: val_accuracy did not improve from 0.72682\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.5357 - accuracy: 0.7313 - val_loss: 0.5418 - val_accuracy: 0.7257\n",
            "Epoch 6/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.7349\n",
            "Epoch 00006: val_accuracy improved from 0.72682 to 0.72937, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.5242 - accuracy: 0.7349 - val_loss: 0.5307 - val_accuracy: 0.7294\n",
            "Epoch 7/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5161 - accuracy: 0.7378\n",
            "Epoch 00007: val_accuracy improved from 0.72937 to 0.73334, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.5161 - accuracy: 0.7378 - val_loss: 0.5293 - val_accuracy: 0.7333\n",
            "Epoch 8/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.7406\n",
            "Epoch 00008: val_accuracy improved from 0.73334 to 0.73348, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.5082 - accuracy: 0.7406 - val_loss: 0.5286 - val_accuracy: 0.7335\n",
            "Epoch 9/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5001 - accuracy: 0.7411\n",
            "Epoch 00009: val_accuracy did not improve from 0.73348\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.5001 - accuracy: 0.7411 - val_loss: 0.5271 - val_accuracy: 0.7268\n",
            "Epoch 10/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4940 - accuracy: 0.7436\n",
            "Epoch 00010: val_accuracy did not improve from 0.73348\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4940 - accuracy: 0.7436 - val_loss: 0.5306 - val_accuracy: 0.7284\n",
            "Epoch 11/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4902 - accuracy: 0.7453\n",
            "Epoch 00011: val_accuracy did not improve from 0.73348\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.4902 - accuracy: 0.7453 - val_loss: 0.5271 - val_accuracy: 0.7309\n",
            "Epoch 12/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4864 - accuracy: 0.7476\n",
            "Epoch 00012: val_accuracy improved from 0.73348 to 0.73476, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4864 - accuracy: 0.7476 - val_loss: 0.5272 - val_accuracy: 0.7348\n",
            "Epoch 13/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.7467\n",
            "Epoch 00013: val_accuracy did not improve from 0.73476\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4859 - accuracy: 0.7467 - val_loss: 0.5268 - val_accuracy: 0.7295\n",
            "Epoch 14/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.7486\n",
            "Epoch 00014: val_accuracy improved from 0.73476 to 0.73660, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4815 - accuracy: 0.7486 - val_loss: 0.5223 - val_accuracy: 0.7366\n",
            "Epoch 15/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4819 - accuracy: 0.7474\n",
            "Epoch 00015: val_accuracy did not improve from 0.73660\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4819 - accuracy: 0.7474 - val_loss: 0.5240 - val_accuracy: 0.7348\n",
            "Epoch 16/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.7521\n",
            "Epoch 00016: val_accuracy did not improve from 0.73660\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.4758 - accuracy: 0.7521 - val_loss: 0.5326 - val_accuracy: 0.7349\n",
            "Epoch 17/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4754 - accuracy: 0.7517\n",
            "Epoch 00017: val_accuracy did not improve from 0.73660\n",
            "127/127 [==============================] - 14s 113ms/step - loss: 0.4754 - accuracy: 0.7517 - val_loss: 0.5312 - val_accuracy: 0.7349\n",
            "Epoch 18/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4757 - accuracy: 0.7518\n",
            "Epoch 00018: val_accuracy did not improve from 0.73660\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4757 - accuracy: 0.7518 - val_loss: 0.5253 - val_accuracy: 0.7350\n",
            "Epoch 19/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4720 - accuracy: 0.7524\n",
            "Epoch 00019: val_accuracy improved from 0.73660 to 0.73703, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.4720 - accuracy: 0.7524 - val_loss: 0.5252 - val_accuracy: 0.7370\n",
            "Epoch 20/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.7540\n",
            "Epoch 00020: val_accuracy did not improve from 0.73703\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4714 - accuracy: 0.7540 - val_loss: 0.5332 - val_accuracy: 0.7349\n",
            "Epoch 21/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4693 - accuracy: 0.7541\n",
            "Epoch 00021: val_accuracy did not improve from 0.73703\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4693 - accuracy: 0.7541 - val_loss: 0.5223 - val_accuracy: 0.7346\n",
            "Epoch 22/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.7545\n",
            "Epoch 00022: val_accuracy improved from 0.73703 to 0.73845, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4690 - accuracy: 0.7545 - val_loss: 0.5233 - val_accuracy: 0.7384\n",
            "Epoch 23/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4657 - accuracy: 0.7569\n",
            "Epoch 00023: val_accuracy did not improve from 0.73845\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4657 - accuracy: 0.7569 - val_loss: 0.5208 - val_accuracy: 0.7370\n",
            "Epoch 24/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4626 - accuracy: 0.7606\n",
            "Epoch 00024: val_accuracy improved from 0.73845 to 0.74270, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4626 - accuracy: 0.7606 - val_loss: 0.5227 - val_accuracy: 0.7427\n",
            "Epoch 25/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4605 - accuracy: 0.7605\n",
            "Epoch 00025: val_accuracy improved from 0.74270 to 0.74298, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.4605 - accuracy: 0.7605 - val_loss: 0.5268 - val_accuracy: 0.7430\n",
            "Epoch 26/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4608 - accuracy: 0.7607\n",
            "Epoch 00026: val_accuracy did not improve from 0.74298\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4608 - accuracy: 0.7607 - val_loss: 0.5276 - val_accuracy: 0.7420\n",
            "Epoch 27/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.7626\n",
            "Epoch 00027: val_accuracy did not improve from 0.74298\n",
            "127/127 [==============================] - 13s 102ms/step - loss: 0.4580 - accuracy: 0.7626 - val_loss: 0.5255 - val_accuracy: 0.7399\n",
            "Epoch 28/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4564 - accuracy: 0.7633\n",
            "Epoch 00028: val_accuracy did not improve from 0.74298\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4564 - accuracy: 0.7633 - val_loss: 0.5349 - val_accuracy: 0.7393\n",
            "Epoch 29/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4554 - accuracy: 0.7650\n",
            "Epoch 00029: val_accuracy did not improve from 0.74298\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4554 - accuracy: 0.7650 - val_loss: 0.5293 - val_accuracy: 0.7421\n",
            "Epoch 30/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.7667\n",
            "Epoch 00030: val_accuracy improved from 0.74298 to 0.74412, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.4534 - accuracy: 0.7667 - val_loss: 0.5222 - val_accuracy: 0.7441\n",
            "Epoch 31/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.7678\n",
            "Epoch 00031: val_accuracy did not improve from 0.74412\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4517 - accuracy: 0.7678 - val_loss: 0.5279 - val_accuracy: 0.7400\n",
            "Epoch 32/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4506 - accuracy: 0.7677\n",
            "Epoch 00032: val_accuracy did not improve from 0.74412\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4506 - accuracy: 0.7677 - val_loss: 0.5262 - val_accuracy: 0.7440\n",
            "Epoch 33/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4472 - accuracy: 0.7702\n",
            "Epoch 00033: val_accuracy did not improve from 0.74412\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4472 - accuracy: 0.7702 - val_loss: 0.5289 - val_accuracy: 0.7426\n",
            "Epoch 34/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.7696\n",
            "Epoch 00034: val_accuracy did not improve from 0.74412\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.4461 - accuracy: 0.7696 - val_loss: 0.5284 - val_accuracy: 0.7414\n",
            "Epoch 35/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.7716\n",
            "Epoch 00035: val_accuracy did not improve from 0.74412\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.4444 - accuracy: 0.7716 - val_loss: 0.5359 - val_accuracy: 0.7382\n",
            "Epoch 36/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4419 - accuracy: 0.7726\n",
            "Epoch 00036: val_accuracy improved from 0.74412 to 0.74440, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4419 - accuracy: 0.7726 - val_loss: 0.5370 - val_accuracy: 0.7444\n",
            "Epoch 37/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.7742\n",
            "Epoch 00037: val_accuracy improved from 0.74440 to 0.74596, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4407 - accuracy: 0.7742 - val_loss: 0.5306 - val_accuracy: 0.7460\n",
            "Epoch 38/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.7754\n",
            "Epoch 00038: val_accuracy did not improve from 0.74596\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4392 - accuracy: 0.7754 - val_loss: 0.5220 - val_accuracy: 0.7440\n",
            "Epoch 39/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4372 - accuracy: 0.7766\n",
            "Epoch 00039: val_accuracy did not improve from 0.74596\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4372 - accuracy: 0.7766 - val_loss: 0.5262 - val_accuracy: 0.7438\n",
            "Epoch 40/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.7777\n",
            "Epoch 00040: val_accuracy did not improve from 0.74596\n",
            "127/127 [==============================] - 14s 114ms/step - loss: 0.4351 - accuracy: 0.7777 - val_loss: 0.5268 - val_accuracy: 0.7458\n",
            "Epoch 41/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.7779\n",
            "Epoch 00041: val_accuracy improved from 0.74596 to 0.74653, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 15s 116ms/step - loss: 0.4345 - accuracy: 0.7779 - val_loss: 0.5250 - val_accuracy: 0.7465\n",
            "Epoch 42/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.7786\n",
            "Epoch 00042: val_accuracy did not improve from 0.74653\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4329 - accuracy: 0.7786 - val_loss: 0.5218 - val_accuracy: 0.7465\n",
            "Epoch 43/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4308 - accuracy: 0.7793\n",
            "Epoch 00043: val_accuracy improved from 0.74653 to 0.74880, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.4308 - accuracy: 0.7793 - val_loss: 0.5253 - val_accuracy: 0.7488\n",
            "Epoch 44/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.7810\n",
            "Epoch 00044: val_accuracy did not improve from 0.74880\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4290 - accuracy: 0.7810 - val_loss: 0.5300 - val_accuracy: 0.7444\n",
            "Epoch 45/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.7827\n",
            "Epoch 00045: val_accuracy improved from 0.74880 to 0.74908, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4264 - accuracy: 0.7827 - val_loss: 0.5350 - val_accuracy: 0.7491\n",
            "Epoch 46/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4251 - accuracy: 0.7823\n",
            "Epoch 00046: val_accuracy improved from 0.74908 to 0.74936, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4251 - accuracy: 0.7823 - val_loss: 0.5248 - val_accuracy: 0.7494\n",
            "Epoch 47/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.7844\n",
            "Epoch 00047: val_accuracy improved from 0.74936 to 0.75064, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4233 - accuracy: 0.7844 - val_loss: 0.5257 - val_accuracy: 0.7506\n",
            "Epoch 48/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4211 - accuracy: 0.7859\n",
            "Epoch 00048: val_accuracy improved from 0.75064 to 0.75191, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.4211 - accuracy: 0.7859 - val_loss: 0.5355 - val_accuracy: 0.7519\n",
            "Epoch 49/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.7865\n",
            "Epoch 00049: val_accuracy did not improve from 0.75191\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4187 - accuracy: 0.7865 - val_loss: 0.5412 - val_accuracy: 0.7501\n",
            "Epoch 50/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.7884\n",
            "Epoch 00050: val_accuracy did not improve from 0.75191\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4162 - accuracy: 0.7884 - val_loss: 0.5323 - val_accuracy: 0.7477\n",
            "Epoch 51/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4144 - accuracy: 0.7887\n",
            "Epoch 00051: val_accuracy did not improve from 0.75191\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.4144 - accuracy: 0.7887 - val_loss: 0.5380 - val_accuracy: 0.7508\n",
            "Epoch 52/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4135 - accuracy: 0.7889\n",
            "Epoch 00052: val_accuracy did not improve from 0.75191\n",
            "127/127 [==============================] - 14s 112ms/step - loss: 0.4135 - accuracy: 0.7889 - val_loss: 0.5334 - val_accuracy: 0.7488\n",
            "Epoch 53/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4152 - accuracy: 0.7889\n",
            "Epoch 00053: val_accuracy improved from 0.75191 to 0.75659, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4152 - accuracy: 0.7889 - val_loss: 0.5175 - val_accuracy: 0.7566\n",
            "Epoch 54/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4134 - accuracy: 0.7900\n",
            "Epoch 00054: val_accuracy did not improve from 0.75659\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4134 - accuracy: 0.7900 - val_loss: 0.5245 - val_accuracy: 0.7553\n",
            "Epoch 55/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.7917\n",
            "Epoch 00055: val_accuracy did not improve from 0.75659\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4089 - accuracy: 0.7917 - val_loss: 0.5275 - val_accuracy: 0.7523\n",
            "Epoch 56/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.7933\n",
            "Epoch 00056: val_accuracy improved from 0.75659 to 0.76155, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.4051 - accuracy: 0.7933 - val_loss: 0.5182 - val_accuracy: 0.7616\n",
            "Epoch 57/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.7955\n",
            "Epoch 00057: val_accuracy did not improve from 0.76155\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.4046 - accuracy: 0.7955 - val_loss: 0.5298 - val_accuracy: 0.7545\n",
            "Epoch 58/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4033 - accuracy: 0.7943\n",
            "Epoch 00058: val_accuracy did not improve from 0.76155\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4033 - accuracy: 0.7943 - val_loss: 0.5331 - val_accuracy: 0.7559\n",
            "Epoch 59/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.7971\n",
            "Epoch 00059: val_accuracy did not improve from 0.76155\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.4002 - accuracy: 0.7971 - val_loss: 0.5371 - val_accuracy: 0.7547\n",
            "Epoch 60/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3975 - accuracy: 0.7980\n",
            "Epoch 00060: val_accuracy did not improve from 0.76155\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3975 - accuracy: 0.7980 - val_loss: 0.5314 - val_accuracy: 0.7593\n",
            "Epoch 61/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8003\n",
            "Epoch 00061: val_accuracy did not improve from 0.76155\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3959 - accuracy: 0.8003 - val_loss: 0.5448 - val_accuracy: 0.7538\n",
            "Epoch 62/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3952 - accuracy: 0.7999\n",
            "Epoch 00062: val_accuracy did not improve from 0.76155\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3952 - accuracy: 0.7999 - val_loss: 0.5402 - val_accuracy: 0.7582\n",
            "Epoch 63/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.8015\n",
            "Epoch 00063: val_accuracy did not improve from 0.76155\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3916 - accuracy: 0.8015 - val_loss: 0.5345 - val_accuracy: 0.7523\n",
            "Epoch 64/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8040\n",
            "Epoch 00064: val_accuracy improved from 0.76155 to 0.76354, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 112ms/step - loss: 0.3896 - accuracy: 0.8040 - val_loss: 0.5309 - val_accuracy: 0.7635\n",
            "Epoch 65/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3855 - accuracy: 0.8049\n",
            "Epoch 00065: val_accuracy improved from 0.76354 to 0.76496, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3855 - accuracy: 0.8049 - val_loss: 0.5250 - val_accuracy: 0.7650\n",
            "Epoch 66/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3846 - accuracy: 0.8061\n",
            "Epoch 00066: val_accuracy improved from 0.76496 to 0.76864, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3846 - accuracy: 0.8061 - val_loss: 0.5347 - val_accuracy: 0.7686\n",
            "Epoch 67/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8067\n",
            "Epoch 00067: val_accuracy did not improve from 0.76864\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.3824 - accuracy: 0.8067 - val_loss: 0.5449 - val_accuracy: 0.7610\n",
            "Epoch 68/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3776 - accuracy: 0.8089\n",
            "Epoch 00068: val_accuracy did not improve from 0.76864\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.3776 - accuracy: 0.8089 - val_loss: 0.5424 - val_accuracy: 0.7623\n",
            "Epoch 69/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8094\n",
            "Epoch 00069: val_accuracy improved from 0.76864 to 0.76949, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3770 - accuracy: 0.8094 - val_loss: 0.5335 - val_accuracy: 0.7695\n",
            "Epoch 70/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3756 - accuracy: 0.8122\n",
            "Epoch 00070: val_accuracy did not improve from 0.76949\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.3756 - accuracy: 0.8122 - val_loss: 0.5335 - val_accuracy: 0.7611\n",
            "Epoch 71/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8130\n",
            "Epoch 00071: val_accuracy did not improve from 0.76949\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3735 - accuracy: 0.8130 - val_loss: 0.5385 - val_accuracy: 0.7679\n",
            "Epoch 72/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.8144\n",
            "Epoch 00072: val_accuracy did not improve from 0.76949\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.3680 - accuracy: 0.8144 - val_loss: 0.5382 - val_accuracy: 0.7665\n",
            "Epoch 73/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3646 - accuracy: 0.8181\n",
            "Epoch 00073: val_accuracy did not improve from 0.76949\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3646 - accuracy: 0.8181 - val_loss: 0.5505 - val_accuracy: 0.7658\n",
            "Epoch 74/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.8178\n",
            "Epoch 00074: val_accuracy did not improve from 0.76949\n",
            "\n",
            "Epoch 00074: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.3638 - accuracy: 0.8178 - val_loss: 0.5395 - val_accuracy: 0.7692\n",
            "Epoch 75/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3611 - accuracy: 0.8188\n",
            "Epoch 00075: val_accuracy improved from 0.76949 to 0.77233, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 111ms/step - loss: 0.3611 - accuracy: 0.8188 - val_loss: 0.5412 - val_accuracy: 0.7723\n",
            "Epoch 76/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8209\n",
            "Epoch 00076: val_accuracy did not improve from 0.77233\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.3549 - accuracy: 0.8209 - val_loss: 0.5530 - val_accuracy: 0.7627\n",
            "Epoch 77/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8213\n",
            "Epoch 00077: val_accuracy improved from 0.77233 to 0.77771, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.3536 - accuracy: 0.8213 - val_loss: 0.5346 - val_accuracy: 0.7777\n",
            "Epoch 78/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8255\n",
            "Epoch 00078: val_accuracy did not improve from 0.77771\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3455 - accuracy: 0.8255 - val_loss: 0.5493 - val_accuracy: 0.7711\n",
            "Epoch 79/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3479 - accuracy: 0.8255\n",
            "Epoch 00079: val_accuracy did not improve from 0.77771\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.3479 - accuracy: 0.8255 - val_loss: 0.5519 - val_accuracy: 0.7719\n",
            "Epoch 80/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3424 - accuracy: 0.8274\n",
            "Epoch 00080: val_accuracy improved from 0.77771 to 0.77800, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.3424 - accuracy: 0.8274 - val_loss: 0.5433 - val_accuracy: 0.7780\n",
            "Epoch 81/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.8273\n",
            "Epoch 00081: val_accuracy did not improve from 0.77800\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.3399 - accuracy: 0.8273 - val_loss: 0.5433 - val_accuracy: 0.7746\n",
            "Epoch 82/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.8316\n",
            "Epoch 00082: val_accuracy did not improve from 0.77800\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3368 - accuracy: 0.8316 - val_loss: 0.5546 - val_accuracy: 0.7754\n",
            "Epoch 83/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.8345\n",
            "Epoch 00083: val_accuracy improved from 0.77800 to 0.77814, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.3316 - accuracy: 0.8345 - val_loss: 0.5452 - val_accuracy: 0.7781\n",
            "Epoch 84/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.8376\n",
            "Epoch 00084: val_accuracy improved from 0.77814 to 0.77942, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.3276 - accuracy: 0.8376 - val_loss: 0.5627 - val_accuracy: 0.7794\n",
            "Epoch 85/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.8377\n",
            "Epoch 00085: val_accuracy did not improve from 0.77942\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.3278 - accuracy: 0.8377 - val_loss: 0.5481 - val_accuracy: 0.7746\n",
            "Epoch 86/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.8402\n",
            "Epoch 00086: val_accuracy improved from 0.77942 to 0.78594, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.3217 - accuracy: 0.8402 - val_loss: 0.5527 - val_accuracy: 0.7859\n",
            "Epoch 87/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.8431\n",
            "Epoch 00087: val_accuracy improved from 0.78594 to 0.79033, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 113ms/step - loss: 0.3169 - accuracy: 0.8431 - val_loss: 0.5528 - val_accuracy: 0.7903\n",
            "Epoch 88/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8441\n",
            "Epoch 00088: val_accuracy improved from 0.79033 to 0.79161, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.3121 - accuracy: 0.8441 - val_loss: 0.5451 - val_accuracy: 0.7916\n",
            "Epoch 89/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8473\n",
            "Epoch 00089: val_accuracy improved from 0.79161 to 0.79246, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.3072 - accuracy: 0.8473 - val_loss: 0.5483 - val_accuracy: 0.7925\n",
            "Epoch 90/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3030 - accuracy: 0.8507\n",
            "Epoch 00090: val_accuracy did not improve from 0.79246\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.3030 - accuracy: 0.8507 - val_loss: 0.5598 - val_accuracy: 0.7869\n",
            "Epoch 91/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3018 - accuracy: 0.8516\n",
            "Epoch 00091: val_accuracy improved from 0.79246 to 0.79558, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.3018 - accuracy: 0.8516 - val_loss: 0.5539 - val_accuracy: 0.7956\n",
            "Epoch 92/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2971 - accuracy: 0.8535\n",
            "Epoch 00092: val_accuracy did not improve from 0.79558\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.2971 - accuracy: 0.8535 - val_loss: 0.5605 - val_accuracy: 0.7933\n",
            "Epoch 93/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2921 - accuracy: 0.8572\n",
            "Epoch 00093: val_accuracy improved from 0.79558 to 0.79685, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.2921 - accuracy: 0.8572 - val_loss: 0.5595 - val_accuracy: 0.7969\n",
            "Epoch 94/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.8586\n",
            "Epoch 00094: val_accuracy improved from 0.79685 to 0.79855, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.2891 - accuracy: 0.8586 - val_loss: 0.5598 - val_accuracy: 0.7986\n",
            "Epoch 95/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8615\n",
            "Epoch 00095: val_accuracy improved from 0.79855 to 0.80096, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.2856 - accuracy: 0.8615 - val_loss: 0.5585 - val_accuracy: 0.8010\n",
            "Epoch 96/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2804 - accuracy: 0.8631\n",
            "Epoch 00096: val_accuracy did not improve from 0.80096\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.2804 - accuracy: 0.8631 - val_loss: 0.5623 - val_accuracy: 0.7988\n",
            "Epoch 97/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8663\n",
            "Epoch 00097: val_accuracy improved from 0.80096 to 0.80238, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.2769 - accuracy: 0.8663 - val_loss: 0.5575 - val_accuracy: 0.8024\n",
            "Epoch 98/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2732 - accuracy: 0.8679\n",
            "Epoch 00098: val_accuracy improved from 0.80238 to 0.80422, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.2732 - accuracy: 0.8679 - val_loss: 0.5604 - val_accuracy: 0.8042\n",
            "Epoch 99/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8694\n",
            "Epoch 00099: val_accuracy improved from 0.80422 to 0.80862, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.2702 - accuracy: 0.8694 - val_loss: 0.5615 - val_accuracy: 0.8086\n",
            "Epoch 100/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.8723\n",
            "Epoch 00100: val_accuracy improved from 0.80862 to 0.81117, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.2684 - accuracy: 0.8723 - val_loss: 0.5558 - val_accuracy: 0.8112\n",
            "Epoch 101/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.8752\n",
            "Epoch 00101: val_accuracy improved from 0.81117 to 0.81415, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.2613 - accuracy: 0.8752 - val_loss: 0.5732 - val_accuracy: 0.8141\n",
            "Epoch 102/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2579 - accuracy: 0.8772\n",
            "Epoch 00102: val_accuracy did not improve from 0.81415\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.2579 - accuracy: 0.8772 - val_loss: 0.5758 - val_accuracy: 0.8068\n",
            "Epoch 103/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8787\n",
            "Epoch 00103: val_accuracy did not improve from 0.81415\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.2554 - accuracy: 0.8787 - val_loss: 0.5770 - val_accuracy: 0.8141\n",
            "Epoch 104/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2486 - accuracy: 0.8822\n",
            "Epoch 00104: val_accuracy did not improve from 0.81415\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.2486 - accuracy: 0.8822 - val_loss: 0.5741 - val_accuracy: 0.8136\n",
            "Epoch 105/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.8837\n",
            "Epoch 00105: val_accuracy improved from 0.81415 to 0.81954, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.2455 - accuracy: 0.8837 - val_loss: 0.5829 - val_accuracy: 0.8195\n",
            "Epoch 106/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2429 - accuracy: 0.8859\n",
            "Epoch 00106: val_accuracy did not improve from 0.81954\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.2429 - accuracy: 0.8859 - val_loss: 0.5749 - val_accuracy: 0.8173\n",
            "Epoch 107/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2392 - accuracy: 0.8878\n",
            "Epoch 00107: val_accuracy did not improve from 0.81954\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.2392 - accuracy: 0.8878 - val_loss: 0.5813 - val_accuracy: 0.8171\n",
            "Epoch 108/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.8909\n",
            "Epoch 00108: val_accuracy improved from 0.81954 to 0.82506, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.2341 - accuracy: 0.8909 - val_loss: 0.5640 - val_accuracy: 0.8251\n",
            "Epoch 109/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.8941\n",
            "Epoch 00109: val_accuracy did not improve from 0.82506\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.2316 - accuracy: 0.8941 - val_loss: 0.5800 - val_accuracy: 0.8245\n",
            "Epoch 110/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.8944\n",
            "Epoch 00110: val_accuracy did not improve from 0.82506\n",
            "127/127 [==============================] - 14s 112ms/step - loss: 0.2283 - accuracy: 0.8944 - val_loss: 0.5761 - val_accuracy: 0.8212\n",
            "Epoch 111/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.8968\n",
            "Epoch 00111: val_accuracy improved from 0.82506 to 0.82747, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.2247 - accuracy: 0.8968 - val_loss: 0.5806 - val_accuracy: 0.8275\n",
            "Epoch 112/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.8987\n",
            "Epoch 00112: val_accuracy improved from 0.82747 to 0.83003, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.2193 - accuracy: 0.8987 - val_loss: 0.5953 - val_accuracy: 0.8300\n",
            "Epoch 113/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9033\n",
            "Epoch 00113: val_accuracy improved from 0.83003 to 0.83258, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.2145 - accuracy: 0.9033 - val_loss: 0.5901 - val_accuracy: 0.8326\n",
            "Epoch 114/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2111 - accuracy: 0.9039\n",
            "Epoch 00114: val_accuracy improved from 0.83258 to 0.83371, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.2111 - accuracy: 0.9039 - val_loss: 0.6036 - val_accuracy: 0.8337\n",
            "Epoch 115/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9047\n",
            "Epoch 00115: val_accuracy improved from 0.83371 to 0.83867, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.2101 - accuracy: 0.9047 - val_loss: 0.5848 - val_accuracy: 0.8387\n",
            "Epoch 116/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9066\n",
            "Epoch 00116: val_accuracy did not improve from 0.83867\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.2054 - accuracy: 0.9066 - val_loss: 0.6017 - val_accuracy: 0.8340\n",
            "Epoch 117/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1996 - accuracy: 0.9099\n",
            "Epoch 00117: val_accuracy did not improve from 0.83867\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1996 - accuracy: 0.9099 - val_loss: 0.6029 - val_accuracy: 0.8350\n",
            "Epoch 118/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.9110\n",
            "Epoch 00118: val_accuracy improved from 0.83867 to 0.83995, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1969 - accuracy: 0.9110 - val_loss: 0.5980 - val_accuracy: 0.8399\n",
            "Epoch 119/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9134\n",
            "Epoch 00119: val_accuracy did not improve from 0.83995\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1930 - accuracy: 0.9134 - val_loss: 0.6158 - val_accuracy: 0.8397\n",
            "Epoch 120/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1919 - accuracy: 0.9139\n",
            "Epoch 00120: val_accuracy improved from 0.83995 to 0.84633, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.1919 - accuracy: 0.9139 - val_loss: 0.6127 - val_accuracy: 0.8463\n",
            "Epoch 121/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9159\n",
            "Epoch 00121: val_accuracy did not improve from 0.84633\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1875 - accuracy: 0.9159 - val_loss: 0.6164 - val_accuracy: 0.8441\n",
            "Epoch 122/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9145\n",
            "Epoch 00122: val_accuracy did not improve from 0.84633\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1885 - accuracy: 0.9145 - val_loss: 0.6154 - val_accuracy: 0.8401\n",
            "Epoch 123/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1853 - accuracy: 0.9178\n",
            "Epoch 00123: val_accuracy did not improve from 0.84633\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1853 - accuracy: 0.9178 - val_loss: 0.6063 - val_accuracy: 0.8439\n",
            "Epoch 124/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1876 - accuracy: 0.9159\n",
            "Epoch 00124: val_accuracy did not improve from 0.84633\n",
            "127/127 [==============================] - 13s 102ms/step - loss: 0.1876 - accuracy: 0.9159 - val_loss: 0.6041 - val_accuracy: 0.8436\n",
            "Epoch 125/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.9193\n",
            "Epoch 00125: val_accuracy improved from 0.84633 to 0.84803, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1821 - accuracy: 0.9193 - val_loss: 0.6016 - val_accuracy: 0.8480\n",
            "Epoch 126/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1749 - accuracy: 0.9222\n",
            "Epoch 00126: val_accuracy improved from 0.84803 to 0.84987, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1749 - accuracy: 0.9222 - val_loss: 0.5931 - val_accuracy: 0.8499\n",
            "Epoch 127/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9252\n",
            "Epoch 00127: val_accuracy did not improve from 0.84987\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1699 - accuracy: 0.9252 - val_loss: 0.6074 - val_accuracy: 0.8404\n",
            "Epoch 128/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9257\n",
            "Epoch 00128: val_accuracy did not improve from 0.84987\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1682 - accuracy: 0.9257 - val_loss: 0.6190 - val_accuracy: 0.8460\n",
            "Epoch 129/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9268\n",
            "Epoch 00129: val_accuracy improved from 0.84987 to 0.85724, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1639 - accuracy: 0.9268 - val_loss: 0.6053 - val_accuracy: 0.8572\n",
            "Epoch 130/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9312\n",
            "Epoch 00130: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1571 - accuracy: 0.9312 - val_loss: 0.6180 - val_accuracy: 0.8516\n",
            "Epoch 131/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9313\n",
            "Epoch 00131: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1570 - accuracy: 0.9313 - val_loss: 0.6542 - val_accuracy: 0.8456\n",
            "Epoch 132/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1526 - accuracy: 0.9335\n",
            "Epoch 00132: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1526 - accuracy: 0.9335 - val_loss: 0.6346 - val_accuracy: 0.8546\n",
            "Epoch 133/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9337\n",
            "Epoch 00133: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 14s 113ms/step - loss: 0.1498 - accuracy: 0.9337 - val_loss: 0.6387 - val_accuracy: 0.8557\n",
            "Epoch 134/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1511 - accuracy: 0.9343\n",
            "Epoch 00134: val_accuracy did not improve from 0.85724\n",
            "\n",
            "Epoch 00134: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.1511 - accuracy: 0.9343 - val_loss: 0.6347 - val_accuracy: 0.8534\n",
            "Epoch 135/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.9322\n",
            "Epoch 00135: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1513 - accuracy: 0.9322 - val_loss: 0.6555 - val_accuracy: 0.8544\n",
            "Epoch 136/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9361\n",
            "Epoch 00136: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.1463 - accuracy: 0.9361 - val_loss: 0.6464 - val_accuracy: 0.8563\n",
            "Epoch 137/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1392 - accuracy: 0.9402\n",
            "Epoch 00137: val_accuracy did not improve from 0.85724\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1392 - accuracy: 0.9402 - val_loss: 0.6607 - val_accuracy: 0.8570\n",
            "Epoch 138/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9378\n",
            "Epoch 00138: val_accuracy improved from 0.85724 to 0.86206, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.1418 - accuracy: 0.9378 - val_loss: 0.6337 - val_accuracy: 0.8621\n",
            "Epoch 139/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9407\n",
            "Epoch 00139: val_accuracy improved from 0.86206 to 0.86263, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1366 - accuracy: 0.9407 - val_loss: 0.6496 - val_accuracy: 0.8626\n",
            "Epoch 140/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.9423\n",
            "Epoch 00140: val_accuracy improved from 0.86263 to 0.86277, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1339 - accuracy: 0.9423 - val_loss: 0.6528 - val_accuracy: 0.8628\n",
            "Epoch 141/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9413\n",
            "Epoch 00141: val_accuracy improved from 0.86277 to 0.86306, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1349 - accuracy: 0.9413 - val_loss: 0.6723 - val_accuracy: 0.8631\n",
            "Epoch 142/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9436\n",
            "Epoch 00142: val_accuracy did not improve from 0.86306\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.1324 - accuracy: 0.9436 - val_loss: 0.6804 - val_accuracy: 0.8567\n",
            "Epoch 143/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9439\n",
            "Epoch 00143: val_accuracy improved from 0.86306 to 0.86603, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 111ms/step - loss: 0.1323 - accuracy: 0.9439 - val_loss: 0.6618 - val_accuracy: 0.8660\n",
            "Epoch 144/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9449\n",
            "Epoch 00144: val_accuracy improved from 0.86603 to 0.86844, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.1294 - accuracy: 0.9449 - val_loss: 0.6651 - val_accuracy: 0.8684\n",
            "Epoch 145/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1223 - accuracy: 0.9479\n",
            "Epoch 00145: val_accuracy improved from 0.86844 to 0.86887, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.1223 - accuracy: 0.9479 - val_loss: 0.6603 - val_accuracy: 0.8689\n",
            "Epoch 146/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9481\n",
            "Epoch 00146: val_accuracy did not improve from 0.86887\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.1208 - accuracy: 0.9481 - val_loss: 0.6778 - val_accuracy: 0.8639\n",
            "Epoch 147/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9464\n",
            "Epoch 00147: val_accuracy did not improve from 0.86887\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1238 - accuracy: 0.9464 - val_loss: 0.7039 - val_accuracy: 0.8558\n",
            "Epoch 148/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9505\n",
            "Epoch 00148: val_accuracy improved from 0.86887 to 0.87241, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.1148 - accuracy: 0.9505 - val_loss: 0.6863 - val_accuracy: 0.8724\n",
            "Epoch 149/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9516\n",
            "Epoch 00149: val_accuracy did not improve from 0.87241\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.1136 - accuracy: 0.9516 - val_loss: 0.6908 - val_accuracy: 0.8649\n",
            "Epoch 150/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1133 - accuracy: 0.9519\n",
            "Epoch 00150: val_accuracy did not improve from 0.87241\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.1133 - accuracy: 0.9519 - val_loss: 0.6993 - val_accuracy: 0.8683\n",
            "Epoch 151/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9504\n",
            "Epoch 00151: val_accuracy did not improve from 0.87241\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.1156 - accuracy: 0.9504 - val_loss: 0.6941 - val_accuracy: 0.8686\n",
            "Epoch 152/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1172 - accuracy: 0.9503\n",
            "Epoch 00152: val_accuracy did not improve from 0.87241\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.1172 - accuracy: 0.9503 - val_loss: 0.7151 - val_accuracy: 0.8686\n",
            "Epoch 153/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9539\n",
            "Epoch 00153: val_accuracy improved from 0.87241 to 0.87298, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.1121 - accuracy: 0.9539 - val_loss: 0.6963 - val_accuracy: 0.8730\n",
            "Epoch 154/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.9540\n",
            "Epoch 00154: val_accuracy did not improve from 0.87298\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.1097 - accuracy: 0.9540 - val_loss: 0.6816 - val_accuracy: 0.8710\n",
            "Epoch 155/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9536\n",
            "Epoch 00155: val_accuracy did not improve from 0.87298\n",
            "127/127 [==============================] - 14s 112ms/step - loss: 0.1117 - accuracy: 0.9536 - val_loss: 0.6947 - val_accuracy: 0.8730\n",
            "Epoch 156/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1078 - accuracy: 0.9550\n",
            "Epoch 00156: val_accuracy improved from 0.87298 to 0.87865, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.1078 - accuracy: 0.9550 - val_loss: 0.6785 - val_accuracy: 0.8787\n",
            "Epoch 157/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9578\n",
            "Epoch 00157: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.1015 - accuracy: 0.9578 - val_loss: 0.7009 - val_accuracy: 0.8764\n",
            "Epoch 158/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9577\n",
            "Epoch 00158: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.1029 - accuracy: 0.9577 - val_loss: 0.7228 - val_accuracy: 0.8718\n",
            "Epoch 159/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9583\n",
            "Epoch 00159: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.1019 - accuracy: 0.9583 - val_loss: 0.7058 - val_accuracy: 0.8754\n",
            "Epoch 160/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9591\n",
            "Epoch 00160: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0978 - accuracy: 0.9591 - val_loss: 0.7168 - val_accuracy: 0.8781\n",
            "Epoch 161/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0942 - accuracy: 0.9614\n",
            "Epoch 00161: val_accuracy did not improve from 0.87865\n",
            "\n",
            "Epoch 00161: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0942 - accuracy: 0.9614 - val_loss: 0.7158 - val_accuracy: 0.8768\n",
            "Epoch 162/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0944 - accuracy: 0.9610\n",
            "Epoch 00162: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.0944 - accuracy: 0.9610 - val_loss: 0.7329 - val_accuracy: 0.8730\n",
            "Epoch 163/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9606\n",
            "Epoch 00163: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0930 - accuracy: 0.9606 - val_loss: 0.7313 - val_accuracy: 0.8762\n",
            "Epoch 164/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0965 - accuracy: 0.9598\n",
            "Epoch 00164: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0965 - accuracy: 0.9598 - val_loss: 0.7378 - val_accuracy: 0.8716\n",
            "Epoch 165/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9584\n",
            "Epoch 00165: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.1019 - accuracy: 0.9584 - val_loss: 0.7233 - val_accuracy: 0.8768\n",
            "Epoch 166/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9622\n",
            "Epoch 00166: val_accuracy did not improve from 0.87865\n",
            "\n",
            "Epoch 00166: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0906 - accuracy: 0.9622 - val_loss: 0.7326 - val_accuracy: 0.8757\n",
            "Epoch 167/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9654\n",
            "Epoch 00167: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0856 - accuracy: 0.9654 - val_loss: 0.7462 - val_accuracy: 0.8760\n",
            "Epoch 168/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9646\n",
            "Epoch 00168: val_accuracy did not improve from 0.87865\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0860 - accuracy: 0.9646 - val_loss: 0.7524 - val_accuracy: 0.8758\n",
            "Epoch 169/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9629\n",
            "Epoch 00169: val_accuracy improved from 0.87865 to 0.88120, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0914 - accuracy: 0.9629 - val_loss: 0.7506 - val_accuracy: 0.8812\n",
            "Epoch 170/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9639\n",
            "Epoch 00170: val_accuracy did not improve from 0.88120\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.0869 - accuracy: 0.9639 - val_loss: 0.7446 - val_accuracy: 0.8760\n",
            "Epoch 171/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9658\n",
            "Epoch 00171: val_accuracy did not improve from 0.88120\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0837 - accuracy: 0.9658 - val_loss: 0.7506 - val_accuracy: 0.8792\n",
            "Epoch 172/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9656\n",
            "Epoch 00172: val_accuracy did not improve from 0.88120\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0842 - accuracy: 0.9656 - val_loss: 0.7681 - val_accuracy: 0.8788\n",
            "Epoch 173/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9658\n",
            "Epoch 00173: val_accuracy improved from 0.88120 to 0.88390, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0819 - accuracy: 0.9658 - val_loss: 0.7574 - val_accuracy: 0.8839\n",
            "Epoch 174/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9685\n",
            "Epoch 00174: val_accuracy did not improve from 0.88390\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0769 - accuracy: 0.9685 - val_loss: 0.7580 - val_accuracy: 0.8830\n",
            "Epoch 175/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9646\n",
            "Epoch 00175: val_accuracy did not improve from 0.88390\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0878 - accuracy: 0.9646 - val_loss: 0.7625 - val_accuracy: 0.8767\n",
            "Epoch 176/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0888 - accuracy: 0.9638\n",
            "Epoch 00176: val_accuracy did not improve from 0.88390\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0888 - accuracy: 0.9638 - val_loss: 0.7506 - val_accuracy: 0.8826\n",
            "Epoch 177/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9680\n",
            "Epoch 00177: val_accuracy improved from 0.88390 to 0.88446, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.0798 - accuracy: 0.9680 - val_loss: 0.7497 - val_accuracy: 0.8845\n",
            "Epoch 178/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9702\n",
            "Epoch 00178: val_accuracy did not improve from 0.88446\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0746 - accuracy: 0.9702 - val_loss: 0.7618 - val_accuracy: 0.8818\n",
            "Epoch 179/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9696\n",
            "Epoch 00179: val_accuracy did not improve from 0.88446\n",
            "127/127 [==============================] - 14s 112ms/step - loss: 0.0750 - accuracy: 0.9696 - val_loss: 0.7617 - val_accuracy: 0.8830\n",
            "Epoch 180/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9687\n",
            "Epoch 00180: val_accuracy improved from 0.88446 to 0.88801, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.0765 - accuracy: 0.9687 - val_loss: 0.7735 - val_accuracy: 0.8880\n",
            "Epoch 181/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9699\n",
            "Epoch 00181: val_accuracy did not improve from 0.88801\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.0740 - accuracy: 0.9699 - val_loss: 0.8031 - val_accuracy: 0.8805\n",
            "Epoch 182/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9713\n",
            "Epoch 00182: val_accuracy did not improve from 0.88801\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0711 - accuracy: 0.9713 - val_loss: 0.7800 - val_accuracy: 0.8822\n",
            "Epoch 183/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9691\n",
            "Epoch 00183: val_accuracy did not improve from 0.88801\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0763 - accuracy: 0.9691 - val_loss: 0.7751 - val_accuracy: 0.8850\n",
            "Epoch 184/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0674 - accuracy: 0.9734\n",
            "Epoch 00184: val_accuracy improved from 0.88801 to 0.88829, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0674 - accuracy: 0.9734 - val_loss: 0.7991 - val_accuracy: 0.8883\n",
            "Epoch 185/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9722\n",
            "Epoch 00185: val_accuracy did not improve from 0.88829\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.0693 - accuracy: 0.9722 - val_loss: 0.7861 - val_accuracy: 0.8852\n",
            "Epoch 186/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9708\n",
            "Epoch 00186: val_accuracy improved from 0.88829 to 0.89141, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0723 - accuracy: 0.9708 - val_loss: 0.7722 - val_accuracy: 0.8914\n",
            "Epoch 187/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9710\n",
            "Epoch 00187: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0767 - accuracy: 0.9710 - val_loss: 0.7485 - val_accuracy: 0.8873\n",
            "Epoch 188/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9693\n",
            "Epoch 00188: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 14s 113ms/step - loss: 0.0788 - accuracy: 0.9693 - val_loss: 0.7554 - val_accuracy: 0.8847\n",
            "Epoch 189/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9708\n",
            "Epoch 00189: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0748 - accuracy: 0.9708 - val_loss: 0.7457 - val_accuracy: 0.8872\n",
            "Epoch 190/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9754\n",
            "Epoch 00190: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0616 - accuracy: 0.9754 - val_loss: 0.7580 - val_accuracy: 0.8889\n",
            "Epoch 191/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9704\n",
            "Epoch 00191: val_accuracy did not improve from 0.89141\n",
            "\n",
            "Epoch 00191: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0730 - accuracy: 0.9704 - val_loss: 0.7764 - val_accuracy: 0.8830\n",
            "Epoch 192/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9733\n",
            "Epoch 00192: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0654 - accuracy: 0.9733 - val_loss: 0.7766 - val_accuracy: 0.8877\n",
            "Epoch 193/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9759\n",
            "Epoch 00193: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0611 - accuracy: 0.9759 - val_loss: 0.7722 - val_accuracy: 0.8890\n",
            "Epoch 194/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9760\n",
            "Epoch 00194: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0609 - accuracy: 0.9760 - val_loss: 0.8124 - val_accuracy: 0.8870\n",
            "Epoch 195/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9736\n",
            "Epoch 00195: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0683 - accuracy: 0.9736 - val_loss: 0.7815 - val_accuracy: 0.8842\n",
            "Epoch 196/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9744\n",
            "Epoch 00196: val_accuracy did not improve from 0.89141\n",
            "\n",
            "Epoch 00196: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0655 - accuracy: 0.9744 - val_loss: 0.7888 - val_accuracy: 0.8852\n",
            "Epoch 197/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9744\n",
            "Epoch 00197: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0637 - accuracy: 0.9744 - val_loss: 0.7776 - val_accuracy: 0.8906\n",
            "Epoch 198/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9765\n",
            "Epoch 00198: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0604 - accuracy: 0.9765 - val_loss: 0.7986 - val_accuracy: 0.8876\n",
            "Epoch 199/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9731\n",
            "Epoch 00199: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0695 - accuracy: 0.9731 - val_loss: 0.7888 - val_accuracy: 0.8883\n",
            "Epoch 200/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9752\n",
            "Epoch 00200: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0648 - accuracy: 0.9752 - val_loss: 0.7976 - val_accuracy: 0.8870\n",
            "Epoch 201/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9737\n",
            "Epoch 00201: val_accuracy did not improve from 0.89141\n",
            "\n",
            "Epoch 00201: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0655 - accuracy: 0.9737 - val_loss: 0.7980 - val_accuracy: 0.8896\n",
            "Epoch 202/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9772\n",
            "Epoch 00202: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 14s 113ms/step - loss: 0.0583 - accuracy: 0.9772 - val_loss: 0.8173 - val_accuracy: 0.8897\n",
            "Epoch 203/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9741\n",
            "Epoch 00203: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0641 - accuracy: 0.9741 - val_loss: 0.7970 - val_accuracy: 0.8900\n",
            "Epoch 204/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9778\n",
            "Epoch 00204: val_accuracy did not improve from 0.89141\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0568 - accuracy: 0.9778 - val_loss: 0.7949 - val_accuracy: 0.8904\n",
            "Epoch 205/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9780\n",
            "Epoch 00205: val_accuracy improved from 0.89141 to 0.89212, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0562 - accuracy: 0.9780 - val_loss: 0.8105 - val_accuracy: 0.8921\n",
            "Epoch 206/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9793\n",
            "Epoch 00206: val_accuracy improved from 0.89212 to 0.89283, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0520 - accuracy: 0.9793 - val_loss: 0.8100 - val_accuracy: 0.8928\n",
            "Epoch 207/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9799\n",
            "Epoch 00207: val_accuracy did not improve from 0.89283\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0519 - accuracy: 0.9799 - val_loss: 0.8200 - val_accuracy: 0.8927\n",
            "Epoch 208/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9791\n",
            "Epoch 00208: val_accuracy improved from 0.89283 to 0.89495, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0536 - accuracy: 0.9791 - val_loss: 0.8271 - val_accuracy: 0.8950\n",
            "Epoch 209/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9796\n",
            "Epoch 00209: val_accuracy did not improve from 0.89495\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0528 - accuracy: 0.9796 - val_loss: 0.8398 - val_accuracy: 0.8904\n",
            "Epoch 210/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9729\n",
            "Epoch 00210: val_accuracy did not improve from 0.89495\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0687 - accuracy: 0.9729 - val_loss: 0.8510 - val_accuracy: 0.8872\n",
            "Epoch 211/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9735\n",
            "Epoch 00211: val_accuracy did not improve from 0.89495\n",
            "127/127 [==============================] - 14s 111ms/step - loss: 0.0661 - accuracy: 0.9735 - val_loss: 0.8310 - val_accuracy: 0.8900\n",
            "Epoch 212/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9770\n",
            "Epoch 00212: val_accuracy did not improve from 0.89495\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0590 - accuracy: 0.9770 - val_loss: 0.8546 - val_accuracy: 0.8880\n",
            "Epoch 213/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9793\n",
            "Epoch 00213: val_accuracy improved from 0.89495 to 0.89552, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 106ms/step - loss: 0.0541 - accuracy: 0.9793 - val_loss: 0.8115 - val_accuracy: 0.8955\n",
            "Epoch 214/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9785\n",
            "Epoch 00214: val_accuracy improved from 0.89552 to 0.89665, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0562 - accuracy: 0.9785 - val_loss: 0.8196 - val_accuracy: 0.8967\n",
            "Epoch 215/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9792\n",
            "Epoch 00215: val_accuracy did not improve from 0.89665\n",
            "127/127 [==============================] - 13s 106ms/step - loss: 0.0535 - accuracy: 0.9792 - val_loss: 0.8035 - val_accuracy: 0.8959\n",
            "Epoch 216/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9789\n",
            "Epoch 00216: val_accuracy did not improve from 0.89665\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0538 - accuracy: 0.9789 - val_loss: 0.8325 - val_accuracy: 0.8948\n",
            "Epoch 217/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9785\n",
            "Epoch 00217: val_accuracy did not improve from 0.89665\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0576 - accuracy: 0.9785 - val_loss: 0.8235 - val_accuracy: 0.8941\n",
            "Epoch 218/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0536 - accuracy: 0.9792\n",
            "Epoch 00218: val_accuracy did not improve from 0.89665\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0536 - accuracy: 0.9792 - val_loss: 0.8209 - val_accuracy: 0.8954\n",
            "Epoch 219/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9793\n",
            "Epoch 00219: val_accuracy improved from 0.89665 to 0.89722, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0554 - accuracy: 0.9793 - val_loss: 0.8060 - val_accuracy: 0.8972\n",
            "Epoch 220/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9814\n",
            "Epoch 00220: val_accuracy did not improve from 0.89722\n",
            "127/127 [==============================] - 13s 102ms/step - loss: 0.0496 - accuracy: 0.9814 - val_loss: 0.8287 - val_accuracy: 0.8925\n",
            "Epoch 221/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9825\n",
            "Epoch 00221: val_accuracy improved from 0.89722 to 0.89736, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0463 - accuracy: 0.9825 - val_loss: 0.8074 - val_accuracy: 0.8974\n",
            "Epoch 222/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9828\n",
            "Epoch 00222: val_accuracy improved from 0.89736 to 0.89836, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0458 - accuracy: 0.9828 - val_loss: 0.8154 - val_accuracy: 0.8984\n",
            "Epoch 223/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9811\n",
            "Epoch 00223: val_accuracy did not improve from 0.89836\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0490 - accuracy: 0.9811 - val_loss: 0.8297 - val_accuracy: 0.8948\n",
            "Epoch 224/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9802\n",
            "Epoch 00224: val_accuracy did not improve from 0.89836\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0518 - accuracy: 0.9802 - val_loss: 0.8368 - val_accuracy: 0.8925\n",
            "Epoch 225/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9812\n",
            "Epoch 00225: val_accuracy did not improve from 0.89836\n",
            "127/127 [==============================] - 14s 110ms/step - loss: 0.0478 - accuracy: 0.9812 - val_loss: 0.8430 - val_accuracy: 0.8965\n",
            "Epoch 226/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9782\n",
            "Epoch 00226: val_accuracy did not improve from 0.89836\n",
            "127/127 [==============================] - 14s 113ms/step - loss: 0.0558 - accuracy: 0.9782 - val_loss: 0.8467 - val_accuracy: 0.8947\n",
            "Epoch 227/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9802\n",
            "Epoch 00227: val_accuracy did not improve from 0.89836\n",
            "\n",
            "Epoch 00227: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0515 - accuracy: 0.9802 - val_loss: 0.8228 - val_accuracy: 0.8969\n",
            "Epoch 228/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9824\n",
            "Epoch 00228: val_accuracy did not improve from 0.89836\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0459 - accuracy: 0.9824 - val_loss: 0.8250 - val_accuracy: 0.8941\n",
            "Epoch 229/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9825\n",
            "Epoch 00229: val_accuracy improved from 0.89836 to 0.89935, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0456 - accuracy: 0.9825 - val_loss: 0.8444 - val_accuracy: 0.8993\n",
            "Epoch 230/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9834\n",
            "Epoch 00230: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.0422 - accuracy: 0.9834 - val_loss: 0.8612 - val_accuracy: 0.8986\n",
            "Epoch 231/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9791\n",
            "Epoch 00231: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.0538 - accuracy: 0.9791 - val_loss: 0.8579 - val_accuracy: 0.8945\n",
            "Epoch 232/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9796\n",
            "Epoch 00232: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0539 - accuracy: 0.9796 - val_loss: 0.8611 - val_accuracy: 0.8941\n",
            "Epoch 233/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9791\n",
            "Epoch 00233: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 111ms/step - loss: 0.0547 - accuracy: 0.9791 - val_loss: 0.8218 - val_accuracy: 0.8971\n",
            "Epoch 234/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9828\n",
            "Epoch 00234: val_accuracy did not improve from 0.89935\n",
            "\n",
            "Epoch 00234: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0467 - accuracy: 0.9828 - val_loss: 0.8220 - val_accuracy: 0.8950\n",
            "Epoch 235/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9843\n",
            "Epoch 00235: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0412 - accuracy: 0.9843 - val_loss: 0.8396 - val_accuracy: 0.8978\n",
            "Epoch 236/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9856\n",
            "Epoch 00236: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0380 - accuracy: 0.9856 - val_loss: 0.8384 - val_accuracy: 0.8993\n",
            "Epoch 237/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9838\n",
            "Epoch 00237: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0432 - accuracy: 0.9838 - val_loss: 0.8433 - val_accuracy: 0.8952\n",
            "Epoch 238/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9821\n",
            "Epoch 00238: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 112ms/step - loss: 0.0458 - accuracy: 0.9821 - val_loss: 0.8712 - val_accuracy: 0.8978\n",
            "Epoch 239/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9832\n",
            "Epoch 00239: val_accuracy did not improve from 0.89935\n",
            "\n",
            "Epoch 00239: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 108ms/step - loss: 0.0453 - accuracy: 0.9832 - val_loss: 0.8648 - val_accuracy: 0.8962\n",
            "Epoch 240/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9806\n",
            "Epoch 00240: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 14s 107ms/step - loss: 0.0504 - accuracy: 0.9806 - val_loss: 0.8665 - val_accuracy: 0.8920\n",
            "Epoch 241/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0496 - accuracy: 0.9808\n",
            "Epoch 00241: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0496 - accuracy: 0.9808 - val_loss: 0.9011 - val_accuracy: 0.8853\n",
            "Epoch 242/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9779\n",
            "Epoch 00242: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0589 - accuracy: 0.9779 - val_loss: 0.8484 - val_accuracy: 0.8982\n",
            "Epoch 243/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9834\n",
            "Epoch 00243: val_accuracy did not improve from 0.89935\n",
            "127/127 [==============================] - 13s 105ms/step - loss: 0.0438 - accuracy: 0.9834 - val_loss: 0.8586 - val_accuracy: 0.8954\n",
            "Epoch 244/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9841\n",
            "Epoch 00244: val_accuracy improved from 0.89935 to 0.90034, saving model to best_lstm_model.h5\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0421 - accuracy: 0.9841 - val_loss: 0.8475 - val_accuracy: 0.9003\n",
            "Epoch 245/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9833\n",
            "Epoch 00245: val_accuracy did not improve from 0.90034\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0445 - accuracy: 0.9833 - val_loss: 0.8555 - val_accuracy: 0.8992\n",
            "Epoch 246/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9831\n",
            "Epoch 00246: val_accuracy did not improve from 0.90034\n",
            "127/127 [==============================] - 13s 104ms/step - loss: 0.0440 - accuracy: 0.9831 - val_loss: 0.8481 - val_accuracy: 0.8967\n",
            "Epoch 247/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9856\n",
            "Epoch 00247: val_accuracy did not improve from 0.90034\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0390 - accuracy: 0.9856 - val_loss: 0.8581 - val_accuracy: 0.8968\n",
            "Epoch 248/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9846\n",
            "Epoch 00248: val_accuracy did not improve from 0.90034\n",
            "127/127 [==============================] - 13s 103ms/step - loss: 0.0413 - accuracy: 0.9846 - val_loss: 0.8775 - val_accuracy: 0.8982\n",
            "Epoch 249/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9855\n",
            "Epoch 00249: val_accuracy did not improve from 0.90034\n",
            "\n",
            "Epoch 00249: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 14s 111ms/step - loss: 0.0387 - accuracy: 0.9855 - val_loss: 0.8681 - val_accuracy: 0.8982\n",
            "Epoch 250/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9841\n",
            "Epoch 00250: val_accuracy did not improve from 0.90034\n",
            "127/127 [==============================] - 14s 109ms/step - loss: 0.0413 - accuracy: 0.9841 - val_loss: 0.8620 - val_accuracy: 0.8985\n",
            "245/245 [==============================] - 2s 7ms/step - loss: 0.7489 - accuracy: 0.9044\n",
            "Accuracy(on Test-data): 0.9044399261474609\n",
            "time of training (s) 3408.0902013778687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmUecfAlGlWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viz(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        " \n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irJYFyT0GovG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "a39147d9-f215-41ac-9b32-9b3032d196e0"
      },
      "source": [
        "viz(fit1)\n",
        "#fit1.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSKp3NRBGua-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf7c675f-d2f0-4a27-e803-f12855111113"
      },
      "source": [
        "# load a saved model\n",
        "from keras.models import load_model\n",
        "saved_model = load_model('best_lstm_model.h5')\n",
        "# evaluate the model\n",
        "_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = saved_model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.978, Test: 0.903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7yhV-8j1nP8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "5bbf28cf-81d9-4935-bf9c-a5ae7a6ab318"
      },
      "source": [
        " \n",
        "import time\n",
        " \n",
        "Xnew=[['70.39932429', '127673.0908', '-49.57230843', '127648.0176', '-169.5783186', '127723.2374', '65.68961121', '605.91099', '-57.00357104', '626.78553', '-173.5890232', '602.4319', '70.4222426', '127673.0908', '0', '0', '0', '0', '65.00779144', '611.5874', '118.5678861', '13.18392', '-100.8692198', '13.91636', '59.999', '0.01', '6.391383458', '0.076290455', '0', '60.65826798', '124631.8125', '-59.29595943', '124484.3594', '-179.3380777', '124715.0703', '-119.5504813', '612.7967529', '117.7267525', '632.5321045', '0.859680212', '610.1417236', '60.6802407', '124611.9844', '0', '0', '0', '0', '-120.3414991', '618.3013916', '-64.05304275', '12.7658844', '69.39789118', '12.8288269', '59.99900055', '0.02', '6.130100104', '3.135101005', '0', '60.66477135', '124187.9063', '-59.31259095', '124162.833', '-179.3014124', '124212.9796', '-119.7539088', '610.12252', '117.6855311', '628.25041', '0.658901464', '606.82654', '60.68768966', '124187.9063', '0', '0', '0', '0', '-120.4872947', '614.88338', '-64.81298579', '12.08526', '70.38786513', '11.90215', '59.999', '0.02', '6.111439531', '3.140520023', '0', '70.45089049', '127723.2374', '-49.53793097', '127096.4056', '-169.532482', '127773.3839', '65.64377459', '604.44611', '-56.87179074', '621.84156', '-173.8697725', '599.86836', '70.46234965', '127522.6512', '0', '0', '0', '0', '64.95049566', '608.47453', '119.3012721', '12.26837', '-102.060972', '11.71904', '59.999', '0.01', '6.341831592', '0.077897157', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#normal    \n",
        " \n",
        "Q=[['8.508423258', '130832.3229', '-111.4632095', '130782.1763', '128.5258926', '130907.5427', '3.729955246', '500.80585', '-116.6026409', '500.62274', '123.632833', '501.35518', '8.519882414', '130832.3229', '0', '0', '0', '0', '3.586715798', '500.98896', '0', '0', '0', '0', '60', '0', '7.438322484', '0.077833008', '0', '0.920105028', '128836.9766', '-119.0066616', '128699.4609', '120.9347557', '128913.4688', '176.7782594', '505.7258606', '56.64825709', '507.2078857', '-63.31146179', '506.9961548', '0.94757082', '128814.1563', '0', '0', '0', '0', '176.7068566', '506.6356812', '0', '0', '0', '0', '60', '0', '7.257351968', '-3.071651355', '0', '0.928191628', '128375.1424', '-119.0319819', '128350.0691', '120.9743089', '128400.2157', '176.7173728', '502.63695', '56.61968931', '503.36939', '-63.3290251', '503.00317', '0.95110994', '128375.1424', '0', '0', '0', '0', '176.6715361', '503.00317', '0', '0', '0', '0', '60', '0', '7.268424496', '-3.0691986', '0', '8.559989459', '130857.3961', '-111.428832', '130230.5644', '128.5831884', '130932.6159', '3.707036934', '497.50987', '-116.3619986', '498.24231', '123.741695', '496.59432', '8.571448615', '130681.8832', '0', '0', '0', '0', '3.689848201', '497.50987', '0', '0', '0', '0', '60', '0', '7.489104114', '0.086553421', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        " \n",
        "N=[['70.97801166', '130957.6892', '-48.99362106', '130932.6159', '-169.0053608', '131032.909', '66.42872677', '482.31174', '-53.82938485', '483.22729', '-173.646319', '483.77662', '70.99520039', '130982.7625', '0', '0', '0', '0', '66.31986479', '483.04418', '0', '0', '0', '0', '59.999', '0', '7.742501568', '0.074426263', '0', '63.67401507', '129107.1016', '-56.26922599', '128963.9531', '-176.3168392', '129194.8594', '-120.8908103', '488.6340942', '119.1714468', '489.3894043', '-0.826721188', '489.4866943', '63.70147926', '129088.1484', '0', '0', '0', '0', '-120.8468648', '489.1662598', '0', '0', '0', '0', '60', '0', '7.521482155', '-3.061590456', '0', '63.67279977', '128650.9484', '-56.28737379', '128625.8751', '-176.2819248', '128676.0216', '-120.8826356', '485.79083', '119.1981397', '485.42461', '-0.916732472', '485.42461', '63.70144766', '128650.9484', '0', '0', '0', '0', '-120.8711765', '485.42461', '0', '0', '0', '0', '59.999', '0', '7.510130448', '-3.061004926', '0', '71.03530744', '131007.8358', '-48.97070275', '130381.004', '-168.9423355', '131083.0556', '66.42872677', '480.11442', '-53.53717638', '480.11442', '-173.5890232', '479.38198', '71.0467666', '130832.3229', '0', '0', '0', '0', '66.44018592', '479.93131', '0', '0', '0', '0', '60', '0', '7.746914069', '0.08213434', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        "A=[['174.2765725', '98337.36494', '59.20372897', '128550.6553', '-72.71407378', '127347.1383', '100.4796085', '1111.29459', '-4.566473627', '241.33898', '-85.22174245', '621.84156', '173.5546457', '117618.7096', '-9.786119141', '17325.62957', '-13.22959549', '1980.78833', '118.854365', '599.68525', '82.17933656', '420.23745', '78.74731936', '154.36173', '60.016', '-0.04', '1.907633306', '1.477346313', '0', '173.1802387', '47452.82422', '50.77606342', '120554.2734', '-77.66510434', '132296.1563', '-80.08758802', '1147.247314', '170.1754746', '297.0142212', '89.53582968', '636.0683594', '167.4398882', '99778.42969', '-10.8901979', '31715.72461', '-27.77069198', '21617.71094', '-63.9047252', '642.6830444', '-97.7590974', '408.9145508', '-101.4559988', '150.7930756', '60.01599884', '-0.050000001', '0.903115513', '-1.682575032', '0', '173.1822232', '47288.18722', '50.75260149', '120226.3297', '-77.62432208', '131760.0339', '85.92075096', '1062.40422', '-112.9070631', '589.06487', '123.4151091', '150.88264', '167.4526452', '99440.58882', '-10.94922346', '31567.24693', '-27.58791784', '21588.08547', '54.61433703', '466.74739', '103.9746511', '479.01576', '110.2084319', '228.33817', '60.016', '-0.04', '0.793198105', '1.484402105', '0', '169.0225496', '41797.14109', '63.34048425', '121254.3337', '-75.34967964', '121881.1655', '122.4983766', '1748.15117', '72.4218653', '600.23458', '-38.82934978', '177.06737', '173.2395189', '93849.24961', '-3.907572163', '37534.68519', '-2.056918485', '14743.08276', '142.8441079', '707.17082', '112.8039307', '436.16802', '107.7561725', '677.507', '60.014', '-0.04', '0.341509407', '1.111764282', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#'Attack'\n",
        "transformer= Normalizer() \n",
        "Z=transformer.transform(A)\n",
        "V = ss.transform(Z)\n",
        " \n",
        "V= np.reshape(V, (V.shape[0], 1, V.shape[1]))\n",
        "start_time = time.time()\n",
        " \n",
        "# make a prediction\n",
        "ynew = saved_model.predict_classes(V)\n",
        "# show the inputs and predicted outputs\n",
        "#for i in range(len(Xnew)):\n",
        "    #print(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
        " \n",
        "duration = time.time() - start_time\n",
        "print(\"time of test (s)\", duration)\n",
        " \n",
        "print(ynew)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time of test (s) 0.04372859001159668\n",
            "[1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:1829: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
            "  X = check_array(X, accept_sparse='csr')\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}
