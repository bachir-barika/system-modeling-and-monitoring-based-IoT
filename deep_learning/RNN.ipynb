{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGBFXvwnuvPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#إستدعاء المكتبيات\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HyaZ7kTvnPA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.read_csv(\"/data/data1.csv\")\n",
        "df2=pd.read_csv(\"/data/data2.csv\")\n",
        "df3=pd.read_csv(\"/data/data3.csv\")\n",
        "df4=pd.read_csv(\"/data/data4.csv\")\n",
        "df5=pd.read_csv(\"/data/data5.csv\")\n",
        "df6=pd.read_csv(\"/data/data6.csv\")\n",
        "df7=pd.read_csv(\"/data/data7.csv\")\n",
        "df8=pd.read_csv(\"/data/data8.csv\")\n",
        "df9=pd.read_csv(\"/data/data9.csv\")\n",
        "df10=pd.read_csv(\"/data/data10.csv\")\n",
        "df11=pd.read_csv(\"/data/data11.csv\")\n",
        "df12=pd.read_csv(\"/data/data12.csv\")\n",
        "df13=pd.read_csv(\"/data/data13.csv\")\n",
        "df14=pd.read_csv(\"/data/data14.csv\")\n",
        "df15=pd.read_csv(\"/data/data15.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCvtrNw4vtE3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#دمج مجموعات البيانات\n",
        "df = pd.concat([df1, df2,df3,df4, df5,df6,df7, df8,df9,df10, df11,df12,df13, df14,df15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDP2ER5FwVuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#تحويل الكلمات إلى أرقام للعمود marker\n",
        "#target equal 0 is natural\n",
        "#target equal 1 is Attack\n",
        "df.loc[df[\"marker\"] == \"Natural\", \"marker\"] = 0\n",
        "df.loc[df[\"marker\"] ==\"Attack\", \"marker\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW3rVLY0wX0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "38972ea6-9d17-4c81-a101-8487eb96ddaf"
      },
      "source": [
        "# إحصاء القيم\n",
        "df[\"marker\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    55663\n",
              "0    22714\n",
              "Name: marker, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gL9C1p0waIx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbade791-61f4-4fba-8c8b-e588f5719909"
      },
      "source": [
        "#سيعطي هذا مجموعة من الأماكن حيث توجد قيم NA.\n",
        " \n",
        "df[df==np.inf]=np.nan\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "#ذا كانت بياناتك تحتوي على Nan ، فجرّب ما يلي:\n",
        "np.isnan(df.values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1gGA0AewyYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X= df.drop(\"marker\", axis = 1)\n",
        "X=X.astype(float)\n",
        " \n",
        "from sklearn.preprocessing import Normalizer\n",
        "transformer= Normalizer().fit(X) \n",
        "transformer\n",
        "S=transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA2iSBzow0nY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "4a51b212-9849-419d-e8da-ca7217ffcbd7"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        " \n",
        "x = pd.DataFrame(ss.fit_transform(S))\n",
        " \n",
        "y = df[\"marker\"]\n",
        " \n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.874304</td>\n",
              "      <td>0.099036</td>\n",
              "      <td>-0.474731</td>\n",
              "      <td>0.061580</td>\n",
              "      <td>-1.867958</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.824335</td>\n",
              "      <td>1.018163</td>\n",
              "      <td>-0.569834</td>\n",
              "      <td>1.564347</td>\n",
              "      <td>-1.946451</td>\n",
              "      <td>1.417422</td>\n",
              "      <td>0.874513</td>\n",
              "      <td>0.069631</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.819024</td>\n",
              "      <td>1.415316</td>\n",
              "      <td>1.806088</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-1.503010</td>\n",
              "      <td>-0.004960</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100647</td>\n",
              "      <td>-0.023363</td>\n",
              "      <td>0.205864</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.763126</td>\n",
              "      <td>0.129576</td>\n",
              "      <td>-0.593351</td>\n",
              "      <td>0.052249</td>\n",
              "      <td>-1.979122</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>-1.394714</td>\n",
              "      <td>1.193198</td>\n",
              "      <td>1.353806</td>\n",
              "      <td>1.556328</td>\n",
              "      <td>0.066997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063134</td>\n",
              "      <td>-0.412118</td>\n",
              "      <td>-0.061167</td>\n",
              "      <td>-1.434133</td>\n",
              "      <td>-0.019137</td>\n",
              "      <td>0.620453</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>-0.567535</td>\n",
              "      <td>0.131541</td>\n",
              "      <td>-1.941687</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.869845</td>\n",
              "      <td>-0.002144</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.814642</td>\n",
              "      <td>1.448899</td>\n",
              "      <td>1.834972</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>-1.538149</td>\n",
              "      <td>0.036196</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100102</td>\n",
              "      <td>-0.022051</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.889684</td>\n",
              "      <td>0.036331</td>\n",
              "      <td>-0.434350</td>\n",
              "      <td>-0.014128</td>\n",
              "      <td>-1.790412</td>\n",
              "      <td>-0.049095</td>\n",
              "      <td>0.869987</td>\n",
              "      <td>0.398801</td>\n",
              "      <td>-0.501111</td>\n",
              "      <td>0.701235</td>\n",
              "      <td>-1.836419</td>\n",
              "      <td>0.602633</td>\n",
              "      <td>0.889830</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864449</td>\n",
              "      <td>0.601895</td>\n",
              "      <td>1.868931</td>\n",
              "      <td>-0.007345</td>\n",
              "      <td>-1.400215</td>\n",
              "      <td>-0.008872</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020642</td>\n",
              "      <td>0.008042</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.801732</td>\n",
              "      <td>0.166731</td>\n",
              "      <td>-0.531031</td>\n",
              "      <td>0.101154</td>\n",
              "      <td>-1.876163</td>\n",
              "      <td>0.083426</td>\n",
              "      <td>-1.318168</td>\n",
              "      <td>0.481710</td>\n",
              "      <td>1.369793</td>\n",
              "      <td>0.693499</td>\n",
              "      <td>0.109440</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.377415</td>\n",
              "      <td>-0.134235</td>\n",
              "      <td>-1.374928</td>\n",
              "      <td>-0.094753</td>\n",
              "      <td>0.655781</td>\n",
              "      <td>0.075937</td>\n",
              "      <td>-0.498681</td>\n",
              "      <td>0.058612</td>\n",
              "      <td>-1.830469</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>0.885227</td>\n",
              "      <td>-0.079381</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860780</td>\n",
              "      <td>0.614187</td>\n",
              "      <td>1.864821</td>\n",
              "      <td>-0.011410</td>\n",
              "      <td>-1.411007</td>\n",
              "      <td>-0.005342</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019336</td>\n",
              "      <td>0.008870</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.890043</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.433884</td>\n",
              "      <td>-0.013045</td>\n",
              "      <td>-1.789712</td>\n",
              "      <td>-0.048005</td>\n",
              "      <td>0.869660</td>\n",
              "      <td>0.398513</td>\n",
              "      <td>-0.500733</td>\n",
              "      <td>0.700832</td>\n",
              "      <td>-1.835701</td>\n",
              "      <td>0.600059</td>\n",
              "      <td>0.890189</td>\n",
              "      <td>-0.002797</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864586</td>\n",
              "      <td>0.601516</td>\n",
              "      <td>1.857189</td>\n",
              "      <td>-0.007370</td>\n",
              "      <td>-1.381310</td>\n",
              "      <td>-0.008874</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020636</td>\n",
              "      <td>0.019097</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.802080</td>\n",
              "      <td>0.166042</td>\n",
              "      <td>-0.530669</td>\n",
              "      <td>0.101267</td>\n",
              "      <td>-1.875407</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>-1.317960</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>1.370322</td>\n",
              "      <td>0.691744</td>\n",
              "      <td>0.109383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>-0.376972</td>\n",
              "      <td>-0.133153</td>\n",
              "      <td>-1.374439</td>\n",
              "      <td>-0.097088</td>\n",
              "      <td>0.655930</td>\n",
              "      <td>0.075577</td>\n",
              "      <td>-0.498514</td>\n",
              "      <td>0.058768</td>\n",
              "      <td>-1.829935</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.885529</td>\n",
              "      <td>-0.078267</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860917</td>\n",
              "      <td>0.614905</td>\n",
              "      <td>1.879398</td>\n",
              "      <td>-0.011434</td>\n",
              "      <td>-1.418009</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019302</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.891902</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.429791</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-1.782334</td>\n",
              "      <td>-0.049518</td>\n",
              "      <td>0.871534</td>\n",
              "      <td>0.390431</td>\n",
              "      <td>-0.495338</td>\n",
              "      <td>0.682959</td>\n",
              "      <td>-1.830180</td>\n",
              "      <td>0.595921</td>\n",
              "      <td>0.892105</td>\n",
              "      <td>-0.007788</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.866303</td>\n",
              "      <td>0.590921</td>\n",
              "      <td>1.891025</td>\n",
              "      <td>-0.023903</td>\n",
              "      <td>-1.407753</td>\n",
              "      <td>-0.010136</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>0.010396</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.804544</td>\n",
              "      <td>0.167318</td>\n",
              "      <td>-0.525924</td>\n",
              "      <td>0.102684</td>\n",
              "      <td>-1.867290</td>\n",
              "      <td>0.085049</td>\n",
              "      <td>-1.314364</td>\n",
              "      <td>0.478504</td>\n",
              "      <td>1.372069</td>\n",
              "      <td>0.677139</td>\n",
              "      <td>0.111401</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>-0.373513</td>\n",
              "      <td>-0.137835</td>\n",
              "      <td>-1.368761</td>\n",
              "      <td>-0.095179</td>\n",
              "      <td>0.657141</td>\n",
              "      <td>0.074537</td>\n",
              "      <td>-0.493178</td>\n",
              "      <td>0.057356</td>\n",
              "      <td>-1.824012</td>\n",
              "      <td>0.047841</td>\n",
              "      <td>0.887439</td>\n",
              "      <td>-0.079738</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.862749</td>\n",
              "      <td>0.605183</td>\n",
              "      <td>1.892994</td>\n",
              "      <td>-0.027845</td>\n",
              "      <td>-1.432902</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019377</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893597</td>\n",
              "      <td>0.033239</td>\n",
              "      <td>-0.423901</td>\n",
              "      <td>-0.017844</td>\n",
              "      <td>-1.770758</td>\n",
              "      <td>-0.052923</td>\n",
              "      <td>0.868269</td>\n",
              "      <td>0.389099</td>\n",
              "      <td>-0.489603</td>\n",
              "      <td>0.661865</td>\n",
              "      <td>-1.824963</td>\n",
              "      <td>0.604001</td>\n",
              "      <td>0.893742</td>\n",
              "      <td>-0.007717</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864216</td>\n",
              "      <td>0.586934</td>\n",
              "      <td>1.884543</td>\n",
              "      <td>-0.063127</td>\n",
              "      <td>-1.447021</td>\n",
              "      <td>-0.012640</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020903</td>\n",
              "      <td>0.037047</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.807090</td>\n",
              "      <td>0.167550</td>\n",
              "      <td>-0.519189</td>\n",
              "      <td>0.103949</td>\n",
              "      <td>-1.854435</td>\n",
              "      <td>0.085472</td>\n",
              "      <td>-1.310773</td>\n",
              "      <td>0.475735</td>\n",
              "      <td>1.371320</td>\n",
              "      <td>0.654512</td>\n",
              "      <td>0.108615</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003178</td>\n",
              "      <td>-0.368471</td>\n",
              "      <td>-0.137439</td>\n",
              "      <td>-1.359878</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>0.654775</td>\n",
              "      <td>0.074295</td>\n",
              "      <td>-0.487400</td>\n",
              "      <td>0.055487</td>\n",
              "      <td>-1.819112</td>\n",
              "      <td>0.048351</td>\n",
              "      <td>0.889183</td>\n",
              "      <td>-0.083081</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860604</td>\n",
              "      <td>0.598972</td>\n",
              "      <td>1.887278</td>\n",
              "      <td>-0.063596</td>\n",
              "      <td>-1.489143</td>\n",
              "      <td>-0.052365</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019594</td>\n",
              "      <td>0.009072</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       125      126       127\n",
              "0  0.874304  0.099036 -0.474731  ... -0.007144 -0.00943 -0.008728\n",
              "1  0.889684  0.036331 -0.434350  ... -0.007144 -0.00943 -0.008728\n",
              "2  0.890043  0.037227 -0.433884  ... -0.007144 -0.00943 -0.008728\n",
              "3  0.891902  0.036003 -0.429791  ... -0.007144 -0.00943 -0.008728\n",
              "4  0.893597  0.033239 -0.423901  ... -0.007144 -0.00943 -0.008728\n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sysPk785yqef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4d521840-8e93-41ce-8c57-91a2ea9c2664"
      },
      "source": [
        "# categorical target y to array \n",
        "from keras.utils.np_utils import to_categorical\n",
        " \n",
        "y_cat = to_categorical(y,2)\n",
        "y_cat[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.],\n",
              "       [1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlkjdIBKytNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#divide datasets into to part training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "x_train, x_test, y_train, y_test = train_test_split(x.values, y_cat,test_size=0.1,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9UX3F11zopc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshape x_train,x_test,y_train and y_test\n",
        "X_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
        "X_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
        "Y_train = np.reshape(y_train, (y_train.shape[0], 1, y_train.shape[1]))\n",
        "Y_test = np.reshape(y_test, (y_test.shape[0], 1, y_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSedmUVgqCJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_test.shape)\n",
        " \n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfC5MkRZywWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vF8W6Vsy78H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "c46930e5-017f-4dbe-dff8-785ad1319bd6"
      },
      "source": [
        "i=x_train.shape[1]\n",
        "rnn_model = keras.Sequential([\n",
        "    layers.SimpleRNN(128, activation=\"tanh\", input_shape=[1,i] ,return_sequences=True,kernel_initializer=\"he_uniform\"),\n",
        "    \n",
        "    layers.SimpleRNN(256,activation=\"tanh\",return_sequences=True,kernel_initializer=\"he_uniform\"),\n",
        " \n",
        "    layers.SimpleRNN(128,activation=\"tanh\",return_sequences=False,kernel_initializer=\"he_uniform\"),\n",
        "    layers.Dropout(0.01),\n",
        "    layers.Dense(2, activation=\"sigmoid\",kernel_initializer=\"he_uniform\")\n",
        "  ])\n",
        " \n",
        "optimizer = tf.keras.optimizers.Adam(0.001,decay=0.000005)\n",
        "rnn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        "rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 1, 128)            32896     \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 1, 256)            98560     \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 128)               49280     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 180,994\n",
            "Trainable params: 180,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lUhsdRMyPjC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a0192569-fa01-4754-d431-2e1118dc040d"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TnICc4pyNNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "# patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_RNN_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "rp = ReduceLROnPlateau(monitor='val_accuracy',patience = 5,verbose=1,factor=0.5,min_lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfNarmuUzARu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fac9614e-156d-4df6-91a2-804b0683f86f"
      },
      "source": [
        "start_time = time.time()\n",
        "fit1 = rnn_model.fit(X_train, y_train, epochs=250, batch_size=500,callbacks=[es, mc,rp],validation_split=0.1)\n",
        "duration = time.time() - start_time\n",
        "print(\"time of training (s)\", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.5941 - accuracy: 0.7061\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71406, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 9ms/step - loss: 0.5942 - accuracy: 0.7060 - val_loss: 0.5807 - val_accuracy: 0.7141\n",
            "Epoch 2/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.5747 - accuracy: 0.7170\n",
            "Epoch 00002: val_accuracy improved from 0.71406 to 0.71747, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5742 - accuracy: 0.7173 - val_loss: 0.5682 - val_accuracy: 0.7175\n",
            "Epoch 3/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.5617 - accuracy: 0.7202\n",
            "Epoch 00003: val_accuracy did not improve from 0.71747\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5616 - accuracy: 0.7202 - val_loss: 0.5606 - val_accuracy: 0.7136\n",
            "Epoch 4/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.5528 - accuracy: 0.7260\n",
            "Epoch 00004: val_accuracy improved from 0.71747 to 0.72739, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5527 - accuracy: 0.7259 - val_loss: 0.5531 - val_accuracy: 0.7274\n",
            "Epoch 5/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.5403 - accuracy: 0.7297\n",
            "Epoch 00005: val_accuracy improved from 0.72739 to 0.72881, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5399 - accuracy: 0.7300 - val_loss: 0.5392 - val_accuracy: 0.7288\n",
            "Epoch 6/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.5331 - accuracy: 0.7312\n",
            "Epoch 00006: val_accuracy did not improve from 0.72881\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5330 - accuracy: 0.7311 - val_loss: 0.5382 - val_accuracy: 0.7234\n",
            "Epoch 7/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.5254 - accuracy: 0.7357\n",
            "Epoch 00007: val_accuracy did not improve from 0.72881\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5252 - accuracy: 0.7359 - val_loss: 0.5328 - val_accuracy: 0.7263\n",
            "Epoch 8/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.5195 - accuracy: 0.7363\n",
            "Epoch 00008: val_accuracy did not improve from 0.72881\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5194 - accuracy: 0.7362 - val_loss: 0.5353 - val_accuracy: 0.7261\n",
            "Epoch 9/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.5116 - accuracy: 0.7406\n",
            "Epoch 00009: val_accuracy did not improve from 0.72881\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5117 - accuracy: 0.7403 - val_loss: 0.5464 - val_accuracy: 0.7250\n",
            "Epoch 10/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.5093 - accuracy: 0.7397\n",
            "Epoch 00010: val_accuracy improved from 0.72881 to 0.73533, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5085 - accuracy: 0.7403 - val_loss: 0.5219 - val_accuracy: 0.7353\n",
            "Epoch 11/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.5008 - accuracy: 0.7453\n",
            "Epoch 00011: val_accuracy did not improve from 0.73533\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.5007 - accuracy: 0.7455 - val_loss: 0.5213 - val_accuracy: 0.7294\n",
            "Epoch 12/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4957 - accuracy: 0.7479\n",
            "Epoch 00012: val_accuracy did not improve from 0.73533\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4962 - accuracy: 0.7467 - val_loss: 0.5165 - val_accuracy: 0.7295\n",
            "Epoch 13/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4924 - accuracy: 0.7489\n",
            "Epoch 00013: val_accuracy did not improve from 0.73533\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4924 - accuracy: 0.7490 - val_loss: 0.5160 - val_accuracy: 0.7292\n",
            "Epoch 14/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.4899 - accuracy: 0.7496\n",
            "Epoch 00014: val_accuracy did not improve from 0.73533\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4898 - accuracy: 0.7495 - val_loss: 0.5137 - val_accuracy: 0.7253\n",
            "Epoch 15/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.4840 - accuracy: 0.7523\n",
            "Epoch 00015: val_accuracy did not improve from 0.73533\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4844 - accuracy: 0.7521 - val_loss: 0.5134 - val_accuracy: 0.7328\n",
            "Epoch 16/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.4839 - accuracy: 0.7537\n",
            "Epoch 00016: val_accuracy did not improve from 0.73533\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4846 - accuracy: 0.7532 - val_loss: 0.5150 - val_accuracy: 0.7315\n",
            "Epoch 17/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4778 - accuracy: 0.7567\n",
            "Epoch 00017: val_accuracy improved from 0.73533 to 0.73788, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4776 - accuracy: 0.7566 - val_loss: 0.5108 - val_accuracy: 0.7379\n",
            "Epoch 18/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4737 - accuracy: 0.7581\n",
            "Epoch 00018: val_accuracy improved from 0.73788 to 0.74199, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4736 - accuracy: 0.7583 - val_loss: 0.5016 - val_accuracy: 0.7420\n",
            "Epoch 19/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4685 - accuracy: 0.7614\n",
            "Epoch 00019: val_accuracy improved from 0.74199 to 0.74256, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4685 - accuracy: 0.7614 - val_loss: 0.4977 - val_accuracy: 0.7426\n",
            "Epoch 20/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4647 - accuracy: 0.7654\n",
            "Epoch 00020: val_accuracy improved from 0.74256 to 0.74568, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4649 - accuracy: 0.7650 - val_loss: 0.4970 - val_accuracy: 0.7457\n",
            "Epoch 21/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4611 - accuracy: 0.7658\n",
            "Epoch 00021: val_accuracy did not improve from 0.74568\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4613 - accuracy: 0.7660 - val_loss: 0.4963 - val_accuracy: 0.7411\n",
            "Epoch 22/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7681\n",
            "Epoch 00022: val_accuracy did not improve from 0.74568\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4585 - accuracy: 0.7679 - val_loss: 0.4963 - val_accuracy: 0.7451\n",
            "Epoch 23/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.4530 - accuracy: 0.7706\n",
            "Epoch 00023: val_accuracy did not improve from 0.74568\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4523 - accuracy: 0.7713 - val_loss: 0.4909 - val_accuracy: 0.7451\n",
            "Epoch 24/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4504 - accuracy: 0.7745\n",
            "Epoch 00024: val_accuracy improved from 0.74568 to 0.75078, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4505 - accuracy: 0.7742 - val_loss: 0.4877 - val_accuracy: 0.7508\n",
            "Epoch 25/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.4460 - accuracy: 0.7758\n",
            "Epoch 00025: val_accuracy improved from 0.75078 to 0.75404, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4461 - accuracy: 0.7756 - val_loss: 0.4875 - val_accuracy: 0.7540\n",
            "Epoch 26/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4434 - accuracy: 0.7780\n",
            "Epoch 00026: val_accuracy did not improve from 0.75404\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4438 - accuracy: 0.7778 - val_loss: 0.4823 - val_accuracy: 0.7540\n",
            "Epoch 27/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4374 - accuracy: 0.7811\n",
            "Epoch 00027: val_accuracy did not improve from 0.75404\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4378 - accuracy: 0.7810 - val_loss: 0.4936 - val_accuracy: 0.7506\n",
            "Epoch 28/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.4342 - accuracy: 0.7831\n",
            "Epoch 00028: val_accuracy improved from 0.75404 to 0.75645, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4344 - accuracy: 0.7831 - val_loss: 0.4811 - val_accuracy: 0.7565\n",
            "Epoch 29/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4314 - accuracy: 0.7858\n",
            "Epoch 00029: val_accuracy did not improve from 0.75645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4322 - accuracy: 0.7854 - val_loss: 0.4857 - val_accuracy: 0.7526\n",
            "Epoch 30/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4290 - accuracy: 0.7860\n",
            "Epoch 00030: val_accuracy improved from 0.75645 to 0.76396, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4289 - accuracy: 0.7859 - val_loss: 0.4741 - val_accuracy: 0.7640\n",
            "Epoch 31/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4228 - accuracy: 0.7919\n",
            "Epoch 00031: val_accuracy did not improve from 0.76396\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4230 - accuracy: 0.7916 - val_loss: 0.4773 - val_accuracy: 0.7576\n",
            "Epoch 32/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4183 - accuracy: 0.7930\n",
            "Epoch 00032: val_accuracy did not improve from 0.76396\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4183 - accuracy: 0.7930 - val_loss: 0.4777 - val_accuracy: 0.7607\n",
            "Epoch 33/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.4152 - accuracy: 0.7948\n",
            "Epoch 00033: val_accuracy did not improve from 0.76396\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4153 - accuracy: 0.7949 - val_loss: 0.4785 - val_accuracy: 0.7579\n",
            "Epoch 34/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4131 - accuracy: 0.7965\n",
            "Epoch 00034: val_accuracy improved from 0.76396 to 0.76652, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4130 - accuracy: 0.7965 - val_loss: 0.4714 - val_accuracy: 0.7665\n",
            "Epoch 35/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.4087 - accuracy: 0.7980\n",
            "Epoch 00035: val_accuracy improved from 0.76652 to 0.76822, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4088 - accuracy: 0.7979 - val_loss: 0.4719 - val_accuracy: 0.7682\n",
            "Epoch 36/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.7997\n",
            "Epoch 00036: val_accuracy did not improve from 0.76822\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4056 - accuracy: 0.7995 - val_loss: 0.4683 - val_accuracy: 0.7669\n",
            "Epoch 37/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8028\n",
            "Epoch 00037: val_accuracy improved from 0.76822 to 0.77134, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.4005 - accuracy: 0.8028 - val_loss: 0.4662 - val_accuracy: 0.7713\n",
            "Epoch 38/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.3984 - accuracy: 0.8049\n",
            "Epoch 00038: val_accuracy did not improve from 0.77134\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3987 - accuracy: 0.8046 - val_loss: 0.4673 - val_accuracy: 0.7708\n",
            "Epoch 39/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.3951 - accuracy: 0.8051\n",
            "Epoch 00039: val_accuracy improved from 0.77134 to 0.77261, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3955 - accuracy: 0.8052 - val_loss: 0.4692 - val_accuracy: 0.7726\n",
            "Epoch 40/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3919 - accuracy: 0.8084\n",
            "Epoch 00040: val_accuracy improved from 0.77261 to 0.77573, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3915 - accuracy: 0.8087 - val_loss: 0.4588 - val_accuracy: 0.7757\n",
            "Epoch 41/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3868 - accuracy: 0.8107\n",
            "Epoch 00041: val_accuracy did not improve from 0.77573\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3871 - accuracy: 0.8108 - val_loss: 0.4642 - val_accuracy: 0.7745\n",
            "Epoch 42/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3847 - accuracy: 0.8120\n",
            "Epoch 00042: val_accuracy improved from 0.77573 to 0.77701, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3846 - accuracy: 0.8119 - val_loss: 0.4641 - val_accuracy: 0.7770\n",
            "Epoch 43/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3800 - accuracy: 0.8157\n",
            "Epoch 00043: val_accuracy did not improve from 0.77701\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3808 - accuracy: 0.8151 - val_loss: 0.4672 - val_accuracy: 0.7698\n",
            "Epoch 44/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.8150\n",
            "Epoch 00044: val_accuracy improved from 0.77701 to 0.77984, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3788 - accuracy: 0.8152 - val_loss: 0.4572 - val_accuracy: 0.7798\n",
            "Epoch 45/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3757 - accuracy: 0.8164\n",
            "Epoch 00045: val_accuracy improved from 0.77984 to 0.78324, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3759 - accuracy: 0.8164 - val_loss: 0.4544 - val_accuracy: 0.7832\n",
            "Epoch 46/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.3708 - accuracy: 0.8214\n",
            "Epoch 00046: val_accuracy did not improve from 0.78324\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3708 - accuracy: 0.8215 - val_loss: 0.4552 - val_accuracy: 0.7830\n",
            "Epoch 47/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3683 - accuracy: 0.8227\n",
            "Epoch 00047: val_accuracy did not improve from 0.78324\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3677 - accuracy: 0.8228 - val_loss: 0.4561 - val_accuracy: 0.7823\n",
            "Epoch 48/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3656 - accuracy: 0.8243\n",
            "Epoch 00048: val_accuracy improved from 0.78324 to 0.78409, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3655 - accuracy: 0.8247 - val_loss: 0.4533 - val_accuracy: 0.7841\n",
            "Epoch 49/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3616 - accuracy: 0.8263\n",
            "Epoch 00049: val_accuracy improved from 0.78409 to 0.78424, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3617 - accuracy: 0.8262 - val_loss: 0.4552 - val_accuracy: 0.7842\n",
            "Epoch 50/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.3593 - accuracy: 0.8275\n",
            "Epoch 00050: val_accuracy improved from 0.78424 to 0.79232, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3594 - accuracy: 0.8279 - val_loss: 0.4490 - val_accuracy: 0.7923\n",
            "Epoch 51/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.3549 - accuracy: 0.8307\n",
            "Epoch 00051: val_accuracy did not improve from 0.79232\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3549 - accuracy: 0.8306 - val_loss: 0.4505 - val_accuracy: 0.7861\n",
            "Epoch 52/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.3539 - accuracy: 0.8310\n",
            "Epoch 00052: val_accuracy did not improve from 0.79232\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3541 - accuracy: 0.8310 - val_loss: 0.4486 - val_accuracy: 0.7835\n",
            "Epoch 53/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.3497 - accuracy: 0.8321\n",
            "Epoch 00053: val_accuracy did not improve from 0.79232\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3496 - accuracy: 0.8324 - val_loss: 0.4466 - val_accuracy: 0.7912\n",
            "Epoch 54/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3464 - accuracy: 0.8352\n",
            "Epoch 00054: val_accuracy did not improve from 0.79232\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3464 - accuracy: 0.8349 - val_loss: 0.4492 - val_accuracy: 0.7883\n",
            "Epoch 55/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3437 - accuracy: 0.8386\n",
            "Epoch 00055: val_accuracy improved from 0.79232 to 0.79402, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3437 - accuracy: 0.8386 - val_loss: 0.4427 - val_accuracy: 0.7940\n",
            "Epoch 56/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3404 - accuracy: 0.8385\n",
            "Epoch 00056: val_accuracy improved from 0.79402 to 0.79714, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3403 - accuracy: 0.8386 - val_loss: 0.4464 - val_accuracy: 0.7971\n",
            "Epoch 57/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3396 - accuracy: 0.8390\n",
            "Epoch 00057: val_accuracy did not improve from 0.79714\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3395 - accuracy: 0.8390 - val_loss: 0.4387 - val_accuracy: 0.7954\n",
            "Epoch 58/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3352 - accuracy: 0.8418\n",
            "Epoch 00058: val_accuracy did not improve from 0.79714\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3354 - accuracy: 0.8416 - val_loss: 0.4403 - val_accuracy: 0.7970\n",
            "Epoch 59/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3305 - accuracy: 0.8449\n",
            "Epoch 00059: val_accuracy did not improve from 0.79714\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3308 - accuracy: 0.8450 - val_loss: 0.4455 - val_accuracy: 0.7971\n",
            "Epoch 60/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.3294 - accuracy: 0.8436\n",
            "Epoch 00060: val_accuracy did not improve from 0.79714\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3293 - accuracy: 0.8436 - val_loss: 0.4392 - val_accuracy: 0.7952\n",
            "Epoch 61/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8471\n",
            "Epoch 00061: val_accuracy did not improve from 0.79714\n",
            "\n",
            "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3252 - accuracy: 0.8469 - val_loss: 0.4444 - val_accuracy: 0.7917\n",
            "Epoch 62/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.8498\n",
            "Epoch 00062: val_accuracy improved from 0.79714 to 0.79728, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3226 - accuracy: 0.8498 - val_loss: 0.4393 - val_accuracy: 0.7973\n",
            "Epoch 63/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3191 - accuracy: 0.8513\n",
            "Epoch 00063: val_accuracy improved from 0.79728 to 0.79983, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3199 - accuracy: 0.8508 - val_loss: 0.4438 - val_accuracy: 0.7998\n",
            "Epoch 64/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.3165 - accuracy: 0.8520\n",
            "Epoch 00064: val_accuracy did not improve from 0.79983\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3175 - accuracy: 0.8512 - val_loss: 0.4376 - val_accuracy: 0.7990\n",
            "Epoch 65/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.3163 - accuracy: 0.8525\n",
            "Epoch 00065: val_accuracy did not improve from 0.79983\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3163 - accuracy: 0.8524 - val_loss: 0.4524 - val_accuracy: 0.7956\n",
            "Epoch 66/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.8553\n",
            "Epoch 00066: val_accuracy improved from 0.79983 to 0.79997, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3116 - accuracy: 0.8553 - val_loss: 0.4502 - val_accuracy: 0.8000\n",
            "Epoch 67/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.3097 - accuracy: 0.8568\n",
            "Epoch 00067: val_accuracy improved from 0.79997 to 0.80366, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3097 - accuracy: 0.8568 - val_loss: 0.4418 - val_accuracy: 0.8037\n",
            "Epoch 68/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.3067 - accuracy: 0.8573\n",
            "Epoch 00068: val_accuracy did not improve from 0.80366\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3064 - accuracy: 0.8576 - val_loss: 0.4409 - val_accuracy: 0.8037\n",
            "Epoch 69/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8591\n",
            "Epoch 00069: val_accuracy improved from 0.80366 to 0.80408, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.3046 - accuracy: 0.8589 - val_loss: 0.4394 - val_accuracy: 0.8041\n",
            "Epoch 70/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.3013 - accuracy: 0.8614\n",
            "Epoch 00070: val_accuracy improved from 0.80408 to 0.80522, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3008 - accuracy: 0.8617 - val_loss: 0.4384 - val_accuracy: 0.8052\n",
            "Epoch 71/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.8622\n",
            "Epoch 00071: val_accuracy improved from 0.80522 to 0.81174, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.3004 - accuracy: 0.8622 - val_loss: 0.4307 - val_accuracy: 0.8117\n",
            "Epoch 72/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2955 - accuracy: 0.8643\n",
            "Epoch 00072: val_accuracy did not improve from 0.81174\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2959 - accuracy: 0.8641 - val_loss: 0.4331 - val_accuracy: 0.8106\n",
            "Epoch 73/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2929 - accuracy: 0.8672\n",
            "Epoch 00073: val_accuracy did not improve from 0.81174\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2925 - accuracy: 0.8674 - val_loss: 0.4308 - val_accuracy: 0.8090\n",
            "Epoch 74/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2925 - accuracy: 0.8656\n",
            "Epoch 00074: val_accuracy did not improve from 0.81174\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2929 - accuracy: 0.8654 - val_loss: 0.4366 - val_accuracy: 0.8076\n",
            "Epoch 75/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8681\n",
            "Epoch 00075: val_accuracy did not improve from 0.81174\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2897 - accuracy: 0.8679 - val_loss: 0.4369 - val_accuracy: 0.8109\n",
            "Epoch 76/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2868 - accuracy: 0.8687\n",
            "Epoch 00076: val_accuracy improved from 0.81174 to 0.81330, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2866 - accuracy: 0.8689 - val_loss: 0.4338 - val_accuracy: 0.8133\n",
            "Epoch 77/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2826 - accuracy: 0.8721\n",
            "Epoch 00077: val_accuracy improved from 0.81330 to 0.81585, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2830 - accuracy: 0.8720 - val_loss: 0.4287 - val_accuracy: 0.8158\n",
            "Epoch 78/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.8725\n",
            "Epoch 00078: val_accuracy improved from 0.81585 to 0.81925, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2811 - accuracy: 0.8725 - val_loss: 0.4266 - val_accuracy: 0.8193\n",
            "Epoch 79/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8724\n",
            "Epoch 00079: val_accuracy did not improve from 0.81925\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2789 - accuracy: 0.8723 - val_loss: 0.4305 - val_accuracy: 0.8177\n",
            "Epoch 80/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8742\n",
            "Epoch 00080: val_accuracy did not improve from 0.81925\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2778 - accuracy: 0.8740 - val_loss: 0.4363 - val_accuracy: 0.8085\n",
            "Epoch 81/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2749 - accuracy: 0.8738\n",
            "Epoch 00081: val_accuracy did not improve from 0.81925\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2748 - accuracy: 0.8739 - val_loss: 0.4383 - val_accuracy: 0.8137\n",
            "Epoch 82/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.8776\n",
            "Epoch 00082: val_accuracy did not improve from 0.81925\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2730 - accuracy: 0.8773 - val_loss: 0.4372 - val_accuracy: 0.8127\n",
            "Epoch 83/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2698 - accuracy: 0.8777\n",
            "Epoch 00083: val_accuracy improved from 0.81925 to 0.82081, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2701 - accuracy: 0.8774 - val_loss: 0.4328 - val_accuracy: 0.8208\n",
            "Epoch 84/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2676 - accuracy: 0.8797\n",
            "Epoch 00084: val_accuracy did not improve from 0.82081\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2679 - accuracy: 0.8796 - val_loss: 0.4327 - val_accuracy: 0.8191\n",
            "Epoch 85/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2654 - accuracy: 0.8795\n",
            "Epoch 00085: val_accuracy improved from 0.82081 to 0.82138, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2654 - accuracy: 0.8793 - val_loss: 0.4338 - val_accuracy: 0.8214\n",
            "Epoch 86/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2642 - accuracy: 0.8814\n",
            "Epoch 00086: val_accuracy improved from 0.82138 to 0.82549, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2643 - accuracy: 0.8815 - val_loss: 0.4339 - val_accuracy: 0.8255\n",
            "Epoch 87/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.8837\n",
            "Epoch 00087: val_accuracy did not improve from 0.82549\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2600 - accuracy: 0.8840 - val_loss: 0.4308 - val_accuracy: 0.8194\n",
            "Epoch 88/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2612 - accuracy: 0.8833\n",
            "Epoch 00088: val_accuracy did not improve from 0.82549\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2609 - accuracy: 0.8836 - val_loss: 0.4356 - val_accuracy: 0.8249\n",
            "Epoch 89/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2587 - accuracy: 0.8834\n",
            "Epoch 00089: val_accuracy did not improve from 0.82549\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2588 - accuracy: 0.8834 - val_loss: 0.4296 - val_accuracy: 0.8249\n",
            "Epoch 90/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.8849\n",
            "Epoch 00090: val_accuracy did not improve from 0.82549\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2572 - accuracy: 0.8846 - val_loss: 0.4335 - val_accuracy: 0.8255\n",
            "Epoch 91/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.2538 - accuracy: 0.8871\n",
            "Epoch 00091: val_accuracy improved from 0.82549 to 0.82705, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2542 - accuracy: 0.8870 - val_loss: 0.4332 - val_accuracy: 0.8270\n",
            "Epoch 92/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2531 - accuracy: 0.8873\n",
            "Epoch 00092: val_accuracy did not improve from 0.82705\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2531 - accuracy: 0.8873 - val_loss: 0.4303 - val_accuracy: 0.8261\n",
            "Epoch 93/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2498 - accuracy: 0.8894\n",
            "Epoch 00093: val_accuracy improved from 0.82705 to 0.82776, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2496 - accuracy: 0.8895 - val_loss: 0.4300 - val_accuracy: 0.8278\n",
            "Epoch 94/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2478 - accuracy: 0.8894\n",
            "Epoch 00094: val_accuracy did not improve from 0.82776\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2478 - accuracy: 0.8891 - val_loss: 0.4311 - val_accuracy: 0.8270\n",
            "Epoch 95/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.2477 - accuracy: 0.8895\n",
            "Epoch 00095: val_accuracy improved from 0.82776 to 0.82988, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2470 - accuracy: 0.8898 - val_loss: 0.4306 - val_accuracy: 0.8299\n",
            "Epoch 96/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2449 - accuracy: 0.8905\n",
            "Epoch 00096: val_accuracy improved from 0.82988 to 0.83102, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2447 - accuracy: 0.8907 - val_loss: 0.4269 - val_accuracy: 0.8310\n",
            "Epoch 97/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.8944\n",
            "Epoch 00097: val_accuracy improved from 0.83102 to 0.83201, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2403 - accuracy: 0.8942 - val_loss: 0.4291 - val_accuracy: 0.8320\n",
            "Epoch 98/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2388 - accuracy: 0.8961\n",
            "Epoch 00098: val_accuracy improved from 0.83201 to 0.83513, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2386 - accuracy: 0.8963 - val_loss: 0.4322 - val_accuracy: 0.8351\n",
            "Epoch 99/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.8954\n",
            "Epoch 00099: val_accuracy did not improve from 0.83513\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2371 - accuracy: 0.8953 - val_loss: 0.4360 - val_accuracy: 0.8303\n",
            "Epoch 100/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.8950\n",
            "Epoch 00100: val_accuracy did not improve from 0.83513\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2359 - accuracy: 0.8952 - val_loss: 0.4339 - val_accuracy: 0.8336\n",
            "Epoch 101/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.2355 - accuracy: 0.8955\n",
            "Epoch 00101: val_accuracy did not improve from 0.83513\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2352 - accuracy: 0.8960 - val_loss: 0.4294 - val_accuracy: 0.8326\n",
            "Epoch 102/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2312 - accuracy: 0.8986\n",
            "Epoch 00102: val_accuracy did not improve from 0.83513\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2315 - accuracy: 0.8986 - val_loss: 0.4321 - val_accuracy: 0.8307\n",
            "Epoch 103/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2319 - accuracy: 0.8981\n",
            "Epoch 00103: val_accuracy did not improve from 0.83513\n",
            "\n",
            "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2320 - accuracy: 0.8980 - val_loss: 0.4337 - val_accuracy: 0.8319\n",
            "Epoch 104/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2297 - accuracy: 0.8976\n",
            "Epoch 00104: val_accuracy improved from 0.83513 to 0.83541, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2300 - accuracy: 0.8976 - val_loss: 0.4336 - val_accuracy: 0.8354\n",
            "Epoch 105/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2291 - accuracy: 0.8991\n",
            "Epoch 00105: val_accuracy did not improve from 0.83541\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2294 - accuracy: 0.8990 - val_loss: 0.4308 - val_accuracy: 0.8348\n",
            "Epoch 106/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2255 - accuracy: 0.9005\n",
            "Epoch 00106: val_accuracy improved from 0.83541 to 0.83839, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2253 - accuracy: 0.9006 - val_loss: 0.4302 - val_accuracy: 0.8384\n",
            "Epoch 107/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9019\n",
            "Epoch 00107: val_accuracy improved from 0.83839 to 0.84108, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2243 - accuracy: 0.9020 - val_loss: 0.4267 - val_accuracy: 0.8411\n",
            "Epoch 108/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2230 - accuracy: 0.9027\n",
            "Epoch 00108: val_accuracy did not improve from 0.84108\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2227 - accuracy: 0.9030 - val_loss: 0.4302 - val_accuracy: 0.8368\n",
            "Epoch 109/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2204 - accuracy: 0.9029\n",
            "Epoch 00109: val_accuracy did not improve from 0.84108\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2209 - accuracy: 0.9025 - val_loss: 0.4338 - val_accuracy: 0.8374\n",
            "Epoch 110/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.2182 - accuracy: 0.9042\n",
            "Epoch 00110: val_accuracy did not improve from 0.84108\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2187 - accuracy: 0.9039 - val_loss: 0.4282 - val_accuracy: 0.8350\n",
            "Epoch 111/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.2196 - accuracy: 0.9046\n",
            "Epoch 00111: val_accuracy improved from 0.84108 to 0.84193, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2196 - accuracy: 0.9046 - val_loss: 0.4276 - val_accuracy: 0.8419\n",
            "Epoch 112/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2171 - accuracy: 0.9047\n",
            "Epoch 00112: val_accuracy improved from 0.84193 to 0.84307, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2171 - accuracy: 0.9047 - val_loss: 0.4260 - val_accuracy: 0.8431\n",
            "Epoch 113/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2163 - accuracy: 0.9058\n",
            "Epoch 00113: val_accuracy did not improve from 0.84307\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2164 - accuracy: 0.9057 - val_loss: 0.4284 - val_accuracy: 0.8422\n",
            "Epoch 114/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9070\n",
            "Epoch 00114: val_accuracy did not improve from 0.84307\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2132 - accuracy: 0.9071 - val_loss: 0.4309 - val_accuracy: 0.8419\n",
            "Epoch 115/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2108 - accuracy: 0.9080\n",
            "Epoch 00115: val_accuracy improved from 0.84307 to 0.84378, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2114 - accuracy: 0.9076 - val_loss: 0.4292 - val_accuracy: 0.8438\n",
            "Epoch 116/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2100 - accuracy: 0.9085\n",
            "Epoch 00116: val_accuracy did not improve from 0.84378\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2100 - accuracy: 0.9085 - val_loss: 0.4346 - val_accuracy: 0.8407\n",
            "Epoch 117/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.2089 - accuracy: 0.9086\n",
            "Epoch 00117: val_accuracy improved from 0.84378 to 0.84420, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2094 - accuracy: 0.9080 - val_loss: 0.4366 - val_accuracy: 0.8442\n",
            "Epoch 118/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2054 - accuracy: 0.9107\n",
            "Epoch 00118: val_accuracy did not improve from 0.84420\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2054 - accuracy: 0.9107 - val_loss: 0.4367 - val_accuracy: 0.8438\n",
            "Epoch 119/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9108\n",
            "Epoch 00119: val_accuracy improved from 0.84420 to 0.84477, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2058 - accuracy: 0.9105 - val_loss: 0.4325 - val_accuracy: 0.8448\n",
            "Epoch 120/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.2034 - accuracy: 0.9125\n",
            "Epoch 00120: val_accuracy did not improve from 0.84477\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2033 - accuracy: 0.9125 - val_loss: 0.4346 - val_accuracy: 0.8441\n",
            "Epoch 121/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2018 - accuracy: 0.9118\n",
            "Epoch 00121: val_accuracy did not improve from 0.84477\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2028 - accuracy: 0.9114 - val_loss: 0.4420 - val_accuracy: 0.8398\n",
            "Epoch 122/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9140\n",
            "Epoch 00122: val_accuracy improved from 0.84477 to 0.84732, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.2002 - accuracy: 0.9143 - val_loss: 0.4273 - val_accuracy: 0.8473\n",
            "Epoch 123/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9135\n",
            "Epoch 00123: val_accuracy improved from 0.84732 to 0.84831, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1988 - accuracy: 0.9135 - val_loss: 0.4307 - val_accuracy: 0.8483\n",
            "Epoch 124/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1961 - accuracy: 0.9152\n",
            "Epoch 00124: val_accuracy did not improve from 0.84831\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1961 - accuracy: 0.9152 - val_loss: 0.4324 - val_accuracy: 0.8470\n",
            "Epoch 125/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1980 - accuracy: 0.9148\n",
            "Epoch 00125: val_accuracy did not improve from 0.84831\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1982 - accuracy: 0.9148 - val_loss: 0.4375 - val_accuracy: 0.8365\n",
            "Epoch 126/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1960 - accuracy: 0.9160\n",
            "Epoch 00126: val_accuracy did not improve from 0.84831\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1964 - accuracy: 0.9156 - val_loss: 0.4310 - val_accuracy: 0.8473\n",
            "Epoch 127/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1938 - accuracy: 0.9170\n",
            "Epoch 00127: val_accuracy did not improve from 0.84831\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1938 - accuracy: 0.9170 - val_loss: 0.4316 - val_accuracy: 0.8465\n",
            "Epoch 128/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1915 - accuracy: 0.9179\n",
            "Epoch 00128: val_accuracy improved from 0.84831 to 0.84973, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1919 - accuracy: 0.9176 - val_loss: 0.4350 - val_accuracy: 0.8497\n",
            "Epoch 129/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9174\n",
            "Epoch 00129: val_accuracy did not improve from 0.84973\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1923 - accuracy: 0.9174 - val_loss: 0.4300 - val_accuracy: 0.8490\n",
            "Epoch 130/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1903 - accuracy: 0.9183\n",
            "Epoch 00130: val_accuracy improved from 0.84973 to 0.85228, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1903 - accuracy: 0.9183 - val_loss: 0.4347 - val_accuracy: 0.8523\n",
            "Epoch 131/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1871 - accuracy: 0.9201\n",
            "Epoch 00131: val_accuracy improved from 0.85228 to 0.85455, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1878 - accuracy: 0.9195 - val_loss: 0.4307 - val_accuracy: 0.8546\n",
            "Epoch 132/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9195\n",
            "Epoch 00132: val_accuracy did not improve from 0.85455\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1859 - accuracy: 0.9195 - val_loss: 0.4333 - val_accuracy: 0.8521\n",
            "Epoch 133/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9218\n",
            "Epoch 00133: val_accuracy did not improve from 0.85455\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1847 - accuracy: 0.9217 - val_loss: 0.4347 - val_accuracy: 0.8519\n",
            "Epoch 134/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9211\n",
            "Epoch 00134: val_accuracy did not improve from 0.85455\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1833 - accuracy: 0.9211 - val_loss: 0.4304 - val_accuracy: 0.8540\n",
            "Epoch 135/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9213\n",
            "Epoch 00135: val_accuracy did not improve from 0.85455\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1846 - accuracy: 0.9214 - val_loss: 0.4354 - val_accuracy: 0.8503\n",
            "Epoch 136/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1830 - accuracy: 0.9223\n",
            "Epoch 00136: val_accuracy did not improve from 0.85455\n",
            "\n",
            "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1829 - accuracy: 0.9224 - val_loss: 0.4381 - val_accuracy: 0.8520\n",
            "Epoch 137/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9229\n",
            "Epoch 00137: val_accuracy improved from 0.85455 to 0.85980, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1810 - accuracy: 0.9229 - val_loss: 0.4337 - val_accuracy: 0.8598\n",
            "Epoch 138/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1791 - accuracy: 0.9240\n",
            "Epoch 00138: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1791 - accuracy: 0.9240 - val_loss: 0.4367 - val_accuracy: 0.8544\n",
            "Epoch 139/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1777 - accuracy: 0.9247\n",
            "Epoch 00139: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1785 - accuracy: 0.9241 - val_loss: 0.4437 - val_accuracy: 0.8551\n",
            "Epoch 140/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1765 - accuracy: 0.9252\n",
            "Epoch 00140: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1767 - accuracy: 0.9251 - val_loss: 0.4360 - val_accuracy: 0.8598\n",
            "Epoch 141/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1752 - accuracy: 0.9261\n",
            "Epoch 00141: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1753 - accuracy: 0.9261 - val_loss: 0.4365 - val_accuracy: 0.8591\n",
            "Epoch 142/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1761 - accuracy: 0.9249\n",
            "Epoch 00142: val_accuracy did not improve from 0.85980\n",
            "\n",
            "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1765 - accuracy: 0.9247 - val_loss: 0.4427 - val_accuracy: 0.8553\n",
            "Epoch 143/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1745 - accuracy: 0.9260\n",
            "Epoch 00143: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1747 - accuracy: 0.9259 - val_loss: 0.4383 - val_accuracy: 0.8526\n",
            "Epoch 144/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1721 - accuracy: 0.9272\n",
            "Epoch 00144: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1721 - accuracy: 0.9272 - val_loss: 0.4404 - val_accuracy: 0.8575\n",
            "Epoch 145/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1710 - accuracy: 0.9283\n",
            "Epoch 00145: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1715 - accuracy: 0.9283 - val_loss: 0.4400 - val_accuracy: 0.8544\n",
            "Epoch 146/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1711 - accuracy: 0.9270\n",
            "Epoch 00146: val_accuracy did not improve from 0.85980\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1711 - accuracy: 0.9270 - val_loss: 0.4449 - val_accuracy: 0.8540\n",
            "Epoch 147/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9287\n",
            "Epoch 00147: val_accuracy did not improve from 0.85980\n",
            "\n",
            "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1683 - accuracy: 0.9288 - val_loss: 0.4317 - val_accuracy: 0.8575\n",
            "Epoch 148/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1696 - accuracy: 0.9286\n",
            "Epoch 00148: val_accuracy improved from 0.85980 to 0.86178, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1696 - accuracy: 0.9286 - val_loss: 0.4327 - val_accuracy: 0.8618\n",
            "Epoch 149/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1706 - accuracy: 0.9274\n",
            "Epoch 00149: val_accuracy did not improve from 0.86178\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1707 - accuracy: 0.9274 - val_loss: 0.4345 - val_accuracy: 0.8598\n",
            "Epoch 150/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1675 - accuracy: 0.9302\n",
            "Epoch 00150: val_accuracy did not improve from 0.86178\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1676 - accuracy: 0.9300 - val_loss: 0.4365 - val_accuracy: 0.8604\n",
            "Epoch 151/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1635 - accuracy: 0.9321\n",
            "Epoch 00151: val_accuracy improved from 0.86178 to 0.86249, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1635 - accuracy: 0.9320 - val_loss: 0.4314 - val_accuracy: 0.8625\n",
            "Epoch 152/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1648 - accuracy: 0.9301\n",
            "Epoch 00152: val_accuracy did not improve from 0.86249\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1653 - accuracy: 0.9299 - val_loss: 0.4404 - val_accuracy: 0.8599\n",
            "Epoch 153/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1644 - accuracy: 0.9314\n",
            "Epoch 00153: val_accuracy did not improve from 0.86249\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1650 - accuracy: 0.9311 - val_loss: 0.4401 - val_accuracy: 0.8582\n",
            "Epoch 154/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9310\n",
            "Epoch 00154: val_accuracy did not improve from 0.86249\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1637 - accuracy: 0.9310 - val_loss: 0.4399 - val_accuracy: 0.8609\n",
            "Epoch 155/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.9325\n",
            "Epoch 00155: val_accuracy did not improve from 0.86249\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1611 - accuracy: 0.9325 - val_loss: 0.4377 - val_accuracy: 0.8601\n",
            "Epoch 156/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1592 - accuracy: 0.9329\n",
            "Epoch 00156: val_accuracy did not improve from 0.86249\n",
            "\n",
            "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1598 - accuracy: 0.9326 - val_loss: 0.4412 - val_accuracy: 0.8612\n",
            "Epoch 157/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9342\n",
            "Epoch 00157: val_accuracy improved from 0.86249 to 0.86334, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1581 - accuracy: 0.9342 - val_loss: 0.4455 - val_accuracy: 0.8633\n",
            "Epoch 158/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1566 - accuracy: 0.9352\n",
            "Epoch 00158: val_accuracy did not improve from 0.86334\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1565 - accuracy: 0.9352 - val_loss: 0.4419 - val_accuracy: 0.8629\n",
            "Epoch 159/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1572 - accuracy: 0.9345\n",
            "Epoch 00159: val_accuracy improved from 0.86334 to 0.86575, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1572 - accuracy: 0.9345 - val_loss: 0.4462 - val_accuracy: 0.8657\n",
            "Epoch 160/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1592 - accuracy: 0.9327\n",
            "Epoch 00160: val_accuracy did not improve from 0.86575\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1592 - accuracy: 0.9327 - val_loss: 0.4486 - val_accuracy: 0.8599\n",
            "Epoch 161/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1557 - accuracy: 0.9343\n",
            "Epoch 00161: val_accuracy did not improve from 0.86575\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1555 - accuracy: 0.9343 - val_loss: 0.4413 - val_accuracy: 0.8649\n",
            "Epoch 162/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1529 - accuracy: 0.9359\n",
            "Epoch 00162: val_accuracy did not improve from 0.86575\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1531 - accuracy: 0.9358 - val_loss: 0.4485 - val_accuracy: 0.8608\n",
            "Epoch 163/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1549 - accuracy: 0.9347\n",
            "Epoch 00163: val_accuracy did not improve from 0.86575\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1550 - accuracy: 0.9346 - val_loss: 0.4379 - val_accuracy: 0.8653\n",
            "Epoch 164/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1516 - accuracy: 0.9363\n",
            "Epoch 00164: val_accuracy did not improve from 0.86575\n",
            "\n",
            "Epoch 00164: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1518 - accuracy: 0.9362 - val_loss: 0.4491 - val_accuracy: 0.8622\n",
            "Epoch 165/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1515 - accuracy: 0.9358\n",
            "Epoch 00165: val_accuracy did not improve from 0.86575\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1521 - accuracy: 0.9356 - val_loss: 0.4468 - val_accuracy: 0.8633\n",
            "Epoch 166/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1520 - accuracy: 0.9373\n",
            "Epoch 00166: val_accuracy did not improve from 0.86575\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1522 - accuracy: 0.9372 - val_loss: 0.4530 - val_accuracy: 0.8623\n",
            "Epoch 167/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1503 - accuracy: 0.9380\n",
            "Epoch 00167: val_accuracy improved from 0.86575 to 0.86788, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1503 - accuracy: 0.9380 - val_loss: 0.4446 - val_accuracy: 0.8679\n",
            "Epoch 168/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1486 - accuracy: 0.9373\n",
            "Epoch 00168: val_accuracy improved from 0.86788 to 0.87128, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1488 - accuracy: 0.9372 - val_loss: 0.4408 - val_accuracy: 0.8713\n",
            "Epoch 169/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9394\n",
            "Epoch 00169: val_accuracy did not improve from 0.87128\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1457 - accuracy: 0.9394 - val_loss: 0.4454 - val_accuracy: 0.8704\n",
            "Epoch 170/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9391\n",
            "Epoch 00170: val_accuracy did not improve from 0.87128\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1465 - accuracy: 0.9391 - val_loss: 0.4447 - val_accuracy: 0.8686\n",
            "Epoch 171/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1456 - accuracy: 0.9394\n",
            "Epoch 00171: val_accuracy did not improve from 0.87128\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1456 - accuracy: 0.9394 - val_loss: 0.4475 - val_accuracy: 0.8704\n",
            "Epoch 172/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1459 - accuracy: 0.9392\n",
            "Epoch 00172: val_accuracy did not improve from 0.87128\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1460 - accuracy: 0.9393 - val_loss: 0.4505 - val_accuracy: 0.8659\n",
            "Epoch 173/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1461 - accuracy: 0.9395\n",
            "Epoch 00173: val_accuracy did not improve from 0.87128\n",
            "\n",
            "Epoch 00173: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1467 - accuracy: 0.9392 - val_loss: 0.4555 - val_accuracy: 0.8639\n",
            "Epoch 174/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9387\n",
            "Epoch 00174: val_accuracy did not improve from 0.87128\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1478 - accuracy: 0.9388 - val_loss: 0.4434 - val_accuracy: 0.8680\n",
            "Epoch 175/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1438 - accuracy: 0.9404\n",
            "Epoch 00175: val_accuracy improved from 0.87128 to 0.87213, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1435 - accuracy: 0.9405 - val_loss: 0.4457 - val_accuracy: 0.8721\n",
            "Epoch 176/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1428 - accuracy: 0.9408\n",
            "Epoch 00176: val_accuracy did not improve from 0.87213\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1434 - accuracy: 0.9403 - val_loss: 0.4503 - val_accuracy: 0.8662\n",
            "Epoch 177/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9405\n",
            "Epoch 00177: val_accuracy did not improve from 0.87213\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1436 - accuracy: 0.9405 - val_loss: 0.4467 - val_accuracy: 0.8690\n",
            "Epoch 178/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1434 - accuracy: 0.9407\n",
            "Epoch 00178: val_accuracy did not improve from 0.87213\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1428 - accuracy: 0.9409 - val_loss: 0.4488 - val_accuracy: 0.8659\n",
            "Epoch 179/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9426\n",
            "Epoch 00179: val_accuracy improved from 0.87213 to 0.87284, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1407 - accuracy: 0.9425 - val_loss: 0.4493 - val_accuracy: 0.8728\n",
            "Epoch 180/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.9422\n",
            "Epoch 00180: val_accuracy did not improve from 0.87284\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1387 - accuracy: 0.9422 - val_loss: 0.4521 - val_accuracy: 0.8704\n",
            "Epoch 181/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1359 - accuracy: 0.9436\n",
            "Epoch 00181: val_accuracy improved from 0.87284 to 0.87312, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1362 - accuracy: 0.9436 - val_loss: 0.4455 - val_accuracy: 0.8731\n",
            "Epoch 182/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1368 - accuracy: 0.9435\n",
            "Epoch 00182: val_accuracy did not improve from 0.87312\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1370 - accuracy: 0.9435 - val_loss: 0.4593 - val_accuracy: 0.8706\n",
            "Epoch 183/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1354 - accuracy: 0.9447\n",
            "Epoch 00183: val_accuracy did not improve from 0.87312\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1354 - accuracy: 0.9447 - val_loss: 0.4520 - val_accuracy: 0.8707\n",
            "Epoch 184/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1356 - accuracy: 0.9444\n",
            "Epoch 00184: val_accuracy did not improve from 0.87312\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1361 - accuracy: 0.9440 - val_loss: 0.4533 - val_accuracy: 0.8679\n",
            "Epoch 185/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1361 - accuracy: 0.9437\n",
            "Epoch 00185: val_accuracy improved from 0.87312 to 0.87482, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1366 - accuracy: 0.9435 - val_loss: 0.4455 - val_accuracy: 0.8748\n",
            "Epoch 186/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9443\n",
            "Epoch 00186: val_accuracy did not improve from 0.87482\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1348 - accuracy: 0.9443 - val_loss: 0.4482 - val_accuracy: 0.8724\n",
            "Epoch 187/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9442\n",
            "Epoch 00187: val_accuracy did not improve from 0.87482\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1345 - accuracy: 0.9441 - val_loss: 0.4523 - val_accuracy: 0.8709\n",
            "Epoch 188/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9443\n",
            "Epoch 00188: val_accuracy improved from 0.87482 to 0.87553, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1341 - accuracy: 0.9443 - val_loss: 0.4480 - val_accuracy: 0.8755\n",
            "Epoch 189/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1313 - accuracy: 0.9457\n",
            "Epoch 00189: val_accuracy did not improve from 0.87553\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1314 - accuracy: 0.9456 - val_loss: 0.4495 - val_accuracy: 0.8755\n",
            "Epoch 190/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9464\n",
            "Epoch 00190: val_accuracy did not improve from 0.87553\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1301 - accuracy: 0.9464 - val_loss: 0.4479 - val_accuracy: 0.8747\n",
            "Epoch 191/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9457\n",
            "Epoch 00191: val_accuracy did not improve from 0.87553\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1315 - accuracy: 0.9456 - val_loss: 0.4575 - val_accuracy: 0.8718\n",
            "Epoch 192/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1302 - accuracy: 0.9457\n",
            "Epoch 00192: val_accuracy did not improve from 0.87553\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1297 - accuracy: 0.9460 - val_loss: 0.4589 - val_accuracy: 0.8707\n",
            "Epoch 193/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1284 - accuracy: 0.9470\n",
            "Epoch 00193: val_accuracy did not improve from 0.87553\n",
            "\n",
            "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1285 - accuracy: 0.9469 - val_loss: 0.4572 - val_accuracy: 0.8710\n",
            "Epoch 194/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9476\n",
            "Epoch 00194: val_accuracy did not improve from 0.87553\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1276 - accuracy: 0.9478 - val_loss: 0.4632 - val_accuracy: 0.8704\n",
            "Epoch 195/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9461\n",
            "Epoch 00195: val_accuracy improved from 0.87553 to 0.88021, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1298 - accuracy: 0.9462 - val_loss: 0.4476 - val_accuracy: 0.8802\n",
            "Epoch 196/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1275 - accuracy: 0.9485\n",
            "Epoch 00196: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1280 - accuracy: 0.9482 - val_loss: 0.4582 - val_accuracy: 0.8747\n",
            "Epoch 197/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9483\n",
            "Epoch 00197: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1265 - accuracy: 0.9482 - val_loss: 0.4559 - val_accuracy: 0.8740\n",
            "Epoch 198/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9491\n",
            "Epoch 00198: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1267 - accuracy: 0.9491 - val_loss: 0.4613 - val_accuracy: 0.8758\n",
            "Epoch 199/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9498\n",
            "Epoch 00199: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1257 - accuracy: 0.9497 - val_loss: 0.4577 - val_accuracy: 0.8741\n",
            "Epoch 200/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1245 - accuracy: 0.9485\n",
            "Epoch 00200: val_accuracy did not improve from 0.88021\n",
            "\n",
            "Epoch 00200: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1244 - accuracy: 0.9488 - val_loss: 0.4531 - val_accuracy: 0.8757\n",
            "Epoch 201/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9484\n",
            "Epoch 00201: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1265 - accuracy: 0.9484 - val_loss: 0.4582 - val_accuracy: 0.8731\n",
            "Epoch 202/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9488\n",
            "Epoch 00202: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1230 - accuracy: 0.9488 - val_loss: 0.4582 - val_accuracy: 0.8754\n",
            "Epoch 203/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1224 - accuracy: 0.9502\n",
            "Epoch 00203: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1224 - accuracy: 0.9501 - val_loss: 0.4583 - val_accuracy: 0.8792\n",
            "Epoch 204/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1210 - accuracy: 0.9501\n",
            "Epoch 00204: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1212 - accuracy: 0.9499 - val_loss: 0.4619 - val_accuracy: 0.8761\n",
            "Epoch 205/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1212 - accuracy: 0.9509\n",
            "Epoch 00205: val_accuracy did not improve from 0.88021\n",
            "\n",
            "Epoch 00205: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1222 - accuracy: 0.9504 - val_loss: 0.4613 - val_accuracy: 0.8764\n",
            "Epoch 206/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9492\n",
            "Epoch 00206: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1238 - accuracy: 0.9492 - val_loss: 0.4632 - val_accuracy: 0.8764\n",
            "Epoch 207/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9506\n",
            "Epoch 00207: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1208 - accuracy: 0.9504 - val_loss: 0.4547 - val_accuracy: 0.8791\n",
            "Epoch 208/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9510\n",
            "Epoch 00208: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1203 - accuracy: 0.9510 - val_loss: 0.4617 - val_accuracy: 0.8750\n",
            "Epoch 209/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1175 - accuracy: 0.9520\n",
            "Epoch 00209: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1177 - accuracy: 0.9519 - val_loss: 0.4649 - val_accuracy: 0.8769\n",
            "Epoch 210/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.1194 - accuracy: 0.9512\n",
            "Epoch 00210: val_accuracy did not improve from 0.88021\n",
            "\n",
            "Epoch 00210: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1193 - accuracy: 0.9512 - val_loss: 0.4731 - val_accuracy: 0.8745\n",
            "Epoch 211/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1167 - accuracy: 0.9530\n",
            "Epoch 00211: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1170 - accuracy: 0.9529 - val_loss: 0.4651 - val_accuracy: 0.8754\n",
            "Epoch 212/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1174 - accuracy: 0.9525\n",
            "Epoch 00212: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1176 - accuracy: 0.9524 - val_loss: 0.4721 - val_accuracy: 0.8760\n",
            "Epoch 213/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9530\n",
            "Epoch 00213: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1161 - accuracy: 0.9530 - val_loss: 0.4608 - val_accuracy: 0.8774\n",
            "Epoch 214/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9511\n",
            "Epoch 00214: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1195 - accuracy: 0.9511 - val_loss: 0.4637 - val_accuracy: 0.8784\n",
            "Epoch 215/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9525\n",
            "Epoch 00215: val_accuracy did not improve from 0.88021\n",
            "\n",
            "Epoch 00215: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1160 - accuracy: 0.9525 - val_loss: 0.4692 - val_accuracy: 0.8789\n",
            "Epoch 216/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1157 - accuracy: 0.9530\n",
            "Epoch 00216: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1161 - accuracy: 0.9527 - val_loss: 0.4687 - val_accuracy: 0.8787\n",
            "Epoch 217/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9537\n",
            "Epoch 00217: val_accuracy did not improve from 0.88021\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1143 - accuracy: 0.9532 - val_loss: 0.4695 - val_accuracy: 0.8801\n",
            "Epoch 218/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9547\n",
            "Epoch 00218: val_accuracy improved from 0.88021 to 0.88149, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1131 - accuracy: 0.9543 - val_loss: 0.4694 - val_accuracy: 0.8815\n",
            "Epoch 219/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1111 - accuracy: 0.9550\n",
            "Epoch 00219: val_accuracy improved from 0.88149 to 0.88276, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1113 - accuracy: 0.9548 - val_loss: 0.4667 - val_accuracy: 0.8828\n",
            "Epoch 220/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9545\n",
            "Epoch 00220: val_accuracy did not improve from 0.88276\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1126 - accuracy: 0.9544 - val_loss: 0.4720 - val_accuracy: 0.8799\n",
            "Epoch 221/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1103 - accuracy: 0.9550\n",
            "Epoch 00221: val_accuracy did not improve from 0.88276\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1113 - accuracy: 0.9546 - val_loss: 0.4712 - val_accuracy: 0.8796\n",
            "Epoch 222/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1105 - accuracy: 0.9556\n",
            "Epoch 00222: val_accuracy did not improve from 0.88276\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1106 - accuracy: 0.9554 - val_loss: 0.4720 - val_accuracy: 0.8799\n",
            "Epoch 223/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1106 - accuracy: 0.9557\n",
            "Epoch 00223: val_accuracy did not improve from 0.88276\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1110 - accuracy: 0.9555 - val_loss: 0.4805 - val_accuracy: 0.8767\n",
            "Epoch 224/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.1114 - accuracy: 0.9549\n",
            "Epoch 00224: val_accuracy did not improve from 0.88276\n",
            "\n",
            "Epoch 00224: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1112 - accuracy: 0.9549 - val_loss: 0.4704 - val_accuracy: 0.8826\n",
            "Epoch 225/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1116 - accuracy: 0.9555\n",
            "Epoch 00225: val_accuracy did not improve from 0.88276\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.1116 - accuracy: 0.9556 - val_loss: 0.4710 - val_accuracy: 0.8811\n",
            "Epoch 226/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9553\n",
            "Epoch 00226: val_accuracy did not improve from 0.88276\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1103 - accuracy: 0.9553 - val_loss: 0.4668 - val_accuracy: 0.8816\n",
            "Epoch 227/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1095 - accuracy: 0.9563\n",
            "Epoch 00227: val_accuracy improved from 0.88276 to 0.88645, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1098 - accuracy: 0.9561 - val_loss: 0.4688 - val_accuracy: 0.8864\n",
            "Epoch 228/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1111 - accuracy: 0.9554\n",
            "Epoch 00228: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1114 - accuracy: 0.9553 - val_loss: 0.4703 - val_accuracy: 0.8811\n",
            "Epoch 229/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9563\n",
            "Epoch 00229: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1098 - accuracy: 0.9563 - val_loss: 0.4676 - val_accuracy: 0.8843\n",
            "Epoch 230/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1064 - accuracy: 0.9573\n",
            "Epoch 00230: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1066 - accuracy: 0.9572 - val_loss: 0.4689 - val_accuracy: 0.8839\n",
            "Epoch 231/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1068 - accuracy: 0.9572\n",
            "Epoch 00231: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1071 - accuracy: 0.9571 - val_loss: 0.4741 - val_accuracy: 0.8808\n",
            "Epoch 232/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1072 - accuracy: 0.9566\n",
            "Epoch 00232: val_accuracy did not improve from 0.88645\n",
            "\n",
            "Epoch 00232: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1075 - accuracy: 0.9565 - val_loss: 0.4707 - val_accuracy: 0.8846\n",
            "Epoch 233/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1043 - accuracy: 0.9586\n",
            "Epoch 00233: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1044 - accuracy: 0.9585 - val_loss: 0.4728 - val_accuracy: 0.8826\n",
            "Epoch 234/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1037 - accuracy: 0.9579\n",
            "Epoch 00234: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1040 - accuracy: 0.9578 - val_loss: 0.4727 - val_accuracy: 0.8826\n",
            "Epoch 235/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1051 - accuracy: 0.9573\n",
            "Epoch 00235: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1050 - accuracy: 0.9573 - val_loss: 0.4726 - val_accuracy: 0.8846\n",
            "Epoch 236/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1035 - accuracy: 0.9575\n",
            "Epoch 00236: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1036 - accuracy: 0.9574 - val_loss: 0.4783 - val_accuracy: 0.8857\n",
            "Epoch 237/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1029 - accuracy: 0.9585\n",
            "Epoch 00237: val_accuracy did not improve from 0.88645\n",
            "\n",
            "Epoch 00237: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1029 - accuracy: 0.9583 - val_loss: 0.4790 - val_accuracy: 0.8804\n",
            "Epoch 238/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1039 - accuracy: 0.9573\n",
            "Epoch 00238: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1042 - accuracy: 0.9571 - val_loss: 0.4694 - val_accuracy: 0.8862\n",
            "Epoch 239/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1050 - accuracy: 0.9581\n",
            "Epoch 00239: val_accuracy did not improve from 0.88645\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1050 - accuracy: 0.9581 - val_loss: 0.4744 - val_accuracy: 0.8819\n",
            "Epoch 240/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.1017 - accuracy: 0.9598\n",
            "Epoch 00240: val_accuracy improved from 0.88645 to 0.88758, saving model to best_RNN_model.h5\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1018 - accuracy: 0.9597 - val_loss: 0.4738 - val_accuracy: 0.8876\n",
            "Epoch 241/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1005 - accuracy: 0.9604\n",
            "Epoch 00241: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1007 - accuracy: 0.9603 - val_loss: 0.4705 - val_accuracy: 0.8843\n",
            "Epoch 242/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1022 - accuracy: 0.9585\n",
            "Epoch 00242: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1019 - accuracy: 0.9586 - val_loss: 0.4701 - val_accuracy: 0.8839\n",
            "Epoch 243/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1040 - accuracy: 0.9588\n",
            "Epoch 00243: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1038 - accuracy: 0.9589 - val_loss: 0.4702 - val_accuracy: 0.8816\n",
            "Epoch 244/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1011 - accuracy: 0.9593\n",
            "Epoch 00244: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.1009 - accuracy: 0.9594 - val_loss: 0.4767 - val_accuracy: 0.8826\n",
            "Epoch 245/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0972 - accuracy: 0.9613\n",
            "Epoch 00245: val_accuracy did not improve from 0.88758\n",
            "\n",
            "Epoch 00245: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.0975 - accuracy: 0.9611 - val_loss: 0.4847 - val_accuracy: 0.8838\n",
            "Epoch 246/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0992 - accuracy: 0.9599\n",
            "Epoch 00246: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.0992 - accuracy: 0.9599 - val_loss: 0.4816 - val_accuracy: 0.8815\n",
            "Epoch 247/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0991 - accuracy: 0.9597\n",
            "Epoch 00247: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.0990 - accuracy: 0.9598 - val_loss: 0.4744 - val_accuracy: 0.8857\n",
            "Epoch 248/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0973 - accuracy: 0.9612\n",
            "Epoch 00248: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.0973 - accuracy: 0.9613 - val_loss: 0.4825 - val_accuracy: 0.8855\n",
            "Epoch 249/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9616\n",
            "Epoch 00249: val_accuracy did not improve from 0.88758\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.0971 - accuracy: 0.9616 - val_loss: 0.4847 - val_accuracy: 0.8842\n",
            "Epoch 250/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9599\n",
            "Epoch 00250: val_accuracy did not improve from 0.88758\n",
            "\n",
            "Epoch 00250: ReduceLROnPlateau reducing learning rate to 0.001.\n",
            "127/127 [==============================] - 1s 6ms/step - loss: 0.0993 - accuracy: 0.9600 - val_loss: 0.4762 - val_accuracy: 0.8855\n",
            "time of training (s) 194.4802553653717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHR1qhPuAHRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viz(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        " \n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtK6mEN-AIji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "397aeeb1-3022-4914-ac4b-3b41603e34b5"
      },
      "source": [
        "viz(fit1)\n",
        "#fit1.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5zJrjbZyrYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e91cb23b-2b3d-4429-a3fc-7b5c0328e382"
      },
      "source": [
        "# load a saved model\n",
        "from keras.models import load_model\n",
        "saved_model = load_model('best_RNN_model.h5')\n",
        "# evaluate the model\n",
        "_, train_acc = saved_model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = saved_model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.966, Test: 0.886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzPu7mOW4BYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "a2f9d969-4289-41e1-c47a-2a2b80fe3e03"
      },
      "source": [
        " \n",
        "import time\n",
        " \n",
        "Xnew=[['70.39932429', '127673.0908', '-49.57230843', '127648.0176', '-169.5783186', '127723.2374', '65.68961121', '605.91099', '-57.00357104', '626.78553', '-173.5890232', '602.4319', '70.4222426', '127673.0908', '0', '0', '0', '0', '65.00779144', '611.5874', '118.5678861', '13.18392', '-100.8692198', '13.91636', '59.999', '0.01', '6.391383458', '0.076290455', '0', '60.65826798', '124631.8125', '-59.29595943', '124484.3594', '-179.3380777', '124715.0703', '-119.5504813', '612.7967529', '117.7267525', '632.5321045', '0.859680212', '610.1417236', '60.6802407', '124611.9844', '0', '0', '0', '0', '-120.3414991', '618.3013916', '-64.05304275', '12.7658844', '69.39789118', '12.8288269', '59.99900055', '0.02', '6.130100104', '3.135101005', '0', '60.66477135', '124187.9063', '-59.31259095', '124162.833', '-179.3014124', '124212.9796', '-119.7539088', '610.12252', '117.6855311', '628.25041', '0.658901464', '606.82654', '60.68768966', '124187.9063', '0', '0', '0', '0', '-120.4872947', '614.88338', '-64.81298579', '12.08526', '70.38786513', '11.90215', '59.999', '0.02', '6.111439531', '3.140520023', '0', '70.45089049', '127723.2374', '-49.53793097', '127096.4056', '-169.532482', '127773.3839', '65.64377459', '604.44611', '-56.87179074', '621.84156', '-173.8697725', '599.86836', '70.46234965', '127522.6512', '0', '0', '0', '0', '64.95049566', '608.47453', '119.3012721', '12.26837', '-102.060972', '11.71904', '59.999', '0.01', '6.341831592', '0.077897157', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#normal    \n",
        " \n",
        "Q=[['8.508423258', '130832.3229', '-111.4632095', '130782.1763', '128.5258926', '130907.5427', '3.729955246', '500.80585', '-116.6026409', '500.62274', '123.632833', '501.35518', '8.519882414', '130832.3229', '0', '0', '0', '0', '3.586715798', '500.98896', '0', '0', '0', '0', '60', '0', '7.438322484', '0.077833008', '0', '0.920105028', '128836.9766', '-119.0066616', '128699.4609', '120.9347557', '128913.4688', '176.7782594', '505.7258606', '56.64825709', '507.2078857', '-63.31146179', '506.9961548', '0.94757082', '128814.1563', '0', '0', '0', '0', '176.7068566', '506.6356812', '0', '0', '0', '0', '60', '0', '7.257351968', '-3.071651355', '0', '0.928191628', '128375.1424', '-119.0319819', '128350.0691', '120.9743089', '128400.2157', '176.7173728', '502.63695', '56.61968931', '503.36939', '-63.3290251', '503.00317', '0.95110994', '128375.1424', '0', '0', '0', '0', '176.6715361', '503.00317', '0', '0', '0', '0', '60', '0', '7.268424496', '-3.0691986', '0', '8.559989459', '130857.3961', '-111.428832', '130230.5644', '128.5831884', '130932.6159', '3.707036934', '497.50987', '-116.3619986', '498.24231', '123.741695', '496.59432', '8.571448615', '130681.8832', '0', '0', '0', '0', '3.689848201', '497.50987', '0', '0', '0', '0', '60', '0', '7.489104114', '0.086553421', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        " \n",
        "N=[['70.97801166', '130957.6892', '-48.99362106', '130932.6159', '-169.0053608', '131032.909', '66.42872677', '482.31174', '-53.82938485', '483.22729', '-173.646319', '483.77662', '70.99520039', '130982.7625', '0', '0', '0', '0', '66.31986479', '483.04418', '0', '0', '0', '0', '59.999', '0', '7.742501568', '0.074426263', '0', '63.67401507', '129107.1016', '-56.26922599', '128963.9531', '-176.3168392', '129194.8594', '-120.8908103', '488.6340942', '119.1714468', '489.3894043', '-0.826721188', '489.4866943', '63.70147926', '129088.1484', '0', '0', '0', '0', '-120.8468648', '489.1662598', '0', '0', '0', '0', '60', '0', '7.521482155', '-3.061590456', '0', '63.67279977', '128650.9484', '-56.28737379', '128625.8751', '-176.2819248', '128676.0216', '-120.8826356', '485.79083', '119.1981397', '485.42461', '-0.916732472', '485.42461', '63.70144766', '128650.9484', '0', '0', '0', '0', '-120.8711765', '485.42461', '0', '0', '0', '0', '59.999', '0', '7.510130448', '-3.061004926', '0', '71.03530744', '131007.8358', '-48.97070275', '130381.004', '-168.9423355', '131083.0556', '66.42872677', '480.11442', '-53.53717638', '480.11442', '-173.5890232', '479.38198', '71.0467666', '130832.3229', '0', '0', '0', '0', '66.44018592', '479.93131', '0', '0', '0', '0', '60', '0', '7.746914069', '0.08213434', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        "A=[['174.2765725', '98337.36494', '59.20372897', '128550.6553', '-72.71407378', '127347.1383', '100.4796085', '1111.29459', '-4.566473627', '241.33898', '-85.22174245', '621.84156', '173.5546457', '117618.7096', '-9.786119141', '17325.62957', '-13.22959549', '1980.78833', '118.854365', '599.68525', '82.17933656', '420.23745', '78.74731936', '154.36173', '60.016', '-0.04', '1.907633306', '1.477346313', '0', '173.1802387', '47452.82422', '50.77606342', '120554.2734', '-77.66510434', '132296.1563', '-80.08758802', '1147.247314', '170.1754746', '297.0142212', '89.53582968', '636.0683594', '167.4398882', '99778.42969', '-10.8901979', '31715.72461', '-27.77069198', '21617.71094', '-63.9047252', '642.6830444', '-97.7590974', '408.9145508', '-101.4559988', '150.7930756', '60.01599884', '-0.050000001', '0.903115513', '-1.682575032', '0', '173.1822232', '47288.18722', '50.75260149', '120226.3297', '-77.62432208', '131760.0339', '85.92075096', '1062.40422', '-112.9070631', '589.06487', '123.4151091', '150.88264', '167.4526452', '99440.58882', '-10.94922346', '31567.24693', '-27.58791784', '21588.08547', '54.61433703', '466.74739', '103.9746511', '479.01576', '110.2084319', '228.33817', '60.016', '-0.04', '0.793198105', '1.484402105', '0', '169.0225496', '41797.14109', '63.34048425', '121254.3337', '-75.34967964', '121881.1655', '122.4983766', '1748.15117', '72.4218653', '600.23458', '-38.82934978', '177.06737', '173.2395189', '93849.24961', '-3.907572163', '37534.68519', '-2.056918485', '14743.08276', '142.8441079', '707.17082', '112.8039307', '436.16802', '107.7561725', '677.507', '60.014', '-0.04', '0.341509407', '1.111764282', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#'Attack'\n",
        "transformer= Normalizer() \n",
        "Z=transformer.transform(Q)\n",
        "V = ss.transform(Z)\n",
        " \n",
        "V= np.reshape(V, (V.shape[0], 1, V.shape[1]))\n",
        "start_time = time.time()\n",
        " \n",
        "# make a prediction\n",
        "ynew = saved_model.predict_classes(V)\n",
        "# show the inputs and predicted outputs\n",
        "#for i in range(len(Xnew)):\n",
        "    #print(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
        " \n",
        "duration = time.time() - start_time\n",
        "print(\"time of test (s)\", duration)\n",
        " \n",
        "print(ynew)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time of test (s) 0.0375666618347168\n",
            "[0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:1829: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
            "  X = check_array(X, accept_sparse='csr')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yytMhvW74x1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
