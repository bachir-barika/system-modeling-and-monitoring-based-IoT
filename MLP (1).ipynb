{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "geWQZvoob8Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#إستدعاء المكتبيات\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us0zhNAXq6hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install patool\n",
        "!pip install pyunpack\n",
        "from pyunpack import Archive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy-JqXvTq7gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Archive(\"/data/binaryAllNaturalPlusNormalVsAttacks.7z\").extractall(\".\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jtXCdi4cjam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1=pd.read_csv(\"/data/data1.csv\")\n",
        "df2=pd.read_csv(\"/data/data2.csv\")\n",
        "df3=pd.read_csv(\"/data/data3.csv\")\n",
        "df4=pd.read_csv(\"/data/data4.csv\")\n",
        "df5=pd.read_csv(\"/data/data5.csv\")\n",
        "df6=pd.read_csv(\"/data/data6.csv\")\n",
        "df7=pd.read_csv(\"/data/data7.csv\")\n",
        "df8=pd.read_csv(\"/data/data8.csv\")\n",
        "df9=pd.read_csv(\"/data/data9.csv\")\n",
        "df10=pd.read_csv(\"/data/data10.csv\")\n",
        "df11=pd.read_csv(\"/data/data11.csv\")\n",
        "df12=pd.read_csv(\"/data/data12.csv\")\n",
        "df13=pd.read_csv(\"/data/data13.csv\")\n",
        "df14=pd.read_csv(\"/data/data14.csv\")\n",
        "df15=pd.read_csv(\"/data/data15.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeSNDfi-c123",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#دمج مجموعات البيانات\n",
        "df = pd.concat([df1, df2,df3,df4, df5,df6,df7, df8,df9,df10, df11,df12,df13, df14,df15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3CY2cktc_6P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ff95197-ee5f-4dfc-db3d-abc7ec275048"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzvO2f7oUeA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56e3c879-60b4-49be-c90e-b66e76101526"
      },
      "source": [
        "df.ndim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UsAzHz9dCun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "66e0ea01-0ad6-4e88-e468-0e91915da769"
      },
      "source": [
        "df.describe(include=\"all\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-PA1:VH</th>\n",
              "      <th>R1-PM1:V</th>\n",
              "      <th>R1-PA2:VH</th>\n",
              "      <th>R1-PM2:V</th>\n",
              "      <th>R1-PA3:VH</th>\n",
              "      <th>R1-PM3:V</th>\n",
              "      <th>R1-PA4:IH</th>\n",
              "      <th>R1-PM4:I</th>\n",
              "      <th>R1-PA5:IH</th>\n",
              "      <th>R1-PM5:I</th>\n",
              "      <th>R1-PA6:IH</th>\n",
              "      <th>R1-PM6:I</th>\n",
              "      <th>R1-PA7:VH</th>\n",
              "      <th>R1-PM7:V</th>\n",
              "      <th>R1-PA8:VH</th>\n",
              "      <th>R1-PM8:V</th>\n",
              "      <th>R1-PA9:VH</th>\n",
              "      <th>R1-PM9:V</th>\n",
              "      <th>R1-PA10:IH</th>\n",
              "      <th>R1-PM10:I</th>\n",
              "      <th>R1-PA11:IH</th>\n",
              "      <th>R1-PM11:I</th>\n",
              "      <th>R1-PA12:IH</th>\n",
              "      <th>R1-PM12:I</th>\n",
              "      <th>R1:F</th>\n",
              "      <th>R1:DF</th>\n",
              "      <th>R1-PA:Z</th>\n",
              "      <th>R1-PA:ZH</th>\n",
              "      <th>R1:S</th>\n",
              "      <th>R2-PA1:VH</th>\n",
              "      <th>R2-PM1:V</th>\n",
              "      <th>R2-PA2:VH</th>\n",
              "      <th>R2-PM2:V</th>\n",
              "      <th>R2-PA3:VH</th>\n",
              "      <th>R2-PM3:V</th>\n",
              "      <th>R2-PA4:IH</th>\n",
              "      <th>R2-PM4:I</th>\n",
              "      <th>R2-PA5:IH</th>\n",
              "      <th>R2-PM5:I</th>\n",
              "      <th>R2-PA6:IH</th>\n",
              "      <th>...</th>\n",
              "      <th>R4-PA2:VH</th>\n",
              "      <th>R4-PM2:V</th>\n",
              "      <th>R4-PA3:VH</th>\n",
              "      <th>R4-PM3:V</th>\n",
              "      <th>R4-PA4:IH</th>\n",
              "      <th>R4-PM4:I</th>\n",
              "      <th>R4-PA5:IH</th>\n",
              "      <th>R4-PM5:I</th>\n",
              "      <th>R4-PA6:IH</th>\n",
              "      <th>R4-PM6:I</th>\n",
              "      <th>R4-PA7:VH</th>\n",
              "      <th>R4-PM7:V</th>\n",
              "      <th>R4-PA8:VH</th>\n",
              "      <th>R4-PM8:V</th>\n",
              "      <th>R4-PA9:VH</th>\n",
              "      <th>R4-PM9:V</th>\n",
              "      <th>R4-PA10:IH</th>\n",
              "      <th>R4-PM10:I</th>\n",
              "      <th>R4-PA11:IH</th>\n",
              "      <th>R4-PM11:I</th>\n",
              "      <th>R4-PA12:IH</th>\n",
              "      <th>R4-PM12:I</th>\n",
              "      <th>R4:F</th>\n",
              "      <th>R4:DF</th>\n",
              "      <th>R4-PA:Z</th>\n",
              "      <th>R4-PA:ZH</th>\n",
              "      <th>R4:S</th>\n",
              "      <th>control_panel_log1</th>\n",
              "      <th>control_panel_log2</th>\n",
              "      <th>control_panel_log3</th>\n",
              "      <th>control_panel_log4</th>\n",
              "      <th>relay1_log</th>\n",
              "      <th>relay2_log</th>\n",
              "      <th>relay3_log</th>\n",
              "      <th>relay4_log</th>\n",
              "      <th>snort_log1</th>\n",
              "      <th>snort_log2</th>\n",
              "      <th>snort_log3</th>\n",
              "      <th>snort_log4</th>\n",
              "      <th>marker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>7.837700e+04</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377.000000</td>\n",
              "      <td>78377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Attack</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>55663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-15.802424</td>\n",
              "      <td>130764.039577</td>\n",
              "      <td>2.175196</td>\n",
              "      <td>131035.528095</td>\n",
              "      <td>6.834315</td>\n",
              "      <td>131395.717581</td>\n",
              "      <td>-14.334996</td>\n",
              "      <td>393.949321</td>\n",
              "      <td>3.538540</td>\n",
              "      <td>387.438133</td>\n",
              "      <td>6.129781</td>\n",
              "      <td>381.912845</td>\n",
              "      <td>-15.798835</td>\n",
              "      <td>131056.980030</td>\n",
              "      <td>0.207857</td>\n",
              "      <td>297.083556</td>\n",
              "      <td>0.227606</td>\n",
              "      <td>87.397031</td>\n",
              "      <td>-14.504282</td>\n",
              "      <td>386.557188</td>\n",
              "      <td>-1.734936</td>\n",
              "      <td>9.979982</td>\n",
              "      <td>6.123374</td>\n",
              "      <td>9.494176</td>\n",
              "      <td>59.992801</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.018428</td>\n",
              "      <td>788.868750</td>\n",
              "      <td>-15.216491</td>\n",
              "      <td>127033.389923</td>\n",
              "      <td>4.751134</td>\n",
              "      <td>128015.428015</td>\n",
              "      <td>5.510410</td>\n",
              "      <td>128362.246185</td>\n",
              "      <td>15.836436</td>\n",
              "      <td>395.109497</td>\n",
              "      <td>-6.961603</td>\n",
              "      <td>392.508845</td>\n",
              "      <td>-6.437082</td>\n",
              "      <td>...</td>\n",
              "      <td>2.278991</td>\n",
              "      <td>131355.212680</td>\n",
              "      <td>7.065760</td>\n",
              "      <td>131745.074472</td>\n",
              "      <td>-13.931742</td>\n",
              "      <td>391.330912</td>\n",
              "      <td>3.446031</td>\n",
              "      <td>384.399819</td>\n",
              "      <td>6.096400</td>\n",
              "      <td>379.952713</td>\n",
              "      <td>-15.563852</td>\n",
              "      <td>131397.999652</td>\n",
              "      <td>0.257084</td>\n",
              "      <td>292.112647</td>\n",
              "      <td>0.207103</td>\n",
              "      <td>82.439295</td>\n",
              "      <td>-14.144585</td>\n",
              "      <td>384.036050</td>\n",
              "      <td>-1.859917</td>\n",
              "      <td>9.834635</td>\n",
              "      <td>5.989009</td>\n",
              "      <td>9.073233</td>\n",
              "      <td>59.992750</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>inf</td>\n",
              "      <td>0.016616</td>\n",
              "      <td>749.014459</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.035916</td>\n",
              "      <td>0.026436</td>\n",
              "      <td>0.026500</td>\n",
              "      <td>0.035597</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000051</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>100.876750</td>\n",
              "      <td>8546.118477</td>\n",
              "      <td>111.743169</td>\n",
              "      <td>5393.135370</td>\n",
              "      <td>97.065063</td>\n",
              "      <td>5443.752388</td>\n",
              "      <td>99.601107</td>\n",
              "      <td>190.966011</td>\n",
              "      <td>109.504977</td>\n",
              "      <td>151.277183</td>\n",
              "      <td>95.294904</td>\n",
              "      <td>153.326452</td>\n",
              "      <td>100.877118</td>\n",
              "      <td>6152.379663</td>\n",
              "      <td>13.075863</td>\n",
              "      <td>2687.617199</td>\n",
              "      <td>12.488596</td>\n",
              "      <td>897.541412</td>\n",
              "      <td>99.605025</td>\n",
              "      <td>154.484403</td>\n",
              "      <td>68.383257</td>\n",
              "      <td>47.241783</td>\n",
              "      <td>73.059209</td>\n",
              "      <td>47.875569</td>\n",
              "      <td>0.610045</td>\n",
              "      <td>0.087799</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.242813</td>\n",
              "      <td>14048.448459</td>\n",
              "      <td>101.837622</td>\n",
              "      <td>16155.767175</td>\n",
              "      <td>111.043204</td>\n",
              "      <td>12106.876201</td>\n",
              "      <td>96.270117</td>\n",
              "      <td>11990.863815</td>\n",
              "      <td>99.876094</td>\n",
              "      <td>171.765698</td>\n",
              "      <td>94.996062</td>\n",
              "      <td>152.357765</td>\n",
              "      <td>108.896267</td>\n",
              "      <td>...</td>\n",
              "      <td>111.828597</td>\n",
              "      <td>4733.901358</td>\n",
              "      <td>97.085981</td>\n",
              "      <td>4777.648212</td>\n",
              "      <td>99.653296</td>\n",
              "      <td>187.094100</td>\n",
              "      <td>109.561785</td>\n",
              "      <td>148.882516</td>\n",
              "      <td>95.495537</td>\n",
              "      <td>150.929876</td>\n",
              "      <td>100.882320</td>\n",
              "      <td>5536.542517</td>\n",
              "      <td>13.150046</td>\n",
              "      <td>2621.155809</td>\n",
              "      <td>12.523032</td>\n",
              "      <td>850.696972</td>\n",
              "      <td>99.627784</td>\n",
              "      <td>151.746973</td>\n",
              "      <td>67.783975</td>\n",
              "      <td>47.562328</td>\n",
              "      <td>72.087423</td>\n",
              "      <td>45.998572</td>\n",
              "      <td>0.609958</td>\n",
              "      <td>0.087273</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.248023</td>\n",
              "      <td>14041.170907</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.006187</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.186082</td>\n",
              "      <td>0.160430</td>\n",
              "      <td>0.160618</td>\n",
              "      <td>0.185285</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.007144</td>\n",
              "      <td>0.009450</td>\n",
              "      <td>0.008749</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.501948</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.903018</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.010000</td>\n",
              "      <td>1.852102e-01</td>\n",
              "      <td>-3.140569</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>...</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-169.984571</td>\n",
              "      <td>-179.994691</td>\n",
              "      <td>-178.875387</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-142.790610</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-165.686647</td>\n",
              "      <td>-179.988962</td>\n",
              "      <td>-155.458725</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>-145.203989</td>\n",
              "      <td>-179.719672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.612733</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-179.997259</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.460000</td>\n",
              "      <td>6.781427e-03</td>\n",
              "      <td>-3.093717</td>\n",
              "      <td>-45.998332</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-100.416583</td>\n",
              "      <td>131057.982300</td>\n",
              "      <td>-102.129727</td>\n",
              "      <td>130732.029800</td>\n",
              "      <td>-69.459673</td>\n",
              "      <td>131133.202100</td>\n",
              "      <td>-98.159129</td>\n",
              "      <td>305.793700</td>\n",
              "      <td>-94.790138</td>\n",
              "      <td>311.836330</td>\n",
              "      <td>-66.279758</td>\n",
              "      <td>303.962600</td>\n",
              "      <td>-100.399394</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-98.227884</td>\n",
              "      <td>307.807910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.291006e+00</td>\n",
              "      <td>-0.028589</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-101.562499</td>\n",
              "      <td>128826.461300</td>\n",
              "      <td>-96.490178</td>\n",
              "      <td>128769.000000</td>\n",
              "      <td>-69.052873</td>\n",
              "      <td>128901.681100</td>\n",
              "      <td>-65.091252</td>\n",
              "      <td>310.554560</td>\n",
              "      <td>-83.182013</td>\n",
              "      <td>316.446289</td>\n",
              "      <td>-105.295719</td>\n",
              "      <td>...</td>\n",
              "      <td>-101.992217</td>\n",
              "      <td>131018.156300</td>\n",
              "      <td>-69.430847</td>\n",
              "      <td>131435.093800</td>\n",
              "      <td>-97.643467</td>\n",
              "      <td>304.511930</td>\n",
              "      <td>-94.919132</td>\n",
              "      <td>309.494019</td>\n",
              "      <td>-67.173162</td>\n",
              "      <td>302.598938</td>\n",
              "      <td>-100.009783</td>\n",
              "      <td>131272.515600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-97.723682</td>\n",
              "      <td>306.249634</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>59.999001</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.347653e+00</td>\n",
              "      <td>-0.028625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-28.865614</td>\n",
              "      <td>131684.814000</td>\n",
              "      <td>8.118812</td>\n",
              "      <td>131358.861500</td>\n",
              "      <td>13.401483</td>\n",
              "      <td>131760.033900</td>\n",
              "      <td>-23.514188</td>\n",
              "      <td>378.671480</td>\n",
              "      <td>1.885031</td>\n",
              "      <td>383.249230</td>\n",
              "      <td>6.881223</td>\n",
              "      <td>376.474160</td>\n",
              "      <td>-28.842695</td>\n",
              "      <td>131609.594200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.749101</td>\n",
              "      <td>380.319470</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.011317e+01</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-29.665834</td>\n",
              "      <td>130020.265600</td>\n",
              "      <td>12.330052</td>\n",
              "      <td>129954.758400</td>\n",
              "      <td>10.077210</td>\n",
              "      <td>130080.124800</td>\n",
              "      <td>24.534053</td>\n",
              "      <td>383.615450</td>\n",
              "      <td>-5.729578</td>\n",
              "      <td>388.010090</td>\n",
              "      <td>-6.222322</td>\n",
              "      <td>...</td>\n",
              "      <td>7.969843</td>\n",
              "      <td>131634.667500</td>\n",
              "      <td>13.883972</td>\n",
              "      <td>132060.913100</td>\n",
              "      <td>-23.124577</td>\n",
              "      <td>376.997009</td>\n",
              "      <td>1.512609</td>\n",
              "      <td>380.868800</td>\n",
              "      <td>7.116394</td>\n",
              "      <td>375.009280</td>\n",
              "      <td>-28.742983</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-23.359489</td>\n",
              "      <td>378.353119</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.017212e+01</td>\n",
              "      <td>0.015089</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>68.096034</td>\n",
              "      <td>132186.279400</td>\n",
              "      <td>104.897113</td>\n",
              "      <td>131885.400200</td>\n",
              "      <td>85.324875</td>\n",
              "      <td>132261.499300</td>\n",
              "      <td>66.274028</td>\n",
              "      <td>456.676340</td>\n",
              "      <td>102.674037</td>\n",
              "      <td>460.338540</td>\n",
              "      <td>82.053286</td>\n",
              "      <td>454.295910</td>\n",
              "      <td>68.096034</td>\n",
              "      <td>132085.986400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.027656</td>\n",
              "      <td>457.775000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>14.667720</td>\n",
              "      <td>7.141290</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.269372e+01</td>\n",
              "      <td>0.059942</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69.035338</td>\n",
              "      <td>130932.615900</td>\n",
              "      <td>104.674992</td>\n",
              "      <td>130857.396100</td>\n",
              "      <td>81.577731</td>\n",
              "      <td>130982.762500</td>\n",
              "      <td>102.450263</td>\n",
              "      <td>461.986530</td>\n",
              "      <td>65.091252</td>\n",
              "      <td>465.751648</td>\n",
              "      <td>93.506712</td>\n",
              "      <td>...</td>\n",
              "      <td>105.040353</td>\n",
              "      <td>132325.828100</td>\n",
              "      <td>85.565517</td>\n",
              "      <td>132584.640600</td>\n",
              "      <td>66.849063</td>\n",
              "      <td>454.050049</td>\n",
              "      <td>102.620546</td>\n",
              "      <td>457.489014</td>\n",
              "      <td>82.167877</td>\n",
              "      <td>452.281700</td>\n",
              "      <td>68.321228</td>\n",
              "      <td>132467.359400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66.531859</td>\n",
              "      <td>455.348968</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.780624</td>\n",
              "      <td>7.992761</td>\n",
              "      <td>6.775070</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.277497e+01</td>\n",
              "      <td>0.057622</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>179.994691</td>\n",
              "      <td>151592.990400</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151567.917200</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151693.283500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1779.462980</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1265.656320</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>151618.063700</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>46987.307980</td>\n",
              "      <td>179.467570</td>\n",
              "      <td>17501.142460</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1263.459000</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>611.404290</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>723.467610</td>\n",
              "      <td>66.035000</td>\n",
              "      <td>3.720000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.125937</td>\n",
              "      <td>272394.000000</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>145336.656300</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>145167.796900</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>155526.781300</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1416.722070</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1275.910480</td>\n",
              "      <td>179.986276</td>\n",
              "      <td>...</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151749.687500</td>\n",
              "      <td>179.994691</td>\n",
              "      <td>151924.062500</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1781.324341</td>\n",
              "      <td>179.991768</td>\n",
              "      <td>1266.205650</td>\n",
              "      <td>179.988962</td>\n",
              "      <td>1262.726560</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>151859.328100</td>\n",
              "      <td>179.964297</td>\n",
              "      <td>45946.320310</td>\n",
              "      <td>179.914860</td>\n",
              "      <td>17351.042970</td>\n",
              "      <td>179.997259</td>\n",
              "      <td>1264.740770</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>610.038757</td>\n",
              "      <td>179.977503</td>\n",
              "      <td>715.827942</td>\n",
              "      <td>62.226000</td>\n",
              "      <td>3.150000</td>\n",
              "      <td>inf</td>\n",
              "      <td>3.106656</td>\n",
              "      <td>270336.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           R1-PA1:VH       R1-PM1:V  ...    snort_log4  marker\n",
              "count   78377.000000   78377.000000  ...  78377.000000   78377\n",
              "unique           NaN            NaN  ...           NaN       2\n",
              "top              NaN            NaN  ...           NaN  Attack\n",
              "freq             NaN            NaN  ...           NaN   55663\n",
              "mean      -15.802424  130764.039577  ...      0.000077     NaN\n",
              "std       100.876750    8546.118477  ...      0.008749     NaN\n",
              "min      -179.988962       0.000000  ...      0.000000     NaN\n",
              "25%      -100.416583  131057.982300  ...      0.000000     NaN\n",
              "50%       -28.865614  131684.814000  ...      0.000000     NaN\n",
              "75%        68.096034  132186.279400  ...      0.000000     NaN\n",
              "max       179.994691  151592.990400  ...      1.000000     NaN\n",
              "\n",
              "[11 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfWUYJoUdaA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#تحويل الكلمات إلى أرقام للعمود marker\n",
        "#target equal 0 is natural\n",
        "#target equal 1 is Attack\n",
        "df.loc[df[\"marker\"] == \"Natural\", \"marker\"] = 0\n",
        "df.loc[df[\"marker\"] ==\"Attack\", \"marker\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTezavhjdewf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d6b3d1ca-8862-4dd6-9ff2-c25dcab5a6d6"
      },
      "source": [
        "# إحصاء القيم\n",
        "df[\"marker\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    55663\n",
              "0    22714\n",
              "Name: marker, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMKSuXUXHxdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "107d4228-8ef5-42f7-fa07-476f1637e2c6"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R1-PA1:VH</th>\n",
              "      <th>R1-PM1:V</th>\n",
              "      <th>R1-PA2:VH</th>\n",
              "      <th>R1-PM2:V</th>\n",
              "      <th>R1-PA3:VH</th>\n",
              "      <th>R1-PM3:V</th>\n",
              "      <th>R1-PA4:IH</th>\n",
              "      <th>R1-PM4:I</th>\n",
              "      <th>R1-PA5:IH</th>\n",
              "      <th>R1-PM5:I</th>\n",
              "      <th>R1-PA6:IH</th>\n",
              "      <th>R1-PM6:I</th>\n",
              "      <th>R1-PA7:VH</th>\n",
              "      <th>R1-PM7:V</th>\n",
              "      <th>R1-PA8:VH</th>\n",
              "      <th>R1-PM8:V</th>\n",
              "      <th>R1-PA9:VH</th>\n",
              "      <th>R1-PM9:V</th>\n",
              "      <th>R1-PA10:IH</th>\n",
              "      <th>R1-PM10:I</th>\n",
              "      <th>R1-PA11:IH</th>\n",
              "      <th>R1-PM11:I</th>\n",
              "      <th>R1-PA12:IH</th>\n",
              "      <th>R1-PM12:I</th>\n",
              "      <th>R1:F</th>\n",
              "      <th>R1:DF</th>\n",
              "      <th>R1-PA:Z</th>\n",
              "      <th>R1-PA:ZH</th>\n",
              "      <th>R1:S</th>\n",
              "      <th>R2-PA1:VH</th>\n",
              "      <th>R2-PM1:V</th>\n",
              "      <th>R2-PA2:VH</th>\n",
              "      <th>R2-PM2:V</th>\n",
              "      <th>R2-PA3:VH</th>\n",
              "      <th>R2-PM3:V</th>\n",
              "      <th>R2-PA4:IH</th>\n",
              "      <th>R2-PM4:I</th>\n",
              "      <th>R2-PA5:IH</th>\n",
              "      <th>R2-PM5:I</th>\n",
              "      <th>R2-PA6:IH</th>\n",
              "      <th>...</th>\n",
              "      <th>R4-PA2:VH</th>\n",
              "      <th>R4-PM2:V</th>\n",
              "      <th>R4-PA3:VH</th>\n",
              "      <th>R4-PM3:V</th>\n",
              "      <th>R4-PA4:IH</th>\n",
              "      <th>R4-PM4:I</th>\n",
              "      <th>R4-PA5:IH</th>\n",
              "      <th>R4-PM5:I</th>\n",
              "      <th>R4-PA6:IH</th>\n",
              "      <th>R4-PM6:I</th>\n",
              "      <th>R4-PA7:VH</th>\n",
              "      <th>R4-PM7:V</th>\n",
              "      <th>R4-PA8:VH</th>\n",
              "      <th>R4-PM8:V</th>\n",
              "      <th>R4-PA9:VH</th>\n",
              "      <th>R4-PM9:V</th>\n",
              "      <th>R4-PA10:IH</th>\n",
              "      <th>R4-PM10:I</th>\n",
              "      <th>R4-PA11:IH</th>\n",
              "      <th>R4-PM11:I</th>\n",
              "      <th>R4-PA12:IH</th>\n",
              "      <th>R4-PM12:I</th>\n",
              "      <th>R4:F</th>\n",
              "      <th>R4:DF</th>\n",
              "      <th>R4-PA:Z</th>\n",
              "      <th>R4-PA:ZH</th>\n",
              "      <th>R4:S</th>\n",
              "      <th>control_panel_log1</th>\n",
              "      <th>control_panel_log2</th>\n",
              "      <th>control_panel_log3</th>\n",
              "      <th>control_panel_log4</th>\n",
              "      <th>relay1_log</th>\n",
              "      <th>relay2_log</th>\n",
              "      <th>relay3_log</th>\n",
              "      <th>relay4_log</th>\n",
              "      <th>snort_log1</th>\n",
              "      <th>snort_log2</th>\n",
              "      <th>snort_log3</th>\n",
              "      <th>snort_log4</th>\n",
              "      <th>marker</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70.399324</td>\n",
              "      <td>127673.0908</td>\n",
              "      <td>-49.572308</td>\n",
              "      <td>127648.0176</td>\n",
              "      <td>-169.578319</td>\n",
              "      <td>127723.2374</td>\n",
              "      <td>65.689611</td>\n",
              "      <td>605.91099</td>\n",
              "      <td>-57.003571</td>\n",
              "      <td>626.78553</td>\n",
              "      <td>-173.589023</td>\n",
              "      <td>602.43190</td>\n",
              "      <td>70.422243</td>\n",
              "      <td>127673.0908</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.007791</td>\n",
              "      <td>611.58740</td>\n",
              "      <td>118.567886</td>\n",
              "      <td>13.18392</td>\n",
              "      <td>-100.869220</td>\n",
              "      <td>13.91636</td>\n",
              "      <td>59.999</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.391383</td>\n",
              "      <td>0.076290</td>\n",
              "      <td>0</td>\n",
              "      <td>60.658268</td>\n",
              "      <td>124631.8125</td>\n",
              "      <td>-59.295959</td>\n",
              "      <td>124484.3594</td>\n",
              "      <td>-179.338078</td>\n",
              "      <td>124715.0703</td>\n",
              "      <td>-119.550481</td>\n",
              "      <td>612.796753</td>\n",
              "      <td>117.726752</td>\n",
              "      <td>632.532104</td>\n",
              "      <td>0.859680</td>\n",
              "      <td>...</td>\n",
              "      <td>-49.537931</td>\n",
              "      <td>127096.4056</td>\n",
              "      <td>-169.532482</td>\n",
              "      <td>127773.3839</td>\n",
              "      <td>65.643775</td>\n",
              "      <td>604.44611</td>\n",
              "      <td>-56.871791</td>\n",
              "      <td>621.84156</td>\n",
              "      <td>-173.869773</td>\n",
              "      <td>599.86836</td>\n",
              "      <td>70.462350</td>\n",
              "      <td>127522.6512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.950496</td>\n",
              "      <td>608.47453</td>\n",
              "      <td>119.301272</td>\n",
              "      <td>12.26837</td>\n",
              "      <td>-102.060972</td>\n",
              "      <td>11.71904</td>\n",
              "      <td>59.999</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.341832</td>\n",
              "      <td>0.077897</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>73.688102</td>\n",
              "      <td>130280.7109</td>\n",
              "      <td>-46.300719</td>\n",
              "      <td>130255.6377</td>\n",
              "      <td>-166.278082</td>\n",
              "      <td>130355.9307</td>\n",
              "      <td>71.831719</td>\n",
              "      <td>483.59351</td>\n",
              "      <td>-50.947407</td>\n",
              "      <td>500.98896</td>\n",
              "      <td>-167.487023</td>\n",
              "      <td>481.39619</td>\n",
              "      <td>73.705291</td>\n",
              "      <td>130305.7842</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.109792</td>\n",
              "      <td>488.35437</td>\n",
              "      <td>125.792884</td>\n",
              "      <td>10.62038</td>\n",
              "      <td>-95.884487</td>\n",
              "      <td>11.35282</td>\n",
              "      <td>60.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.185463</td>\n",
              "      <td>0.024924</td>\n",
              "      <td>0</td>\n",
              "      <td>66.069035</td>\n",
              "      <td>128277.4297</td>\n",
              "      <td>-53.885193</td>\n",
              "      <td>128126.3047</td>\n",
              "      <td>-173.932803</td>\n",
              "      <td>128354.7578</td>\n",
              "      <td>-114.925241</td>\n",
              "      <td>489.349365</td>\n",
              "      <td>122.148740</td>\n",
              "      <td>505.754456</td>\n",
              "      <td>5.468445</td>\n",
              "      <td>...</td>\n",
              "      <td>-46.249153</td>\n",
              "      <td>129704.0257</td>\n",
              "      <td>-166.232245</td>\n",
              "      <td>130381.0040</td>\n",
              "      <td>71.837448</td>\n",
              "      <td>481.76241</td>\n",
              "      <td>-50.792709</td>\n",
              "      <td>496.04499</td>\n",
              "      <td>-167.618803</td>\n",
              "      <td>477.73399</td>\n",
              "      <td>73.756857</td>\n",
              "      <td>130130.2713</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.121251</td>\n",
              "      <td>485.05839</td>\n",
              "      <td>124.274546</td>\n",
              "      <td>10.25416</td>\n",
              "      <td>-95.454769</td>\n",
              "      <td>9.70483</td>\n",
              "      <td>60.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.141328</td>\n",
              "      <td>0.027210</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>73.733939</td>\n",
              "      <td>130305.7842</td>\n",
              "      <td>-46.254883</td>\n",
              "      <td>130280.7109</td>\n",
              "      <td>-166.232245</td>\n",
              "      <td>130381.0040</td>\n",
              "      <td>71.808800</td>\n",
              "      <td>483.59351</td>\n",
              "      <td>-50.913030</td>\n",
              "      <td>500.98896</td>\n",
              "      <td>-167.441186</td>\n",
              "      <td>481.02997</td>\n",
              "      <td>73.751127</td>\n",
              "      <td>130330.8575</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.132710</td>\n",
              "      <td>488.35437</td>\n",
              "      <td>125.007932</td>\n",
              "      <td>10.62038</td>\n",
              "      <td>-94.520847</td>\n",
              "      <td>11.35282</td>\n",
              "      <td>60.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.190006</td>\n",
              "      <td>0.027904</td>\n",
              "      <td>0</td>\n",
              "      <td>66.112980</td>\n",
              "      <td>128283.9453</td>\n",
              "      <td>-53.852234</td>\n",
              "      <td>128144.4688</td>\n",
              "      <td>-173.883366</td>\n",
              "      <td>128382.9609</td>\n",
              "      <td>-114.919736</td>\n",
              "      <td>488.885864</td>\n",
              "      <td>122.214665</td>\n",
              "      <td>505.531311</td>\n",
              "      <td>5.462952</td>\n",
              "      <td>...</td>\n",
              "      <td>-46.197587</td>\n",
              "      <td>129729.0990</td>\n",
              "      <td>-166.192138</td>\n",
              "      <td>130381.0040</td>\n",
              "      <td>71.866096</td>\n",
              "      <td>481.39619</td>\n",
              "      <td>-50.781249</td>\n",
              "      <td>496.41121</td>\n",
              "      <td>-167.590155</td>\n",
              "      <td>478.10021</td>\n",
              "      <td>73.796964</td>\n",
              "      <td>130155.3446</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.144169</td>\n",
              "      <td>485.24150</td>\n",
              "      <td>125.277222</td>\n",
              "      <td>10.25416</td>\n",
              "      <td>-95.970431</td>\n",
              "      <td>10.07105</td>\n",
              "      <td>60.005</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.162755</td>\n",
              "      <td>0.026663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>74.083443</td>\n",
              "      <td>130581.5902</td>\n",
              "      <td>-45.899649</td>\n",
              "      <td>130556.5169</td>\n",
              "      <td>-165.882741</td>\n",
              "      <td>130656.8100</td>\n",
              "      <td>72.152575</td>\n",
              "      <td>482.86107</td>\n",
              "      <td>-50.437475</td>\n",
              "      <td>499.15786</td>\n",
              "      <td>-167.286487</td>\n",
              "      <td>481.39619</td>\n",
              "      <td>74.106361</td>\n",
              "      <td>130581.5902</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.459296</td>\n",
              "      <td>487.62193</td>\n",
              "      <td>127.597701</td>\n",
              "      <td>9.70483</td>\n",
              "      <td>-96.657980</td>\n",
              "      <td>10.43727</td>\n",
              "      <td>60.003</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.171532</td>\n",
              "      <td>0.025617</td>\n",
              "      <td>0</td>\n",
              "      <td>66.508489</td>\n",
              "      <td>128585.4063</td>\n",
              "      <td>-53.445742</td>\n",
              "      <td>128442.6094</td>\n",
              "      <td>-173.487857</td>\n",
              "      <td>128673.4141</td>\n",
              "      <td>-114.815371</td>\n",
              "      <td>489.864349</td>\n",
              "      <td>122.648628</td>\n",
              "      <td>504.226685</td>\n",
              "      <td>5.693665</td>\n",
              "      <td>...</td>\n",
              "      <td>-45.848083</td>\n",
              "      <td>129979.8317</td>\n",
              "      <td>-165.836904</td>\n",
              "      <td>130681.8832</td>\n",
              "      <td>72.181223</td>\n",
              "      <td>481.21308</td>\n",
              "      <td>-50.311424</td>\n",
              "      <td>494.76322</td>\n",
              "      <td>-167.395349</td>\n",
              "      <td>478.28332</td>\n",
              "      <td>74.152198</td>\n",
              "      <td>130431.1505</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.482215</td>\n",
              "      <td>484.69217</td>\n",
              "      <td>126.474704</td>\n",
              "      <td>9.33861</td>\n",
              "      <td>-97.253856</td>\n",
              "      <td>9.15550</td>\n",
              "      <td>60.003</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.135292</td>\n",
              "      <td>0.026595</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74.553268</td>\n",
              "      <td>131083.0556</td>\n",
              "      <td>-45.424094</td>\n",
              "      <td>131057.9823</td>\n",
              "      <td>-165.424375</td>\n",
              "      <td>131158.2754</td>\n",
              "      <td>72.118198</td>\n",
              "      <td>484.50906</td>\n",
              "      <td>-50.013486</td>\n",
              "      <td>497.69298</td>\n",
              "      <td>-167.464104</td>\n",
              "      <td>484.69217</td>\n",
              "      <td>74.570457</td>\n",
              "      <td>131108.1288</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.539510</td>\n",
              "      <td>488.90370</td>\n",
              "      <td>127.666456</td>\n",
              "      <td>7.50751</td>\n",
              "      <td>-99.923839</td>\n",
              "      <td>8.60617</td>\n",
              "      <td>60.001</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.079496</td>\n",
              "      <td>0.032941</td>\n",
              "      <td>0</td>\n",
              "      <td>67.035827</td>\n",
              "      <td>129106.9063</td>\n",
              "      <td>-52.912905</td>\n",
              "      <td>128974.5859</td>\n",
              "      <td>-172.949529</td>\n",
              "      <td>129196.8125</td>\n",
              "      <td>-114.919736</td>\n",
              "      <td>491.294861</td>\n",
              "      <td>123.071594</td>\n",
              "      <td>502.498627</td>\n",
              "      <td>5.413513</td>\n",
              "      <td>...</td>\n",
              "      <td>-45.372528</td>\n",
              "      <td>130506.3704</td>\n",
              "      <td>-165.372808</td>\n",
              "      <td>131183.3486</td>\n",
              "      <td>72.164034</td>\n",
              "      <td>482.86107</td>\n",
              "      <td>-49.881706</td>\n",
              "      <td>493.11523</td>\n",
              "      <td>-167.601614</td>\n",
              "      <td>481.21308</td>\n",
              "      <td>74.627753</td>\n",
              "      <td>130932.6159</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>71.556699</td>\n",
              "      <td>485.60772</td>\n",
              "      <td>126.595025</td>\n",
              "      <td>7.32440</td>\n",
              "      <td>-101.711468</td>\n",
              "      <td>7.14129</td>\n",
              "      <td>60.001</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.036023</td>\n",
              "      <td>0.033641</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 129 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   R1-PA1:VH     R1-PM1:V  R1-PA2:VH  ...  snort_log3  snort_log4  marker\n",
              "0  70.399324  127673.0908 -49.572308  ...           0           0       0\n",
              "1  73.688102  130280.7109 -46.300719  ...           0           0       0\n",
              "2  73.733939  130305.7842 -46.254883  ...           0           0       0\n",
              "3  74.083443  130581.5902 -45.899649  ...           0           0       0\n",
              "4  74.553268  131083.0556 -45.424094  ...           0           0       0\n",
              "\n",
              "[5 rows x 129 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeMmh-sjd3NT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f113b0ec-e0c1-446a-f1cd-e55e72f79e27"
      },
      "source": [
        "#سيعطي هذا مجموعة من الأماكن حيث توجد قيم NA.\n",
        " \n",
        "df[df==np.inf]=np.nan\n",
        "#df.fillna(df.max(), inplace=True)\n",
        "df.fillna(df.mean(), inplace=True)\n",
        " \n",
        "#df.fillna(0, inplace=True)\n",
        "##ذا كانت بياناتك تحتوي على Nan ، فجرّب ما يلي:\n",
        "np.isnan(df.values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru6NDUqd_l7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73458013-9cbf-4017-f8d4-41e9076371c0"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 129)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9bv13U6oe4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert type float and string\n",
        " \n",
        "X= df.drop(\"marker\", axis = 1)\n",
        "X=X.astype(\"float\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Rz9Y4W1uyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from sklearn.preprocessing import Normalizer\n",
        "transformer= Normalizer().fit(X) \n",
        "transformer\n",
        "S=transformer.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lTDTP3hoZyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "50fea004-dfea-4c0c-93fd-2ff6e9cb3e05"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "ss = StandardScaler()\n",
        " \n",
        "x = pd.DataFrame(ss.fit_transform(S))\n",
        " \n",
        "y = df[\"marker\"]\n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.874304</td>\n",
              "      <td>0.099036</td>\n",
              "      <td>-0.474731</td>\n",
              "      <td>0.061580</td>\n",
              "      <td>-1.867958</td>\n",
              "      <td>0.023939</td>\n",
              "      <td>0.824335</td>\n",
              "      <td>1.018163</td>\n",
              "      <td>-0.569834</td>\n",
              "      <td>1.564347</td>\n",
              "      <td>-1.946451</td>\n",
              "      <td>1.417422</td>\n",
              "      <td>0.874513</td>\n",
              "      <td>0.069631</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.819024</td>\n",
              "      <td>1.415316</td>\n",
              "      <td>1.806088</td>\n",
              "      <td>0.043663</td>\n",
              "      <td>-1.503010</td>\n",
              "      <td>-0.004960</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100647</td>\n",
              "      <td>-0.023363</td>\n",
              "      <td>0.205864</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.763126</td>\n",
              "      <td>0.129576</td>\n",
              "      <td>-0.593351</td>\n",
              "      <td>0.052249</td>\n",
              "      <td>-1.979122</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>-1.394714</td>\n",
              "      <td>1.193198</td>\n",
              "      <td>1.353806</td>\n",
              "      <td>1.556328</td>\n",
              "      <td>0.066997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063134</td>\n",
              "      <td>-0.412118</td>\n",
              "      <td>-0.061167</td>\n",
              "      <td>-1.434133</td>\n",
              "      <td>-0.019137</td>\n",
              "      <td>0.620453</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>-0.567535</td>\n",
              "      <td>0.131541</td>\n",
              "      <td>-1.941687</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.869845</td>\n",
              "      <td>-0.002144</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.814642</td>\n",
              "      <td>1.448899</td>\n",
              "      <td>1.834972</td>\n",
              "      <td>0.029236</td>\n",
              "      <td>-1.538149</td>\n",
              "      <td>0.036196</td>\n",
              "      <td>-0.013647</td>\n",
              "      <td>0.100102</td>\n",
              "      <td>-0.022051</td>\n",
              "      <td>0.010575</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.889684</td>\n",
              "      <td>0.036331</td>\n",
              "      <td>-0.434350</td>\n",
              "      <td>-0.014128</td>\n",
              "      <td>-1.790412</td>\n",
              "      <td>-0.049095</td>\n",
              "      <td>0.869987</td>\n",
              "      <td>0.398801</td>\n",
              "      <td>-0.501111</td>\n",
              "      <td>0.701235</td>\n",
              "      <td>-1.836419</td>\n",
              "      <td>0.602633</td>\n",
              "      <td>0.889830</td>\n",
              "      <td>-0.003897</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864449</td>\n",
              "      <td>0.601895</td>\n",
              "      <td>1.868931</td>\n",
              "      <td>-0.007345</td>\n",
              "      <td>-1.400215</td>\n",
              "      <td>-0.008872</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020642</td>\n",
              "      <td>0.008042</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.801732</td>\n",
              "      <td>0.166731</td>\n",
              "      <td>-0.531031</td>\n",
              "      <td>0.101154</td>\n",
              "      <td>-1.876163</td>\n",
              "      <td>0.083426</td>\n",
              "      <td>-1.318168</td>\n",
              "      <td>0.481710</td>\n",
              "      <td>1.369793</td>\n",
              "      <td>0.693499</td>\n",
              "      <td>0.109440</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000035</td>\n",
              "      <td>-0.377415</td>\n",
              "      <td>-0.134235</td>\n",
              "      <td>-1.374928</td>\n",
              "      <td>-0.094753</td>\n",
              "      <td>0.655781</td>\n",
              "      <td>0.075937</td>\n",
              "      <td>-0.498681</td>\n",
              "      <td>0.058612</td>\n",
              "      <td>-1.830469</td>\n",
              "      <td>0.048130</td>\n",
              "      <td>0.885227</td>\n",
              "      <td>-0.079381</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860780</td>\n",
              "      <td>0.614187</td>\n",
              "      <td>1.864821</td>\n",
              "      <td>-0.011410</td>\n",
              "      <td>-1.411007</td>\n",
              "      <td>-0.005342</td>\n",
              "      <td>-0.014110</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019336</td>\n",
              "      <td>0.008870</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.890043</td>\n",
              "      <td>0.037227</td>\n",
              "      <td>-0.433884</td>\n",
              "      <td>-0.013045</td>\n",
              "      <td>-1.789712</td>\n",
              "      <td>-0.048005</td>\n",
              "      <td>0.869660</td>\n",
              "      <td>0.398513</td>\n",
              "      <td>-0.500733</td>\n",
              "      <td>0.700832</td>\n",
              "      <td>-1.835701</td>\n",
              "      <td>0.600059</td>\n",
              "      <td>0.890189</td>\n",
              "      <td>-0.002797</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864586</td>\n",
              "      <td>0.601516</td>\n",
              "      <td>1.857189</td>\n",
              "      <td>-0.007370</td>\n",
              "      <td>-1.381310</td>\n",
              "      <td>-0.008874</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020636</td>\n",
              "      <td>0.019097</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.802080</td>\n",
              "      <td>0.166042</td>\n",
              "      <td>-0.530669</td>\n",
              "      <td>0.101267</td>\n",
              "      <td>-1.875407</td>\n",
              "      <td>0.084375</td>\n",
              "      <td>-1.317960</td>\n",
              "      <td>0.479000</td>\n",
              "      <td>1.370322</td>\n",
              "      <td>0.691744</td>\n",
              "      <td>0.109383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>-0.376972</td>\n",
              "      <td>-0.133153</td>\n",
              "      <td>-1.374439</td>\n",
              "      <td>-0.097088</td>\n",
              "      <td>0.655930</td>\n",
              "      <td>0.075577</td>\n",
              "      <td>-0.498514</td>\n",
              "      <td>0.058768</td>\n",
              "      <td>-1.829935</td>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.885529</td>\n",
              "      <td>-0.078267</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860917</td>\n",
              "      <td>0.614905</td>\n",
              "      <td>1.879398</td>\n",
              "      <td>-0.011434</td>\n",
              "      <td>-1.418009</td>\n",
              "      <td>0.001236</td>\n",
              "      <td>-0.014112</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019302</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.891902</td>\n",
              "      <td>0.036003</td>\n",
              "      <td>-0.429791</td>\n",
              "      <td>-0.014517</td>\n",
              "      <td>-1.782334</td>\n",
              "      <td>-0.049518</td>\n",
              "      <td>0.871534</td>\n",
              "      <td>0.390431</td>\n",
              "      <td>-0.495338</td>\n",
              "      <td>0.682959</td>\n",
              "      <td>-1.830180</td>\n",
              "      <td>0.595921</td>\n",
              "      <td>0.892105</td>\n",
              "      <td>-0.007788</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.866303</td>\n",
              "      <td>0.590921</td>\n",
              "      <td>1.891025</td>\n",
              "      <td>-0.023903</td>\n",
              "      <td>-1.407753</td>\n",
              "      <td>-0.010136</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020697</td>\n",
              "      <td>0.010396</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.804544</td>\n",
              "      <td>0.167318</td>\n",
              "      <td>-0.525924</td>\n",
              "      <td>0.102684</td>\n",
              "      <td>-1.867290</td>\n",
              "      <td>0.085049</td>\n",
              "      <td>-1.314364</td>\n",
              "      <td>0.478504</td>\n",
              "      <td>1.372069</td>\n",
              "      <td>0.677139</td>\n",
              "      <td>0.111401</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000378</td>\n",
              "      <td>-0.373513</td>\n",
              "      <td>-0.137835</td>\n",
              "      <td>-1.368761</td>\n",
              "      <td>-0.095179</td>\n",
              "      <td>0.657141</td>\n",
              "      <td>0.074537</td>\n",
              "      <td>-0.493178</td>\n",
              "      <td>0.057356</td>\n",
              "      <td>-1.824012</td>\n",
              "      <td>0.047841</td>\n",
              "      <td>0.887439</td>\n",
              "      <td>-0.079738</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.862749</td>\n",
              "      <td>0.605183</td>\n",
              "      <td>1.892994</td>\n",
              "      <td>-0.027845</td>\n",
              "      <td>-1.432902</td>\n",
              "      <td>-0.015627</td>\n",
              "      <td>-0.014154</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019377</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.893597</td>\n",
              "      <td>0.033239</td>\n",
              "      <td>-0.423901</td>\n",
              "      <td>-0.017844</td>\n",
              "      <td>-1.770758</td>\n",
              "      <td>-0.052923</td>\n",
              "      <td>0.868269</td>\n",
              "      <td>0.389099</td>\n",
              "      <td>-0.489603</td>\n",
              "      <td>0.661865</td>\n",
              "      <td>-1.824963</td>\n",
              "      <td>0.604001</td>\n",
              "      <td>0.893742</td>\n",
              "      <td>-0.007717</td>\n",
              "      <td>-0.016352</td>\n",
              "      <td>-0.109622</td>\n",
              "      <td>-0.018922</td>\n",
              "      <td>-0.095906</td>\n",
              "      <td>0.864216</td>\n",
              "      <td>0.586934</td>\n",
              "      <td>1.884543</td>\n",
              "      <td>-0.063127</td>\n",
              "      <td>-1.447021</td>\n",
              "      <td>-0.012640</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.009767</td>\n",
              "      <td>-0.020903</td>\n",
              "      <td>0.037047</td>\n",
              "      <td>-0.058626</td>\n",
              "      <td>0.807090</td>\n",
              "      <td>0.167550</td>\n",
              "      <td>-0.519189</td>\n",
              "      <td>0.103949</td>\n",
              "      <td>-1.854435</td>\n",
              "      <td>0.085472</td>\n",
              "      <td>-1.310773</td>\n",
              "      <td>0.475735</td>\n",
              "      <td>1.371320</td>\n",
              "      <td>0.654512</td>\n",
              "      <td>0.108615</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003178</td>\n",
              "      <td>-0.368471</td>\n",
              "      <td>-0.137439</td>\n",
              "      <td>-1.359878</td>\n",
              "      <td>-0.098551</td>\n",
              "      <td>0.654775</td>\n",
              "      <td>0.074295</td>\n",
              "      <td>-0.487400</td>\n",
              "      <td>0.055487</td>\n",
              "      <td>-1.819112</td>\n",
              "      <td>0.048351</td>\n",
              "      <td>0.889183</td>\n",
              "      <td>-0.083081</td>\n",
              "      <td>-0.022001</td>\n",
              "      <td>-0.110079</td>\n",
              "      <td>-0.016681</td>\n",
              "      <td>-0.095525</td>\n",
              "      <td>0.860604</td>\n",
              "      <td>0.598972</td>\n",
              "      <td>1.887278</td>\n",
              "      <td>-0.063596</td>\n",
              "      <td>-1.489143</td>\n",
              "      <td>-0.052365</td>\n",
              "      <td>-0.014230</td>\n",
              "      <td>-0.010384</td>\n",
              "      <td>-0.019594</td>\n",
              "      <td>0.009072</td>\n",
              "      <td>-0.054457</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.006187</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.005045</td>\n",
              "      <td>-0.191576</td>\n",
              "      <td>-0.164491</td>\n",
              "      <td>-0.164691</td>\n",
              "      <td>-0.190751</td>\n",
              "      <td>-0.007073</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.00943</td>\n",
              "      <td>-0.008728</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       125      126       127\n",
              "0  0.874304  0.099036 -0.474731  ... -0.007144 -0.00943 -0.008728\n",
              "1  0.889684  0.036331 -0.434350  ... -0.007144 -0.00943 -0.008728\n",
              "2  0.890043  0.037227 -0.433884  ... -0.007144 -0.00943 -0.008728\n",
              "3  0.891902  0.036003 -0.429791  ... -0.007144 -0.00943 -0.008728\n",
              "4  0.893597  0.033239 -0.423901  ... -0.007144 -0.00943 -0.008728\n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nspOhTqZof2W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "bdb69953-85c4-43b6-ab13-18ac5c1bf032"
      },
      "source": [
        "# categorical target y to array \n",
        "from keras.utils.np_utils import to_categorical\n",
        " \n",
        "y_cat = to_categorical(y,2)\n",
        "y_cat[:10]\n",
        "y_cat.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       ...,\n",
              "       [1, 0],\n",
              "       [1, 0],\n",
              "       [1, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh-ds6_4h7lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ab7b59a-b760-42b4-abd4-33bbfad75bcd"
      },
      "source": [
        "y_cat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78377, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH8VkygRbp7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #divide datasets into to part training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_cat,test_size=0.1,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BStJhgIouRi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4327940f-2058-4dd4-9475-07f4867fca1b"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70539, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB_59ytHo-A_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abde2607-b5dc-484e-be46-78a20c467ceb"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7838, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7bS93Gqotq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=x_train.shape[1]\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        " \n",
        " \n",
        "model = keras.Sequential([\n",
        "    \n",
        "layers.Dense(512, activation='tanh', input_shape=[i,],kernel_initializer='he_uniform' ),\n",
        "    \n",
        " \n",
        "layers.Dense(1024, activation='tanh',kernel_initializer='he_uniform',use_bias=False   ),\n",
        "    \n",
        " \n",
        "layers.Dense(512, activation='tanh',kernel_initializer='he_uniform',use_bias=False   ),\n",
        "   \n",
        "layers.Dropout((0.01)),\n",
        " \n",
        "layers.Dense(512, activation='tanh',kernel_initializer='he_uniform',use_bias=False  ),\n",
        "   \n",
        " \n",
        "layers.Dense(256, activation='tanh',kernel_initializer='he_uniform',use_bias=False),\n",
        "    \n",
        "layers.Dense(256, activation='tanh',kernel_initializer='he_uniform',use_bias=False),\n",
        "   \n",
        " layers.Dense(128, activation='tanh',kernel_initializer='he_uniform',use_bias=False),\n",
        "    layers.Dropout((0.001)),\n",
        "    layers.Dense(2, activation='sigmoid',kernel_initializer='he_uniform' )\n",
        "  ])\n",
        " \n",
        "optimizer = tf.keras.optimizers.Adam(0.001,decay=0.000005)\n",
        " \n",
        "model.compile(loss=tf.losses.BinaryCrossentropy(),\n",
        "                optimizer=optimizer,\n",
        "                metrics=['accuracy'])\n",
        " \n",
        " \n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLqhkbvoR9hf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4e91cec5-a026-4d18-dc2b-fd75b11b0fdb"
      },
      "source": [
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miCFhzaY67Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
        "# patient early stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "mc = ModelCheckpoint('best_mlp_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "rp = ReduceLROnPlateau(monitor='val_accuracy',patience =5,verbose=1,factor=0.5,min_lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "notvYvu5_qVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "06d2fa58-dc06-4e55-9eff-470e86dbf7f4"
      },
      "source": [
        "start_time = time.time()\n",
        "fit1 = model.fit(x_train,y_train, epochs=250,batch_size=500,verbose=1,callbacks=[es,mc,rp],validation_split=0.1)\n",
        "score=model.evaluate(x_test,y_test)\n",
        "print('Accuracy(on Test-data): ' + str(score[1]))\n",
        "duration = time.time() - start_time\n",
        "print(\"time of training (s)\", duration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.6063 - accuracy: 0.7027\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.71179, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 7ms/step - loss: 0.6057 - accuracy: 0.7032 - val_loss: 0.5876 - val_accuracy: 0.7118\n",
            "Epoch 2/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5791 - accuracy: 0.7166\n",
            "Epoch 00002: val_accuracy did not improve from 0.71179\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.7166 - val_loss: 0.5883 - val_accuracy: 0.7117\n",
            "Epoch 3/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.5732 - accuracy: 0.7190\n",
            "Epoch 00003: val_accuracy improved from 0.71179 to 0.71350, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.5731 - accuracy: 0.7189 - val_loss: 0.5842 - val_accuracy: 0.7135\n",
            "Epoch 4/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.5687 - accuracy: 0.7211\n",
            "Epoch 00004: val_accuracy improved from 0.71350 to 0.71704, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.5687 - accuracy: 0.7212 - val_loss: 0.5812 - val_accuracy: 0.7170\n",
            "Epoch 5/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.5627 - accuracy: 0.7254\n",
            "Epoch 00005: val_accuracy improved from 0.71704 to 0.72058, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.5627 - accuracy: 0.7254 - val_loss: 0.5793 - val_accuracy: 0.7206\n",
            "Epoch 6/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.5549 - accuracy: 0.7278\n",
            "Epoch 00006: val_accuracy improved from 0.72058 to 0.72158, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.5556 - accuracy: 0.7271 - val_loss: 0.5564 - val_accuracy: 0.7216\n",
            "Epoch 7/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.5393 - accuracy: 0.7335\n",
            "Epoch 00007: val_accuracy improved from 0.72158 to 0.72370, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.5397 - accuracy: 0.7334 - val_loss: 0.5509 - val_accuracy: 0.7237\n",
            "Epoch 8/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.5261 - accuracy: 0.7420\n",
            "Epoch 00008: val_accuracy improved from 0.72370 to 0.72583, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.5261 - accuracy: 0.7419 - val_loss: 0.5438 - val_accuracy: 0.7258\n",
            "Epoch 9/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.5153 - accuracy: 0.7463\n",
            "Epoch 00009: val_accuracy improved from 0.72583 to 0.72696, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.5153 - accuracy: 0.7463 - val_loss: 0.5417 - val_accuracy: 0.7270\n",
            "Epoch 10/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.5036 - accuracy: 0.7494\n",
            "Epoch 00010: val_accuracy improved from 0.72696 to 0.73533, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.5030 - accuracy: 0.7493 - val_loss: 0.5184 - val_accuracy: 0.7353\n",
            "Epoch 11/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.5004 - accuracy: 0.7493\n",
            "Epoch 00011: val_accuracy improved from 0.73533 to 0.74071, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.4998 - accuracy: 0.7495 - val_loss: 0.5179 - val_accuracy: 0.7407\n",
            "Epoch 12/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.4838 - accuracy: 0.7558\n",
            "Epoch 00012: val_accuracy did not improve from 0.74071\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.4835 - accuracy: 0.7556 - val_loss: 0.5022 - val_accuracy: 0.7403\n",
            "Epoch 13/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.4711 - accuracy: 0.7617\n",
            "Epoch 00013: val_accuracy did not improve from 0.74071\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7609 - val_loss: 0.5182 - val_accuracy: 0.7358\n",
            "Epoch 14/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.4525 - accuracy: 0.7712\n",
            "Epoch 00014: val_accuracy improved from 0.74071 to 0.74724, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.4532 - accuracy: 0.7708 - val_loss: 0.4969 - val_accuracy: 0.7472\n",
            "Epoch 15/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.4498 - accuracy: 0.7706\n",
            "Epoch 00015: val_accuracy improved from 0.74724 to 0.75276, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.4501 - accuracy: 0.7701 - val_loss: 0.4897 - val_accuracy: 0.7528\n",
            "Epoch 16/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.4382 - accuracy: 0.7789\n",
            "Epoch 00016: val_accuracy improved from 0.75276 to 0.75546, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.4386 - accuracy: 0.7779 - val_loss: 0.4790 - val_accuracy: 0.7555\n",
            "Epoch 17/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.4261 - accuracy: 0.7831\n",
            "Epoch 00017: val_accuracy improved from 0.75546 to 0.75843, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.4264 - accuracy: 0.7830 - val_loss: 0.4760 - val_accuracy: 0.7584\n",
            "Epoch 18/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.7872\n",
            "Epoch 00018: val_accuracy improved from 0.75843 to 0.76212, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.4183 - accuracy: 0.7873 - val_loss: 0.4693 - val_accuracy: 0.7621\n",
            "Epoch 19/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.4103 - accuracy: 0.7915\n",
            "Epoch 00019: val_accuracy improved from 0.76212 to 0.76907, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.4108 - accuracy: 0.7914 - val_loss: 0.4715 - val_accuracy: 0.7691\n",
            "Epoch 20/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.3995 - accuracy: 0.7978\n",
            "Epoch 00020: val_accuracy did not improve from 0.76907\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.3979 - accuracy: 0.7988 - val_loss: 0.4876 - val_accuracy: 0.7675\n",
            "Epoch 21/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.3957 - accuracy: 0.8012\n",
            "Epoch 00021: val_accuracy improved from 0.76907 to 0.76935, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3952 - accuracy: 0.8016 - val_loss: 0.4646 - val_accuracy: 0.7694\n",
            "Epoch 22/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3822 - accuracy: 0.8091\n",
            "Epoch 00022: val_accuracy improved from 0.76935 to 0.77616, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.3823 - accuracy: 0.8090 - val_loss: 0.4541 - val_accuracy: 0.7762\n",
            "Epoch 23/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.3729 - accuracy: 0.8122\n",
            "Epoch 00023: val_accuracy did not improve from 0.77616\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3725 - accuracy: 0.8122 - val_loss: 0.4618 - val_accuracy: 0.7682\n",
            "Epoch 24/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.3684 - accuracy: 0.8181\n",
            "Epoch 00024: val_accuracy did not improve from 0.77616\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3690 - accuracy: 0.8171 - val_loss: 0.4568 - val_accuracy: 0.7752\n",
            "Epoch 25/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.3547 - accuracy: 0.8242\n",
            "Epoch 00025: val_accuracy improved from 0.77616 to 0.78452, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.3546 - accuracy: 0.8244 - val_loss: 0.4595 - val_accuracy: 0.7845\n",
            "Epoch 26/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.3516 - accuracy: 0.8267\n",
            "Epoch 00026: val_accuracy improved from 0.78452 to 0.78480, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3511 - accuracy: 0.8268 - val_loss: 0.4434 - val_accuracy: 0.7848\n",
            "Epoch 27/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.3364 - accuracy: 0.8354\n",
            "Epoch 00027: val_accuracy improved from 0.78480 to 0.78665, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3376 - accuracy: 0.8346 - val_loss: 0.4515 - val_accuracy: 0.7866\n",
            "Epoch 28/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8405\n",
            "Epoch 00028: val_accuracy improved from 0.78665 to 0.80011, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3285 - accuracy: 0.8404 - val_loss: 0.4345 - val_accuracy: 0.8001\n",
            "Epoch 29/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.3224 - accuracy: 0.8431\n",
            "Epoch 00029: val_accuracy did not improve from 0.80011\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.3224 - accuracy: 0.8430 - val_loss: 0.4398 - val_accuracy: 0.7990\n",
            "Epoch 30/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.3195 - accuracy: 0.8448\n",
            "Epoch 00030: val_accuracy improved from 0.80011 to 0.80408, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.3195 - accuracy: 0.8449 - val_loss: 0.4311 - val_accuracy: 0.8041\n",
            "Epoch 31/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.3042 - accuracy: 0.8524\n",
            "Epoch 00031: val_accuracy improved from 0.80408 to 0.81131, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.3041 - accuracy: 0.8524 - val_loss: 0.4233 - val_accuracy: 0.8113\n",
            "Epoch 32/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.3000 - accuracy: 0.8550\n",
            "Epoch 00032: val_accuracy did not improve from 0.81131\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.8549 - val_loss: 0.4360 - val_accuracy: 0.8064\n",
            "Epoch 33/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2917 - accuracy: 0.8607\n",
            "Epoch 00033: val_accuracy did not improve from 0.81131\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.2915 - accuracy: 0.8605 - val_loss: 0.4184 - val_accuracy: 0.8113\n",
            "Epoch 34/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.2838 - accuracy: 0.8643\n",
            "Epoch 00034: val_accuracy improved from 0.81131 to 0.81216, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2849 - accuracy: 0.8634 - val_loss: 0.4293 - val_accuracy: 0.8122\n",
            "Epoch 35/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2788 - accuracy: 0.8662\n",
            "Epoch 00035: val_accuracy improved from 0.81216 to 0.81372, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2787 - accuracy: 0.8666 - val_loss: 0.4427 - val_accuracy: 0.8137\n",
            "Epoch 36/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8694\n",
            "Epoch 00036: val_accuracy did not improve from 0.81372\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2735 - accuracy: 0.8696 - val_loss: 0.4321 - val_accuracy: 0.8122\n",
            "Epoch 37/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2647 - accuracy: 0.8739\n",
            "Epoch 00037: val_accuracy improved from 0.81372 to 0.82450, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2655 - accuracy: 0.8735 - val_loss: 0.4075 - val_accuracy: 0.8245\n",
            "Epoch 38/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2597 - accuracy: 0.8780\n",
            "Epoch 00038: val_accuracy did not improve from 0.82450\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.2589 - accuracy: 0.8785 - val_loss: 0.4279 - val_accuracy: 0.8222\n",
            "Epoch 39/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2528 - accuracy: 0.8821\n",
            "Epoch 00039: val_accuracy improved from 0.82450 to 0.82960, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2522 - accuracy: 0.8827 - val_loss: 0.4157 - val_accuracy: 0.8296\n",
            "Epoch 40/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.8838\n",
            "Epoch 00040: val_accuracy improved from 0.82960 to 0.83130, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2499 - accuracy: 0.8838 - val_loss: 0.4089 - val_accuracy: 0.8313\n",
            "Epoch 41/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2389 - accuracy: 0.8886\n",
            "Epoch 00041: val_accuracy improved from 0.83130 to 0.83286, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2402 - accuracy: 0.8881 - val_loss: 0.4131 - val_accuracy: 0.8329\n",
            "Epoch 42/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2405 - accuracy: 0.8894\n",
            "Epoch 00042: val_accuracy did not improve from 0.83286\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2406 - accuracy: 0.8891 - val_loss: 0.4252 - val_accuracy: 0.8295\n",
            "Epoch 43/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.2274 - accuracy: 0.8935\n",
            "Epoch 00043: val_accuracy improved from 0.83286 to 0.84193, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.2286 - accuracy: 0.8930 - val_loss: 0.4014 - val_accuracy: 0.8419\n",
            "Epoch 44/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2271 - accuracy: 0.8958\n",
            "Epoch 00044: val_accuracy improved from 0.84193 to 0.84321, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.2260 - accuracy: 0.8961 - val_loss: 0.4128 - val_accuracy: 0.8432\n",
            "Epoch 45/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2189 - accuracy: 0.8987\n",
            "Epoch 00045: val_accuracy did not improve from 0.84321\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2188 - accuracy: 0.8989 - val_loss: 0.4259 - val_accuracy: 0.8388\n",
            "Epoch 46/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2128 - accuracy: 0.9015\n",
            "Epoch 00046: val_accuracy improved from 0.84321 to 0.84434, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.2130 - accuracy: 0.9013 - val_loss: 0.3959 - val_accuracy: 0.8443\n",
            "Epoch 47/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.2111 - accuracy: 0.9030\n",
            "Epoch 00047: val_accuracy did not improve from 0.84434\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.2112 - accuracy: 0.9029 - val_loss: 0.4089 - val_accuracy: 0.8417\n",
            "Epoch 48/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.2066 - accuracy: 0.9054\n",
            "Epoch 00048: val_accuracy improved from 0.84434 to 0.84647, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.2072 - accuracy: 0.9052 - val_loss: 0.4103 - val_accuracy: 0.8465\n",
            "Epoch 49/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.2031 - accuracy: 0.9062\n",
            "Epoch 00049: val_accuracy did not improve from 0.84647\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.2043 - accuracy: 0.9055 - val_loss: 0.4044 - val_accuracy: 0.8460\n",
            "Epoch 50/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.2002 - accuracy: 0.9089\n",
            "Epoch 00050: val_accuracy improved from 0.84647 to 0.85342, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.2000 - accuracy: 0.9091 - val_loss: 0.3968 - val_accuracy: 0.8534\n",
            "Epoch 51/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.1947 - accuracy: 0.9113\n",
            "Epoch 00051: val_accuracy improved from 0.85342 to 0.85852, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.1947 - accuracy: 0.9113 - val_loss: 0.3867 - val_accuracy: 0.8585\n",
            "Epoch 52/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.1910 - accuracy: 0.9129\n",
            "Epoch 00052: val_accuracy did not improve from 0.85852\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.1926 - accuracy: 0.9125 - val_loss: 0.4070 - val_accuracy: 0.8510\n",
            "Epoch 53/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.1942 - accuracy: 0.9124\n",
            "Epoch 00053: val_accuracy did not improve from 0.85852\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1945 - accuracy: 0.9123 - val_loss: 0.3927 - val_accuracy: 0.8563\n",
            "Epoch 54/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1857 - accuracy: 0.9164\n",
            "Epoch 00054: val_accuracy did not improve from 0.85852\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1863 - accuracy: 0.9162 - val_loss: 0.3951 - val_accuracy: 0.8537\n",
            "Epoch 55/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1863 - accuracy: 0.9168\n",
            "Epoch 00055: val_accuracy did not improve from 0.85852\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1861 - accuracy: 0.9169 - val_loss: 0.4081 - val_accuracy: 0.8540\n",
            "Epoch 56/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1845 - accuracy: 0.9169\n",
            "Epoch 00056: val_accuracy did not improve from 0.85852\n",
            "\n",
            "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1841 - accuracy: 0.9173 - val_loss: 0.3923 - val_accuracy: 0.8560\n",
            "Epoch 57/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1355 - accuracy: 0.9401\n",
            "Epoch 00057: val_accuracy improved from 0.85852 to 0.87695, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.1353 - accuracy: 0.9402 - val_loss: 0.3906 - val_accuracy: 0.8769\n",
            "Epoch 58/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.1202 - accuracy: 0.9459\n",
            "Epoch 00058: val_accuracy improved from 0.87695 to 0.88489, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.1207 - accuracy: 0.9453 - val_loss: 0.3934 - val_accuracy: 0.8849\n",
            "Epoch 59/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9506\n",
            "Epoch 00059: val_accuracy did not improve from 0.88489\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1125 - accuracy: 0.9506 - val_loss: 0.4058 - val_accuracy: 0.8836\n",
            "Epoch 60/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.1069 - accuracy: 0.9530\n",
            "Epoch 00060: val_accuracy improved from 0.88489 to 0.88574, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.1073 - accuracy: 0.9528 - val_loss: 0.3991 - val_accuracy: 0.8857\n",
            "Epoch 61/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.1031 - accuracy: 0.9549\n",
            "Epoch 00061: val_accuracy improved from 0.88574 to 0.89056, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.1032 - accuracy: 0.9548 - val_loss: 0.4026 - val_accuracy: 0.8906\n",
            "Epoch 62/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.1065 - accuracy: 0.9524\n",
            "Epoch 00062: val_accuracy did not improve from 0.89056\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.1061 - accuracy: 0.9525 - val_loss: 0.4167 - val_accuracy: 0.8893\n",
            "Epoch 63/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1016 - accuracy: 0.9559\n",
            "Epoch 00063: val_accuracy improved from 0.89056 to 0.89070, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.1012 - accuracy: 0.9561 - val_loss: 0.4147 - val_accuracy: 0.8907\n",
            "Epoch 64/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.1055 - accuracy: 0.9543\n",
            "Epoch 00064: val_accuracy did not improve from 0.89070\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1049 - accuracy: 0.9546 - val_loss: 0.4379 - val_accuracy: 0.8830\n",
            "Epoch 65/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.1014 - accuracy: 0.9549\n",
            "Epoch 00065: val_accuracy did not improve from 0.89070\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.1013 - accuracy: 0.9552 - val_loss: 0.4166 - val_accuracy: 0.8900\n",
            "Epoch 66/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0952 - accuracy: 0.9583\n",
            "Epoch 00066: val_accuracy did not improve from 0.89070\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0956 - accuracy: 0.9580 - val_loss: 0.4334 - val_accuracy: 0.8881\n",
            "Epoch 67/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0955 - accuracy: 0.9584\n",
            "Epoch 00067: val_accuracy improved from 0.89070 to 0.89637, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0952 - accuracy: 0.9584 - val_loss: 0.4108 - val_accuracy: 0.8964\n",
            "Epoch 68/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9610\n",
            "Epoch 00068: val_accuracy did not improve from 0.89637\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0893 - accuracy: 0.9609 - val_loss: 0.4169 - val_accuracy: 0.8911\n",
            "Epoch 69/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0898 - accuracy: 0.9613\n",
            "Epoch 00069: val_accuracy did not improve from 0.89637\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0901 - accuracy: 0.9612 - val_loss: 0.4169 - val_accuracy: 0.8933\n",
            "Epoch 70/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0874 - accuracy: 0.9615\n",
            "Epoch 00070: val_accuracy did not improve from 0.89637\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0872 - accuracy: 0.9618 - val_loss: 0.4317 - val_accuracy: 0.8954\n",
            "Epoch 71/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0895 - accuracy: 0.9616\n",
            "Epoch 00071: val_accuracy did not improve from 0.89637\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0899 - accuracy: 0.9613 - val_loss: 0.4189 - val_accuracy: 0.8921\n",
            "Epoch 72/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0850 - accuracy: 0.9637\n",
            "Epoch 00072: val_accuracy did not improve from 0.89637\n",
            "\n",
            "Epoch 00072: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0859 - accuracy: 0.9631 - val_loss: 0.4217 - val_accuracy: 0.8928\n",
            "Epoch 73/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0606 - accuracy: 0.9733\n",
            "Epoch 00073: val_accuracy improved from 0.89637 to 0.90856, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0608 - accuracy: 0.9733 - val_loss: 0.4347 - val_accuracy: 0.9086\n",
            "Epoch 74/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9769\n",
            "Epoch 00074: val_accuracy improved from 0.90856 to 0.90913, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0533 - accuracy: 0.9769 - val_loss: 0.4552 - val_accuracy: 0.9091\n",
            "Epoch 75/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0532 - accuracy: 0.9771\n",
            "Epoch 00075: val_accuracy did not improve from 0.90913\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9769 - val_loss: 0.4550 - val_accuracy: 0.9076\n",
            "Epoch 76/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0488 - accuracy: 0.9792\n",
            "Epoch 00076: val_accuracy improved from 0.90913 to 0.91055, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0487 - accuracy: 0.9790 - val_loss: 0.4633 - val_accuracy: 0.9105\n",
            "Epoch 77/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0494 - accuracy: 0.9790\n",
            "Epoch 00077: val_accuracy did not improve from 0.91055\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 0.9788 - val_loss: 0.4640 - val_accuracy: 0.9091\n",
            "Epoch 78/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.0473 - accuracy: 0.9793\n",
            "Epoch 00078: val_accuracy improved from 0.91055 to 0.91140, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0479 - accuracy: 0.9791 - val_loss: 0.4621 - val_accuracy: 0.9114\n",
            "Epoch 79/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0487 - accuracy: 0.9794\n",
            "Epoch 00079: val_accuracy did not improve from 0.91140\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0493 - accuracy: 0.9791 - val_loss: 0.4603 - val_accuracy: 0.9100\n",
            "Epoch 80/250\n",
            "113/127 [=========================>....] - ETA: 0s - loss: 0.0475 - accuracy: 0.9792\n",
            "Epoch 00080: val_accuracy did not improve from 0.91140\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0479 - accuracy: 0.9793 - val_loss: 0.4633 - val_accuracy: 0.9090\n",
            "Epoch 81/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0482 - accuracy: 0.9794\n",
            "Epoch 00081: val_accuracy improved from 0.91140 to 0.91239, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0481 - accuracy: 0.9795 - val_loss: 0.4743 - val_accuracy: 0.9124\n",
            "Epoch 82/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9806\n",
            "Epoch 00082: val_accuracy improved from 0.91239 to 0.91381, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0459 - accuracy: 0.9805 - val_loss: 0.4692 - val_accuracy: 0.9138\n",
            "Epoch 83/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0446 - accuracy: 0.9811\n",
            "Epoch 00083: val_accuracy did not improve from 0.91381\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0446 - accuracy: 0.9811 - val_loss: 0.4696 - val_accuracy: 0.9114\n",
            "Epoch 84/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0422 - accuracy: 0.9817\n",
            "Epoch 00084: val_accuracy did not improve from 0.91381\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0426 - accuracy: 0.9816 - val_loss: 0.4864 - val_accuracy: 0.9130\n",
            "Epoch 85/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0437 - accuracy: 0.9820\n",
            "Epoch 00085: val_accuracy improved from 0.91381 to 0.91551, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0443 - accuracy: 0.9818 - val_loss: 0.4703 - val_accuracy: 0.9155\n",
            "Epoch 86/250\n",
            "113/127 [=========================>....] - ETA: 0s - loss: 0.0432 - accuracy: 0.9822\n",
            "Epoch 00086: val_accuracy improved from 0.91551 to 0.91622, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0435 - accuracy: 0.9819 - val_loss: 0.4737 - val_accuracy: 0.9162\n",
            "Epoch 87/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9817\n",
            "Epoch 00087: val_accuracy improved from 0.91622 to 0.91678, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0437 - accuracy: 0.9817 - val_loss: 0.4704 - val_accuracy: 0.9168\n",
            "Epoch 88/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0445 - accuracy: 0.9817\n",
            "Epoch 00088: val_accuracy did not improve from 0.91678\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0445 - accuracy: 0.9817 - val_loss: 0.4622 - val_accuracy: 0.9144\n",
            "Epoch 89/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0420 - accuracy: 0.9829\n",
            "Epoch 00089: val_accuracy did not improve from 0.91678\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0420 - accuracy: 0.9829 - val_loss: 0.4760 - val_accuracy: 0.9132\n",
            "Epoch 90/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0424 - accuracy: 0.9819\n",
            "Epoch 00090: val_accuracy did not improve from 0.91678\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0423 - accuracy: 0.9820 - val_loss: 0.4745 - val_accuracy: 0.9155\n",
            "Epoch 91/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0400 - accuracy: 0.9832\n",
            "Epoch 00091: val_accuracy did not improve from 0.91678\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0404 - accuracy: 0.9829 - val_loss: 0.5010 - val_accuracy: 0.9086\n",
            "Epoch 92/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0412 - accuracy: 0.9823\n",
            "Epoch 00092: val_accuracy did not improve from 0.91678\n",
            "\n",
            "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9823 - val_loss: 0.4997 - val_accuracy: 0.9108\n",
            "Epoch 93/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0288 - accuracy: 0.9880\n",
            "Epoch 00093: val_accuracy improved from 0.91678 to 0.91863, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0285 - accuracy: 0.9881 - val_loss: 0.5021 - val_accuracy: 0.9186\n",
            "Epoch 94/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0251 - accuracy: 0.9895\n",
            "Epoch 00094: val_accuracy did not improve from 0.91863\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0250 - accuracy: 0.9895 - val_loss: 0.5196 - val_accuracy: 0.9181\n",
            "Epoch 95/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0247 - accuracy: 0.9887\n",
            "Epoch 00095: val_accuracy did not improve from 0.91863\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0249 - accuracy: 0.9886 - val_loss: 0.5269 - val_accuracy: 0.9182\n",
            "Epoch 96/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0240 - accuracy: 0.9895\n",
            "Epoch 00096: val_accuracy did not improve from 0.91863\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.9895 - val_loss: 0.5263 - val_accuracy: 0.9183\n",
            "Epoch 97/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0248 - accuracy: 0.9895\n",
            "Epoch 00097: val_accuracy improved from 0.91863 to 0.92033, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0250 - accuracy: 0.9894 - val_loss: 0.5242 - val_accuracy: 0.9203\n",
            "Epoch 98/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0245 - accuracy: 0.9895\n",
            "Epoch 00098: val_accuracy did not improve from 0.92033\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0246 - accuracy: 0.9896 - val_loss: 0.5329 - val_accuracy: 0.9182\n",
            "Epoch 99/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0236 - accuracy: 0.9902\n",
            "Epoch 00099: val_accuracy did not improve from 0.92033\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0239 - accuracy: 0.9901 - val_loss: 0.5330 - val_accuracy: 0.9199\n",
            "Epoch 100/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0231 - accuracy: 0.9904\n",
            "Epoch 00100: val_accuracy did not improve from 0.92033\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0232 - accuracy: 0.9904 - val_loss: 0.5360 - val_accuracy: 0.9181\n",
            "Epoch 101/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0219 - accuracy: 0.9905\n",
            "Epoch 00101: val_accuracy improved from 0.92033 to 0.92061, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0223 - accuracy: 0.9904 - val_loss: 0.5439 - val_accuracy: 0.9206\n",
            "Epoch 102/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0246 - accuracy: 0.9894\n",
            "Epoch 00102: val_accuracy improved from 0.92061 to 0.92075, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0248 - accuracy: 0.9894 - val_loss: 0.5324 - val_accuracy: 0.9208\n",
            "Epoch 103/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0239 - accuracy: 0.9900\n",
            "Epoch 00103: val_accuracy did not improve from 0.92075\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.9901 - val_loss: 0.5375 - val_accuracy: 0.9198\n",
            "Epoch 104/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9900\n",
            "Epoch 00104: val_accuracy improved from 0.92075 to 0.92118, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0237 - accuracy: 0.9900 - val_loss: 0.5377 - val_accuracy: 0.9212\n",
            "Epoch 105/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0216 - accuracy: 0.9909\n",
            "Epoch 00105: val_accuracy did not improve from 0.92118\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.9908 - val_loss: 0.5546 - val_accuracy: 0.9191\n",
            "Epoch 106/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9905\n",
            "Epoch 00106: val_accuracy improved from 0.92118 to 0.92316, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0223 - accuracy: 0.9904 - val_loss: 0.5403 - val_accuracy: 0.9232\n",
            "Epoch 107/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9905\n",
            "Epoch 00107: val_accuracy did not improve from 0.92316\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0222 - accuracy: 0.9905 - val_loss: 0.5476 - val_accuracy: 0.9206\n",
            "Epoch 108/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9901\n",
            "Epoch 00108: val_accuracy did not improve from 0.92316\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0236 - accuracy: 0.9902 - val_loss: 0.5473 - val_accuracy: 0.9199\n",
            "Epoch 109/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0217 - accuracy: 0.9911\n",
            "Epoch 00109: val_accuracy did not improve from 0.92316\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0216 - accuracy: 0.9911 - val_loss: 0.5457 - val_accuracy: 0.9202\n",
            "Epoch 110/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0212 - accuracy: 0.9911\n",
            "Epoch 00110: val_accuracy did not improve from 0.92316\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.9911 - val_loss: 0.5555 - val_accuracy: 0.9185\n",
            "Epoch 111/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0219 - accuracy: 0.9910\n",
            "Epoch 00111: val_accuracy did not improve from 0.92316\n",
            "\n",
            "Epoch 00111: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.9911 - val_loss: 0.5482 - val_accuracy: 0.9195\n",
            "Epoch 112/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0153 - accuracy: 0.9938\n",
            "Epoch 00112: val_accuracy improved from 0.92316 to 0.92359, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0151 - accuracy: 0.9939 - val_loss: 0.5563 - val_accuracy: 0.9236\n",
            "Epoch 113/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0141 - accuracy: 0.9941\n",
            "Epoch 00113: val_accuracy improved from 0.92359 to 0.92401, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0140 - accuracy: 0.9942 - val_loss: 0.5646 - val_accuracy: 0.9240\n",
            "Epoch 114/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0154 - accuracy: 0.9936\n",
            "Epoch 00114: val_accuracy did not improve from 0.92401\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0153 - accuracy: 0.9936 - val_loss: 0.5660 - val_accuracy: 0.9236\n",
            "Epoch 115/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9940\n",
            "Epoch 00115: val_accuracy did not improve from 0.92401\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0134 - accuracy: 0.9941 - val_loss: 0.5751 - val_accuracy: 0.9227\n",
            "Epoch 116/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0142 - accuracy: 0.9941\n",
            "Epoch 00116: val_accuracy did not improve from 0.92401\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9942 - val_loss: 0.5788 - val_accuracy: 0.9220\n",
            "Epoch 117/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0147 - accuracy: 0.9939\n",
            "Epoch 00117: val_accuracy did not improve from 0.92401\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0148 - accuracy: 0.9940 - val_loss: 0.5820 - val_accuracy: 0.9226\n",
            "Epoch 118/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.0143 - accuracy: 0.9938\n",
            "Epoch 00118: val_accuracy did not improve from 0.92401\n",
            "\n",
            "Epoch 00118: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0145 - accuracy: 0.9938 - val_loss: 0.5843 - val_accuracy: 0.9237\n",
            "Epoch 119/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0113 - accuracy: 0.9952\n",
            "Epoch 00119: val_accuracy did not improve from 0.92401\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9952 - val_loss: 0.5889 - val_accuracy: 0.9237\n",
            "Epoch 120/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0104 - accuracy: 0.9956\n",
            "Epoch 00120: val_accuracy improved from 0.92401 to 0.92515, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0105 - accuracy: 0.9956 - val_loss: 0.5880 - val_accuracy: 0.9251\n",
            "Epoch 121/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0105 - accuracy: 0.9959\n",
            "Epoch 00121: val_accuracy improved from 0.92515 to 0.92685, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0104 - accuracy: 0.9959 - val_loss: 0.5926 - val_accuracy: 0.9269\n",
            "Epoch 122/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0103 - accuracy: 0.9955\n",
            "Epoch 00122: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9954 - val_loss: 0.5944 - val_accuracy: 0.9257\n",
            "Epoch 123/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0103 - accuracy: 0.9957\n",
            "Epoch 00123: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9956 - val_loss: 0.6013 - val_accuracy: 0.9243\n",
            "Epoch 124/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0104 - accuracy: 0.9955\n",
            "Epoch 00124: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9955 - val_loss: 0.6041 - val_accuracy: 0.9237\n",
            "Epoch 125/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0106 - accuracy: 0.9955\n",
            "Epoch 00125: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.6064 - val_accuracy: 0.9239\n",
            "Epoch 126/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0097 - accuracy: 0.9961\n",
            "Epoch 00126: val_accuracy did not improve from 0.92685\n",
            "\n",
            "Epoch 00126: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0097 - accuracy: 0.9961 - val_loss: 0.6085 - val_accuracy: 0.9233\n",
            "Epoch 127/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0098 - accuracy: 0.9960\n",
            "Epoch 00127: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0096 - accuracy: 0.9960 - val_loss: 0.6082 - val_accuracy: 0.9246\n",
            "Epoch 128/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0088 - accuracy: 0.9963\n",
            "Epoch 00128: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9964 - val_loss: 0.6069 - val_accuracy: 0.9249\n",
            "Epoch 129/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9961\n",
            "Epoch 00129: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0092 - accuracy: 0.9960 - val_loss: 0.6092 - val_accuracy: 0.9253\n",
            "Epoch 130/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0083 - accuracy: 0.9964\n",
            "Epoch 00130: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0083 - accuracy: 0.9964 - val_loss: 0.6089 - val_accuracy: 0.9263\n",
            "Epoch 131/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9963\n",
            "Epoch 00131: val_accuracy did not improve from 0.92685\n",
            "\n",
            "Epoch 00131: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0093 - accuracy: 0.9963 - val_loss: 0.6107 - val_accuracy: 0.9246\n",
            "Epoch 132/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0084 - accuracy: 0.9965\n",
            "Epoch 00132: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0086 - accuracy: 0.9965 - val_loss: 0.6111 - val_accuracy: 0.9243\n",
            "Epoch 133/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0085 - accuracy: 0.9965\n",
            "Epoch 00133: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0087 - accuracy: 0.9963 - val_loss: 0.6122 - val_accuracy: 0.9242\n",
            "Epoch 134/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 00134: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0083 - accuracy: 0.9966 - val_loss: 0.6148 - val_accuracy: 0.9250\n",
            "Epoch 135/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 00135: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 0.6141 - val_accuracy: 0.9251\n",
            "Epoch 136/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0086 - accuracy: 0.9962\n",
            "Epoch 00136: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0086 - accuracy: 0.9963 - val_loss: 0.6146 - val_accuracy: 0.9247\n",
            "Epoch 137/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0089 - accuracy: 0.9964\n",
            "Epoch 00137: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 0.9965 - val_loss: 0.6174 - val_accuracy: 0.9253\n",
            "Epoch 138/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0081 - accuracy: 0.9969\n",
            "Epoch 00138: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.6190 - val_accuracy: 0.9250\n",
            "Epoch 139/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0089 - accuracy: 0.9961\n",
            "Epoch 00139: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9961 - val_loss: 0.6164 - val_accuracy: 0.9249\n",
            "Epoch 140/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9965\n",
            "Epoch 00140: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0081 - accuracy: 0.9965 - val_loss: 0.6176 - val_accuracy: 0.9256\n",
            "Epoch 141/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0087 - accuracy: 0.9964\n",
            "Epoch 00141: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9965 - val_loss: 0.6195 - val_accuracy: 0.9239\n",
            "Epoch 142/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 00142: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9966 - val_loss: 0.6171 - val_accuracy: 0.9249\n",
            "Epoch 143/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0075 - accuracy: 0.9969\n",
            "Epoch 00143: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9968 - val_loss: 0.6191 - val_accuracy: 0.9253\n",
            "Epoch 144/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0081 - accuracy: 0.9969\n",
            "Epoch 00144: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9969 - val_loss: 0.6194 - val_accuracy: 0.9246\n",
            "Epoch 145/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0087 - accuracy: 0.9966\n",
            "Epoch 00145: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9966 - val_loss: 0.6211 - val_accuracy: 0.9257\n",
            "Epoch 146/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0078 - accuracy: 0.9967\n",
            "Epoch 00146: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9967 - val_loss: 0.6206 - val_accuracy: 0.9257\n",
            "Epoch 147/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9968\n",
            "Epoch 00147: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9968 - val_loss: 0.6224 - val_accuracy: 0.9250\n",
            "Epoch 148/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0080 - accuracy: 0.9968\n",
            "Epoch 00148: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0080 - accuracy: 0.9968 - val_loss: 0.6231 - val_accuracy: 0.9254\n",
            "Epoch 149/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0083 - accuracy: 0.9965\n",
            "Epoch 00149: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.6257 - val_accuracy: 0.9256\n",
            "Epoch 150/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0083 - accuracy: 0.9966\n",
            "Epoch 00150: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9966 - val_loss: 0.6263 - val_accuracy: 0.9259\n",
            "Epoch 151/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0081 - accuracy: 0.9967\n",
            "Epoch 00151: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0083 - accuracy: 0.9967 - val_loss: 0.6256 - val_accuracy: 0.9256\n",
            "Epoch 152/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9971\n",
            "Epoch 00152: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.6261 - val_accuracy: 0.9256\n",
            "Epoch 153/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9969\n",
            "Epoch 00153: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9970 - val_loss: 0.6256 - val_accuracy: 0.9257\n",
            "Epoch 154/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0079 - accuracy: 0.9968\n",
            "Epoch 00154: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9968 - val_loss: 0.6289 - val_accuracy: 0.9257\n",
            "Epoch 155/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9967\n",
            "Epoch 00155: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0080 - accuracy: 0.9967 - val_loss: 0.6299 - val_accuracy: 0.9254\n",
            "Epoch 156/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0078 - accuracy: 0.9966\n",
            "Epoch 00156: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0078 - accuracy: 0.9966 - val_loss: 0.6280 - val_accuracy: 0.9251\n",
            "Epoch 157/250\n",
            "113/127 [=========================>....] - ETA: 0s - loss: 0.0080 - accuracy: 0.9967\n",
            "Epoch 00157: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9967 - val_loss: 0.6312 - val_accuracy: 0.9247\n",
            "Epoch 158/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0078 - accuracy: 0.9969\n",
            "Epoch 00158: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0077 - accuracy: 0.9969 - val_loss: 0.6289 - val_accuracy: 0.9247\n",
            "Epoch 159/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9968\n",
            "Epoch 00159: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9967 - val_loss: 0.6301 - val_accuracy: 0.9247\n",
            "Epoch 160/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.0073 - accuracy: 0.9969\n",
            "Epoch 00160: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.6317 - val_accuracy: 0.9247\n",
            "Epoch 161/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0081 - accuracy: 0.9967\n",
            "Epoch 00161: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0080 - accuracy: 0.9967 - val_loss: 0.6302 - val_accuracy: 0.9251\n",
            "Epoch 162/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9967\n",
            "Epoch 00162: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9967 - val_loss: 0.6322 - val_accuracy: 0.9244\n",
            "Epoch 163/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0079 - accuracy: 0.9969\n",
            "Epoch 00163: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0079 - accuracy: 0.9970 - val_loss: 0.6289 - val_accuracy: 0.9251\n",
            "Epoch 164/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9971\n",
            "Epoch 00164: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.6311 - val_accuracy: 0.9251\n",
            "Epoch 165/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0079 - accuracy: 0.9969\n",
            "Epoch 00165: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9969 - val_loss: 0.6315 - val_accuracy: 0.9260\n",
            "Epoch 166/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9968\n",
            "Epoch 00166: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 0.9968 - val_loss: 0.6298 - val_accuracy: 0.9256\n",
            "Epoch 167/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00167: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 0.6322 - val_accuracy: 0.9261\n",
            "Epoch 168/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0075 - accuracy: 0.9966\n",
            "Epoch 00168: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0078 - accuracy: 0.9966 - val_loss: 0.6328 - val_accuracy: 0.9256\n",
            "Epoch 169/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0070 - accuracy: 0.9972\n",
            "Epoch 00169: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9972 - val_loss: 0.6344 - val_accuracy: 0.9254\n",
            "Epoch 170/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0080 - accuracy: 0.9967\n",
            "Epoch 00170: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0078 - accuracy: 0.9968 - val_loss: 0.6329 - val_accuracy: 0.9266\n",
            "Epoch 171/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0079 - accuracy: 0.9967\n",
            "Epoch 00171: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0079 - accuracy: 0.9967 - val_loss: 0.6338 - val_accuracy: 0.9264\n",
            "Epoch 172/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9967\n",
            "Epoch 00172: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0077 - accuracy: 0.9967 - val_loss: 0.6322 - val_accuracy: 0.9259\n",
            "Epoch 173/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0073 - accuracy: 0.9970\n",
            "Epoch 00173: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0074 - accuracy: 0.9970 - val_loss: 0.6362 - val_accuracy: 0.9259\n",
            "Epoch 174/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0074 - accuracy: 0.9971\n",
            "Epoch 00174: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 0.6350 - val_accuracy: 0.9266\n",
            "Epoch 175/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9966\n",
            "Epoch 00175: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9967 - val_loss: 0.6365 - val_accuracy: 0.9250\n",
            "Epoch 176/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9967\n",
            "Epoch 00176: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9967 - val_loss: 0.6383 - val_accuracy: 0.9261\n",
            "Epoch 177/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0069 - accuracy: 0.9971\n",
            "Epoch 00177: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 0.6389 - val_accuracy: 0.9259\n",
            "Epoch 178/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0070 - accuracy: 0.9970\n",
            "Epoch 00178: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 0.6403 - val_accuracy: 0.9247\n",
            "Epoch 179/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9970\n",
            "Epoch 00179: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.6426 - val_accuracy: 0.9253\n",
            "Epoch 180/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00180: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0070 - accuracy: 0.9970 - val_loss: 0.6416 - val_accuracy: 0.9260\n",
            "Epoch 181/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9967\n",
            "Epoch 00181: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0076 - accuracy: 0.9968 - val_loss: 0.6411 - val_accuracy: 0.9253\n",
            "Epoch 182/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0070 - accuracy: 0.9970\n",
            "Epoch 00182: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.6405 - val_accuracy: 0.9257\n",
            "Epoch 183/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9971\n",
            "Epoch 00183: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0072 - accuracy: 0.9970 - val_loss: 0.6428 - val_accuracy: 0.9254\n",
            "Epoch 184/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0076 - accuracy: 0.9968\n",
            "Epoch 00184: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.6432 - val_accuracy: 0.9257\n",
            "Epoch 185/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9967\n",
            "Epoch 00185: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0079 - accuracy: 0.9968 - val_loss: 0.6436 - val_accuracy: 0.9256\n",
            "Epoch 186/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9967\n",
            "Epoch 00186: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 0.6424 - val_accuracy: 0.9259\n",
            "Epoch 187/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9968\n",
            "Epoch 00187: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 0.6423 - val_accuracy: 0.9269\n",
            "Epoch 188/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9971\n",
            "Epoch 00188: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0072 - accuracy: 0.9971 - val_loss: 0.6431 - val_accuracy: 0.9259\n",
            "Epoch 189/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9973\n",
            "Epoch 00189: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0067 - accuracy: 0.9973 - val_loss: 0.6433 - val_accuracy: 0.9266\n",
            "Epoch 190/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0077 - accuracy: 0.9967\n",
            "Epoch 00190: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0076 - accuracy: 0.9967 - val_loss: 0.6439 - val_accuracy: 0.9260\n",
            "Epoch 191/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9969\n",
            "Epoch 00191: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0075 - accuracy: 0.9969 - val_loss: 0.6431 - val_accuracy: 0.9264\n",
            "Epoch 192/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9969\n",
            "Epoch 00192: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0072 - accuracy: 0.9969 - val_loss: 0.6420 - val_accuracy: 0.9257\n",
            "Epoch 193/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0068 - accuracy: 0.9972\n",
            "Epoch 00193: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9971 - val_loss: 0.6454 - val_accuracy: 0.9250\n",
            "Epoch 194/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9971\n",
            "Epoch 00194: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 5ms/step - loss: 0.0069 - accuracy: 0.9971 - val_loss: 0.6448 - val_accuracy: 0.9256\n",
            "Epoch 195/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9970\n",
            "Epoch 00195: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0068 - accuracy: 0.9970 - val_loss: 0.6463 - val_accuracy: 0.9256\n",
            "Epoch 196/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9974\n",
            "Epoch 00196: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.6445 - val_accuracy: 0.9259\n",
            "Epoch 197/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9971\n",
            "Epoch 00197: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9971 - val_loss: 0.6479 - val_accuracy: 0.9257\n",
            "Epoch 198/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9971\n",
            "Epoch 00198: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0072 - accuracy: 0.9971 - val_loss: 0.6474 - val_accuracy: 0.9257\n",
            "Epoch 199/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00199: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 0.6500 - val_accuracy: 0.9254\n",
            "Epoch 200/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.0071 - accuracy: 0.9972\n",
            "Epoch 00200: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.6484 - val_accuracy: 0.9259\n",
            "Epoch 201/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0075 - accuracy: 0.9972\n",
            "Epoch 00201: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 0.6511 - val_accuracy: 0.9263\n",
            "Epoch 202/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9971\n",
            "Epoch 00202: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 0.6508 - val_accuracy: 0.9251\n",
            "Epoch 203/250\n",
            "118/127 [==========================>...] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00203: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 0.6490 - val_accuracy: 0.9259\n",
            "Epoch 204/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9971\n",
            "Epoch 00204: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 0.6499 - val_accuracy: 0.9256\n",
            "Epoch 205/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9971\n",
            "Epoch 00205: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 0.6496 - val_accuracy: 0.9257\n",
            "Epoch 206/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0070 - accuracy: 0.9971\n",
            "Epoch 00206: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.6492 - val_accuracy: 0.9257\n",
            "Epoch 207/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0071 - accuracy: 0.9972\n",
            "Epoch 00207: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 0.6499 - val_accuracy: 0.9260\n",
            "Epoch 208/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00208: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0069 - accuracy: 0.9972 - val_loss: 0.6528 - val_accuracy: 0.9250\n",
            "Epoch 209/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0077 - accuracy: 0.9969\n",
            "Epoch 00209: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0076 - accuracy: 0.9969 - val_loss: 0.6512 - val_accuracy: 0.9242\n",
            "Epoch 210/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9970\n",
            "Epoch 00210: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 0.9969 - val_loss: 0.6532 - val_accuracy: 0.9250\n",
            "Epoch 211/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0065 - accuracy: 0.9972\n",
            "Epoch 00211: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0065 - accuracy: 0.9972 - val_loss: 0.6506 - val_accuracy: 0.9266\n",
            "Epoch 212/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00212: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9971 - val_loss: 0.6534 - val_accuracy: 0.9257\n",
            "Epoch 213/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0068 - accuracy: 0.9972\n",
            "Epoch 00213: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0067 - accuracy: 0.9972 - val_loss: 0.6519 - val_accuracy: 0.9267\n",
            "Epoch 214/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0072 - accuracy: 0.9972\n",
            "Epoch 00214: val_accuracy did not improve from 0.92685\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9972 - val_loss: 0.6531 - val_accuracy: 0.9260\n",
            "Epoch 215/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0068 - accuracy: 0.9974\n",
            "Epoch 00215: val_accuracy improved from 0.92685 to 0.92713, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0069 - accuracy: 0.9974 - val_loss: 0.6513 - val_accuracy: 0.9271\n",
            "Epoch 216/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0066 - accuracy: 0.9971\n",
            "Epoch 00216: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.6534 - val_accuracy: 0.9261\n",
            "Epoch 217/250\n",
            "115/127 [==========================>...] - ETA: 0s - loss: 0.0072 - accuracy: 0.9972\n",
            "Epoch 00217: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9971 - val_loss: 0.6535 - val_accuracy: 0.9259\n",
            "Epoch 218/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0069 - accuracy: 0.9972\n",
            "Epoch 00218: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0072 - accuracy: 0.9971 - val_loss: 0.6516 - val_accuracy: 0.9260\n",
            "Epoch 219/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0066 - accuracy: 0.9973\n",
            "Epoch 00219: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9974 - val_loss: 0.6562 - val_accuracy: 0.9244\n",
            "Epoch 220/250\n",
            "127/127 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9972\n",
            "Epoch 00220: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0064 - accuracy: 0.9972 - val_loss: 0.6565 - val_accuracy: 0.9254\n",
            "Epoch 221/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0072 - accuracy: 0.9969\n",
            "Epoch 00221: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.6567 - val_accuracy: 0.9256\n",
            "Epoch 222/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9969\n",
            "Epoch 00222: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 0.6564 - val_accuracy: 0.9267\n",
            "Epoch 223/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0061 - accuracy: 0.9975\n",
            "Epoch 00223: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0063 - accuracy: 0.9974 - val_loss: 0.6554 - val_accuracy: 0.9257\n",
            "Epoch 224/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0070 - accuracy: 0.9972\n",
            "Epoch 00224: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9972 - val_loss: 0.6553 - val_accuracy: 0.9264\n",
            "Epoch 225/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0074 - accuracy: 0.9970\n",
            "Epoch 00225: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 0.6549 - val_accuracy: 0.9259\n",
            "Epoch 226/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0064 - accuracy: 0.9973\n",
            "Epoch 00226: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9973 - val_loss: 0.6547 - val_accuracy: 0.9260\n",
            "Epoch 227/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.0070 - accuracy: 0.9972\n",
            "Epoch 00227: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0071 - accuracy: 0.9971 - val_loss: 0.6534 - val_accuracy: 0.9260\n",
            "Epoch 228/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0070 - accuracy: 0.9970\n",
            "Epoch 00228: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9971 - val_loss: 0.6549 - val_accuracy: 0.9263\n",
            "Epoch 229/250\n",
            "114/127 [=========================>....] - ETA: 0s - loss: 0.0070 - accuracy: 0.9970\n",
            "Epoch 00229: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9969 - val_loss: 0.6540 - val_accuracy: 0.9260\n",
            "Epoch 230/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9969\n",
            "Epoch 00230: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.6559 - val_accuracy: 0.9266\n",
            "Epoch 231/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0068 - accuracy: 0.9973\n",
            "Epoch 00231: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.6538 - val_accuracy: 0.9264\n",
            "Epoch 232/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0068 - accuracy: 0.9972\n",
            "Epoch 00232: val_accuracy did not improve from 0.92713\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9973 - val_loss: 0.6551 - val_accuracy: 0.9269\n",
            "Epoch 233/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0072 - accuracy: 0.9971\n",
            "Epoch 00233: val_accuracy improved from 0.92713 to 0.92742, saving model to best_mlp_model.h5\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9971 - val_loss: 0.6548 - val_accuracy: 0.9274\n",
            "Epoch 234/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0062 - accuracy: 0.9974\n",
            "Epoch 00234: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.9974 - val_loss: 0.6550 - val_accuracy: 0.9264\n",
            "Epoch 235/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9970\n",
            "Epoch 00235: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9971 - val_loss: 0.6555 - val_accuracy: 0.9263\n",
            "Epoch 236/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 0.9975\n",
            "Epoch 00236: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0063 - accuracy: 0.9975 - val_loss: 0.6560 - val_accuracy: 0.9260\n",
            "Epoch 237/250\n",
            "117/127 [==========================>...] - ETA: 0s - loss: 0.0066 - accuracy: 0.9972\n",
            "Epoch 00237: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9972 - val_loss: 0.6580 - val_accuracy: 0.9259\n",
            "Epoch 238/250\n",
            "125/127 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 0.9976\n",
            "Epoch 00238: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0064 - accuracy: 0.9976 - val_loss: 0.6581 - val_accuracy: 0.9259\n",
            "Epoch 239/250\n",
            "126/127 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9974\n",
            "Epoch 00239: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0063 - accuracy: 0.9974 - val_loss: 0.6594 - val_accuracy: 0.9261\n",
            "Epoch 240/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0066 - accuracy: 0.9972\n",
            "Epoch 00240: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9972 - val_loss: 0.6569 - val_accuracy: 0.9273\n",
            "Epoch 241/250\n",
            "122/127 [===========================>..] - ETA: 0s - loss: 0.0063 - accuracy: 0.9974\n",
            "Epoch 00241: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0062 - accuracy: 0.9975 - val_loss: 0.6640 - val_accuracy: 0.9257\n",
            "Epoch 242/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9972\n",
            "Epoch 00242: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9972 - val_loss: 0.6601 - val_accuracy: 0.9267\n",
            "Epoch 243/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0066 - accuracy: 0.9973\n",
            "Epoch 00243: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9974 - val_loss: 0.6607 - val_accuracy: 0.9263\n",
            "Epoch 244/250\n",
            "116/127 [==========================>...] - ETA: 0s - loss: 0.0062 - accuracy: 0.9973\n",
            "Epoch 00244: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0064 - accuracy: 0.9973 - val_loss: 0.6610 - val_accuracy: 0.9260\n",
            "Epoch 245/250\n",
            "124/127 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9972\n",
            "Epoch 00245: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 0.6599 - val_accuracy: 0.9257\n",
            "Epoch 246/250\n",
            "121/127 [===========================>..] - ETA: 0s - loss: 0.0070 - accuracy: 0.9970\n",
            "Epoch 00246: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0069 - accuracy: 0.9971 - val_loss: 0.6599 - val_accuracy: 0.9263\n",
            "Epoch 247/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9969\n",
            "Epoch 00247: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0070 - accuracy: 0.9969 - val_loss: 0.6586 - val_accuracy: 0.9267\n",
            "Epoch 248/250\n",
            "120/127 [===========================>..] - ETA: 0s - loss: 0.0071 - accuracy: 0.9970\n",
            "Epoch 00248: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9970 - val_loss: 0.6604 - val_accuracy: 0.9256\n",
            "Epoch 249/250\n",
            "123/127 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9974\n",
            "Epoch 00249: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0059 - accuracy: 0.9975 - val_loss: 0.6604 - val_accuracy: 0.9263\n",
            "Epoch 250/250\n",
            "119/127 [===========================>..] - ETA: 0s - loss: 0.0065 - accuracy: 0.9973\n",
            "Epoch 00250: val_accuracy did not improve from 0.92742\n",
            "127/127 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9972 - val_loss: 0.6598 - val_accuracy: 0.9269\n",
            "245/245 [==============================] - 0s 2ms/step - loss: 0.5887 - accuracy: 0.9287\n",
            "Accuracy(on Test-data): 0.9286807775497437\n",
            "time of training (s) 136.51943469047546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YCBSQ3C5Gvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def viz(history):\n",
        "    # Plot training & validation accuracy values\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "    plt.show()\n",
        " \n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Valid'], loc='upper right')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssjikAsU5KEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "78a24d98-d35e-45a2-8f6a-f8eb9e6043dd"
      },
      "source": [
        "viz(fit1)\n",
        "#fit1.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5dn48e89SyZ7IDskQMK+yB5RwQ1UVLSiVhTUikv11be2Wl+12sXa1r5va+2irb9W3HCppe6i4lJ3FBeCArITIEACYQmQfZnl+f3xTCCEBALJZJLM/bmuXDNnmTP3ceTc51nO84gxBqWUUpHLEe4AlFJKhZcmAqWUinCaCJRSKsJpIlBKqQiniUAppSKcJgKllIpwmgiUagURyRERIyKuVux7tYh82tbjKNVRNBGobkdECkWkXkRSm6z/JngRzglPZEp1TpoIVHe1CZjVsCAiI4HY8IWjVOeliUB1V88AVzVang083XgHEUkSkadFZJeIbBaRn4uII7jNKSIPiMhuEdkInNfMZx8Xke0iUiwi94mI82iDFJHeIjJfRPaISIGIXN9o2wQRyReRchHZISJ/Cq6PFpFnRaRURPaJyGIRyTja71aqgSYC1V19ASSKyLDgBXom8GyTff4KJAH9gdOwieOa4LbrgfOBsUAecEmTz84FfMDA4D5Tge8fQ5zzgCKgd/A7/ldEpgS3PQg8aIxJBAYAzwfXzw7G3QdIAW4Eao7hu5UCNBGo7q2hVHAWsBoobtjQKDncbYypMMYUAn8Evhfc5VLgL8aYrcaYPcD/NfpsBjANuNUYU2WM2Qn8OXi8VhORPsAk4CfGmFpjzFLgMQ6UZLzAQBFJNcZUGmO+aLQ+BRhojPEbY5YYY8qP5ruVakwTgerOngEuB66mSbUQkAq4gc2N1m0GsoLvewNbm2xr0C/42e3Bqpl9wCNA+lHG1xvYY4ypaCGG64DBwJpg9c/5jc7rHWCeiGwTkftFxH2U363UfpoIVLdljNmMbTSeBrzcZPNu7J11v0br+nKg1LAdW/XSeFuDrUAdkGqM6RH8SzTGjDjKELcBySKS0FwMxpj1xphZ2ATze+BFEYkzxniNMb8yxgwHJmKrsK5CqWOkiUB1d9cBU4wxVY1XGmP82Dr334pIgoj0A27jQDvC88CPRCRbRHoCdzX67HbgXeCPIpIoIg4RGSAipx1NYMaYrcAi4P+CDcCjgvE+CyAiV4pImjEmAOwLfiwgIpNFZGSweqscm9ACR/PdSjWmiUB1a8aYDcaY/BY2/xCoAjYCnwLPAU8Etz2KrX5ZBnzNoSWKq4AoYBWwF3gR6HUMIc4CcrClg1eAXxpj3gtuOwdYKSKV2IbjmcaYGiAz+H3l2LaPj7HVRUodE9GJaZRSKrJpiUAppSKcJgKllIpwmgiUUirCaSJQSqkI1+WGwk1NTTU5OTnhDkMppbqUJUuW7DbGpDW3rcslgpycHPLzW+oNqJRSqjkisrmlbVo1pJRSEU4TgVJKRThNBEopFeG6XBuBUkodLa/XS1FREbW1teEOJeSio6PJzs7G7W79gLSaCJRS3V5RUREJCQnk5OQgIuEOJ2SMMZSWllJUVERubm6rP6dVQ0qpbq+2tpaUlJRunQQARISUlJSjLvloIlBKRYTungQaHMt5atWQUkp1Zn4v1FWAvw48SRAV2+5foYlAKaVCqLS0lDPOOAOAkpISnE4naakpIA6+en8+Uf4qMAFAwPjBFW2XA37yv1nO0y+8xkO/udMezOHSRKCUUp2Srw68NeCOsRdrEfDWgq+GFEclSxfMhag47r3/IeKjXdx+41UgTqgrxeeIxhUVBxgQhz2WOMEdRd6JE8mbdDpEJ9oEIaGpzddEoJRSDQI+qNlr3xsDGFsd44yC6l3gq4eA11bXRMXZ/f0+qK+0+zZLIKYneKvtMaMSuPqO3xPtgm9WFTDplFOZOXMmt9xyC7W1tcTExPDkk08yZMhAPvroIx544AHeeOMN7r33XrZs2cLGjRvZsmULt956Kz/60Y/a5bQ1ESilurfKnRAI2Hr2gJ9fvb2RVdvtezvVc7BxteHCf1gS3N1hq3EQEGF4Zjy//M4I8NXaah0TsMkjKhYcUeAI3snHpUJ0PLijKdq9m0VffInT6aS8vJyFCxficrl47733+OlPf8pLL710yLevWbOGDz/8kIqKCoYMGcJNN910VM8LtEQTgVKqe/LVw/u/gs//Bmc/D6X1dn1dub2jF4etggH2V8uI40DVTnA1xmcv7A5Xo/2b8MQf+GulGTNm4HTa45WVlTF79mzWr1+PiOD1epv9zHnnnYfH48Hj8ZCens6OHTvIzs5u9Xe2RBOBUqrzC/jtRbq6FFa/DpU7IPt4e3Heu8lW0WSOttU65UVQVgTfvgj7NsO42RDTA3rmgjOKX16MvVt3hvfyFxcXt//9L37xCyZPnswrr7xCYWEhp59+erOf8Xg8+987nU58Pl+7xKKJQCkVPvXV9iIenQjl22HnKnvHHp8BVbvhm2egrhK2L7X17HWVUFd25OOKE3qPgfP+BIPOhNWrbTLopMrKysjKygJg7ty5Hf79mgiUUu2rfDuUb7MX4upSezdfvRtKVsCeDXZ74SfgirF37AEf9MyB0oJDj9WjHyT2hjFXQMV2cDjh1Dvs3f2WL+xdfc9cwMCOlRCXBolZkJBp9+0i7rzzTmbPns19993Heeed1+HfL8YcqXGkDQcXOQd4EHACjxljftfMPpcC92Jr45YZYy4/3DHz8vKMTkyjVCex7Rt7gU/sDRkjoOA9eOsuqK+w1TaBJlUX4oDoJMg52TbOJmaBKwp2rIL+p0PWOIhJttU9vjoYdkG7VOGsXr2aYcOGtfk4XUVz5ysiS4wxec3tH7ISgYg4gYeBs4AiYLGIzDfGrGq0zyDgbmCSMWaviKSHKp51Oyr4ctMevndiv1B9hVLdX10FVOyAjR/aapvtyw7dJysP8q6BnauhR1+bEGKTIXUwpA1t3Z16xvD2j121KJRVQxOAAmPMRgARmQdMB1Y12ud64GFjzF4AY8zOUAXzybpd3PfmaqYdl0lKvOfIH1AqUgX8tmrHFW2HNVj8OGz9EnatsVU9DTKOg2kPwIAptnG2ZDlkjrJ3+12oWkaFNhFkAVsbLRcBJzTZZzCAiHyGrT661xjzdtMDicgNwA0Affv2PaZghvVKBGD19gpOHqSJQEW4QAD89VC4EIqX2Iv6krmQlGWrexrf6YvD9tAZej4k94fYFOhzAqQOOtDNMmUA9D8tLKei2i7cjcUuYBBwOpANfCIiI40x+xrvZIyZA8wB20ZwLF90IBGUc/Kg1DaErFQXUVsGlbugRx/bG2fTJ8EulG54607Yt+Xg/WNT7T7RiXD2/9n9/F4YfLa90KtuK5SJoBjo02g5O7iusSLgS2OMF9gkIuuwiWFxeweTHBdFRqKH1dvL2/vQSnUO9VV2vJsNH8JXj0BR8J9RbKrtX2/8B/btmQNTfm575eScAkVfQf/J9g7f4QZ3dFhOQYVHKBPBYmCQiORiE8BMoGmPoFeBWcCTIpKKrSraGJJo9m7m7phXeXzbRfD23TByhu2hoFRXYIwdKmHfZtv4WrLc1uM3DItQvQeWzQNvld0/bSic/lNb1bPuHTu0wal32Aexyooh9xTbe6fB8OlhOS3VOYQsERhjfCJyM/AOtv7/CWPMShH5NZBvjJkf3DZVRFYBfuAOY0xpy0dtg5Uvc2HZM4wx/4EvSmzXNE0EqjPye20VTUN1zr6t9qLf4oNUYpPD8Asge4K9+A8578D4NmOvPLBrYm/oPTbkp6AONnnyZO666y7OPvvs/ev+8pe/sHbtWv7+978fsv/pp5/OAw88QF5eHtOmTeO5556jR4+DH4i79957iY+P5/bbb29zfCFtIzDGLAAWNFl3T6P3Brgt+Bdak26lYNMmBm54ioDTg6O5bm9KhUNDl8zKEti2FJY8eeDhqpSBkD4McibZ7pc9c2wjb+qQgxtrVac2a9Ys5s2bd1AimDdvHvfff/8RP7tgwYIj7tNW4W4s7jgiJE7/Pec9MJpbEj/jrB1vIn5f2McbUREmEICVL9u7/T2bbA+dvZsO3idjJMx4CvpNhPiQPVqjOtAll1zCz3/+c+rr64mKiqKwsJBt27bxr3/9i9tuu42amhouueQSfvWrXx3y2ZycHPLz80lNTeW3v/0tTz31FOnp6fTp04fx48e3S3wRdRVMT4zhwjMn8+bbBUyNqoXd6/TBFdVxti6Gt+6wF39xHqimGXulffAqLg3Shtj1KnTeugtKvm3fY2aOhHMPGThhv+TkZCZMmMBbb73F9OnTmTdvHpdeeik//elPSU5Oxu/3c8YZZ7B8+XJGjRrV7DGWLFnCvHnzWLp0KT6fj3HjxmkiOFbXTMrhv/NHQjnUbPmaGE0EKlS2fGmrbvZssk/hFi6E+Ey4+FE47pIDdfgqIjRUDzUkgscff5znn3+eOXPm4PP52L59O6tWrWoxESxcuJCLLrqI2Fg7VeUFF1zQbrFFXCJwOR3cdPFUqp/0sCL/YyYcf+WRP6TU0Sr8FOaez/6JTnrmwpRfwAn/BZ6EsIYW8Q5z5x5K06dP58c//jFff/011dXVJCcn88ADD7B48WJ69uzJ1VdfTW1tbVhii8hbkrE5qWyNHUbmjk8I+P1H/oBSR2N3AbxyEyTnwmXPwjVvwY++gVNv1yQQweLj45k8eTLXXnsts2bNory8nLi4OJKSktixYwdvvfXWYT9/6qmn8uqrr1JTU0NFRQWvv/56u8UWcSWCBvuGX8mQJbez6fOXyD350nCHo7qLb56F12+xQyxf9SpkNzvYo4pQs2bN4qKLLmLevHkMHTqUsWPHMnToUPr06cOkSZMO+9lx48Zx2WWXMXr0aNLT0zn++OPbLa6QDkMdCu01DPWeimqqHxgJiVlk3/axdsNTbbfmTfj3lZB7Glw8R3v8dCI6DPXhh6GOyKohgOSEWN5OnEF2xTI7hrpSbbFqPjw/2/YCuuxZTQKqS4nYRADgH3s1mwIZeN/+mX2aU6ljseULePFamwS+98pRTWCuVGcQ0Ylg8ogs/td3Be7StfDqTfZhH6WOpGQFrHjZDv+Q/wTMu9yO8HnF8weP36M6la5WDX6sjuU8I7axGGBQejxrepzCi1H7uOTbJ+xUeWO1O6k6jIodMPc8qG00UnrvcbZNIKZn+OJShxUdHU1paSkpKSlIN24PNMZQWlpKdPTRjR4b0YlARDhjaAY/++osLk77EMe3L2oiUAdruLsSAb8P3rjVDvV85Uvg9NhqoF5jtLNBJ5ednU1RURG7du0KdyghFx0dTXZ29lF9JqITAcCZwzKYu6iQwvSz6L/ucagqhbiUcIelOgNj4IWrYfNnMOIiOzRE0WI4+39h4Jnhjk4dBbfbTW5ubrjD6LQiuo0A4IT+yfSMdfNSXZ6duGPNG+EOSXUWy+bBqlchLt1O41i+zQ4PcdIPwh2ZUu0q4ksEbqeDqcMzeerbALf36IOsewfGzw53WCrc/D74zy+gz4lwzQKdjF11axFfIgA4d2QmlXV+ipNPgM2fQkCHnYh4mz6Gql0w8YeaBFS3p4kAmDQwlbQED29UDLITfpcsD3dIKtxWvAyeRG0LUBFBEwG2emjWhL48URxsaf/qMfjmn+ENSoVPWTGseR2GnqeTuKuIoIkg6PIJfSmVZHZH58DSZ+G1/4aKknCHpTpa8dfw94m2jeD468MdjVIdQhNBUGZSNBMHpDCHiw5UB5RuCG9QqmPVVdihIjwJcONCyG6f2Z+U6uw0ETQyeUg6c/YdT/HE39gVezQRRJSPfgf7NtunhFMGhDsapTqMJoJGzhhmR4z8T7EbHG4tEUSS+ir4+hkYfqGdNF6pCBLSRCAi54jIWhEpEJG7mtl+tYjsEpGlwb/vhzKeI+mXEkf/tDjeX7cHeuZoiaC7q9kLH/wWqvfA8uehrsxOJalUhAnZA2Ui4gQeBs4CioDFIjLfGLOqya7/NsbcHKo4jtZZwzJ4/NNNeIfl4i7dGO5wVHsxBta9A1njIT7Nrnvzf2DFSzYhbPgAMkdCnxPCG6dSYRDKJ4snAAXGmI0AIjIPmA40TQSdyrkje/HIJxspNJkM2rPQDk3t0Bq0Lm/jR/CvyyAmGcZcDhXbbRKIS4fFj9p9rnhRB49TESmUV7gsYGuj5aLguqa+KyLLReRFEenT3IFE5AYRyReR/FCPHjg6O4neSdF8WdYDfDVQsS2k36c6yOcPQ1wapA+Hrx6Fgvdhwg0wez6IA4ZMg0FnhTtKpcIi3GMNvQ78yxhTJyL/BTwFTGm6kzFmDjAH7JzFoQxIRDj7uEz+81UCVzqBHasg6eiGdFWdzM7VUPAfmPwzOO3OAxMQNZT0bvgYkvuHLz6lwiyUJYJioPEdfnZw3X7GmFJjTF1w8TGgU3TcnpCTzBfeQfjd8bDylXCHo9rqP/fY4SLyrrPLDsfB1X29Run0kiqihbJEsBgYJCK52AQwE7i88Q4i0ssYsz24eAGwOoTxtNqwXonUEcXmjDPpv/p1e9FIzILJPw13aOporf8PrH8Xpt6n80wo1YKQJQJjjE9EbgbeAZzAE8aYlSLyayDfGDMf+JGIXAD4gD3A1aGK52j0TY4lLsrJwugp9K9/Fb551m7IHAXDzg9vcKr1AgF4717omQsTtFuoUi0JaRuBMWYBsKDJunsavb8buDuUMRwLh0MYkpnAgsp4Zo+5wj5g9OUj8MJsGHyOndt4/NXgdIc5UnVYK1+GHSvg4sfAFRXuaJTqtMLdWNxpDeuVyPxl2zD/9bCd7HrgmbDor7DyVTuLma/WjlWvOidj4JM/2F5Cx3033NEo1alpB/kWDO2VSEWtj+J9NXZFQiac/Vu4bSX0Gg2rXrODlFXuDG+gqnkbPoBda2Dij/Q5EKWOQP+FtGBUVhIAX23ac+jGYd+xk5j/4xR49Azwezs4OnWQ/Cdh86IDy746W3qLS4fjLg5fXEp1EZoIWjAyyz5Y9uby7YduHPod+7p3E5RtsaUDFR7Ve+xQEe/+wi7vWgcPjoaNH8JJ/w0uT3jjU6oL0ETQAodDOG9ULz5Zv4uy6iZ3/GlDbJvBqXdAykB79+mrg7nn63MHHcFXbxMAwJo3wfihOB/2bIRXb7LtN1e+DJNuDW+cSnURmggO4/xRvfH6DW+vbFIqEIErX4IpP4eTbobtS+0FqHCh7a7o94Ul3ojx0f/BH4fYKqGVr0Bs8PmAZ79rE8K5f4CBZ+i4QUq1kiaCwxiVncTA9Hie+2pryzuNuQJ69LMDmHkSYW8hrHq1w2KMSAXvQcAHb9wKG96H0bPsqKJ7NsLJt8HIS8IdoVJdiiaCwxARrjqpH8u27mPp1n3N7+SKsiUDgHPvt1VFS+Z2WIwRp7bcPhtwyv/AzOdsF94Tb4IL/wGzX4czf6klAaWOkiaCI7h4XDbxHhdPf17Y8k4jZ8D1H8LomTDwLCjKt/XYkcgYeP1W2Px5aI6/9SswAcg5GYaeZ4eOSMqGtMGQe2povlOpbk4TwRHEe1x8d1wWbyzbTmllXfM7iUDWOPva7yQ7fPX2ZVCxA968HXat7digw6lmLyx50laVNfDVwUvX2x49xyoQgG1L7cN84oTs49seq1IK0ETQKt87qR/1/gDzFh+mraBB35Ps6zdPw6OT7aQnz88Gbw1UlcLqN0IbbLhVBBvWG0/zuW0pfPu8rc8/FpU74aExMOc0m2R6j4GouLbHqpQCNBG0ysD0BCYNTOGfX2zG5w8cfuf4dEgeAF8/be+Ez/5f2LUaPr4fPvgN/PsK2NeKhNJVlQcn8tnTaJrPHSvsa9XuYztmwXuwb7PtDTT9/8F3HmpbjEqpg2giaKWrTsphW1kt761uxZASOZNs9cWMuXDSD2DExbD4MVjxst2++TP7umfTgQtnd9FwPvu2HGgn2Z8IjnF2ueIlEJUAx18HY6+AzOPaHqdSaj9NBK10xtB0snrE8MwXha3Y+Zdw/fuQe4pdnvhDqCuHujJAbCLYuxkeOQ2eugAC/lCG3rEaqoZMwN7FA5QEE0F16bEds/hrWx3kcLY9PqXUITQRtJLL6eCKE/vyWUEpBTsrDr9zXCr0HntgOWsc5Jxip0McNBU2fQIvXgv1lVC6HlbPD23wHam80SR0ezbaRt6dq+xyQ4mgdAN89qDtYdSUrw5euNpOL9mwXPLtwf89lVLtShPBUbgsrw9RLgdPf775GD78LFz7ju32uLfQVndc8gSkDIKFfzp0//pq2PZNm2MOiZq9dojn5koy5dshKThDaekG2FdoE544bBtBwA8vXWenjyz++tDPl6ywTws39DrasQICXvvAmFIqJDQRHIWUeA/nj+rFS0uKqKg9yhFHY3rYhuTB50BcGkz/G4y4EMZeCSXLobJJ/fkX/8+ObFrdzOin4bZqPnxwn427qYrtkDkSPEm2RNBwse89ziaCxY8fSHCrmhmXaXewq+22pfa1KN++Zo1r33NQSu2nieAozT4ph6p6Py9/XXzknZuTNhhuX28TAECfE+xr0VcH77f1KzuY2q41xx5sqDTU/Tc3F0N5MST0sue55XNY/m+7PPBM20by7Qs2UQyaCitfO7R6qOGZi23f2G2rX4fUwQdKGUqpdqeJ4CiN7tOD0X168PTnhZjm6rhbo/EQCL3HgMNtL/wNjLGDp8GBuvLDKXgfnjjHVtl0hH1b7GvljoPXe2tsDIm9Ie86W62z/l0YczkkZNh9tn0DvcbAiIvsEN5Fiw8+xu7gQ2fVu21pYvNndl8dNkKpkNFEcAxmn9SPDbuqWLThGHvBNOaOgV6j7AXRGHj7blj00IEeNq15KnnVa/buu2FM/lBrKRE09BhK7A2jLoXUIXZ5zBUQm2rfB7yQNtRO7hMVD/lPwMs32DYDsCWgpL72/fv32t5Hwy8M6ekoFek0ERyDaSN7kRIXxVOLCtvngNkT7N3v5w/btoGGi2JUgn0Y7Ui2L7WNsd8803wDbHvb26hq6LmZsOhvdrmhm2jPHNvV88K/24fAUgbYdpEG6UPBk2DHZlr2L1t99NWjtj1kb6FtO3G4bO+qzFGQPiz056RUBNNEcAyi3U5mTujDe6t3ULS3uu0HHHSWHZ/o3Z/Z+nNxgCsahk47conAVwc7VsH4q8EZdaC3zc7V7T9JzsaPYNNCqCyxy2VFsP6dA9/z7Qt2esjsCXY5ezyccIN9H5d64DhpQ+3r8d+3rwm9wFsNX/zdlgB6jYZT74RTboerXtNqIaVCLKSJQETOEZG1IlIgIncdZr/viogRkbxQxtOeLj+hHwD//HJL2w828Az4/vv2wjdrnh1Tf9RlkHGcrX6pKLF321XNVEXtWGmrW3JPhf6n28bVPZvsbGkv39C6SXIqd9rurM0pXgKPT7V3/f+cAfMuP7CtaLG9cJcst8dY9zYc911wug49TkMiiEqAxCz7Pn0YzHgKrnsX4jNslRhAxgg4/Sdwxi8gNvnI8Sul2qSZf7HtQ0ScwMPAWUARsFhE5htjVjXZLwG4BfgyVLGEQlaPGM4ekcmzn2/mupNzSY1v49y42Xn2D+wFEGD9f+zrH4N17Zmj4IaPwRHM31sXw+rgfMm9x0JdhW2cfexM29gKtkE2ub99X7rBPuHc9OGsj++3VTR3bT1wbLBdOJ+5yM4BsPVLW1KpK7fbEnodaBPw18P7v7avoy5t/vyie9jqnrQhB9/hjwjW/4+5Ar78B0x7QKuClOpgoSwRTAAKjDEbjTH1wDxgejP7/Qb4PVAbwlhC4n+mDqHG6+eP77ZheOXDyT0Nzrz3wJSYJcsPPIW86jV4YqqdLzm6h50lbcg0Wz0UFWsnyQF78W/w5m32ieamdq2xD301fiq4ZAU8c6F9HuAHX9rSyhUv2os5HDoM9DfP2ETV0hPAIrbtoKXtU34Od2yACdcf4T+KUqq9haxEAGQBjYfZLAJOaLyDiIwD+hhj3hSRO0IYS0gMTI/neyf146lFhfz4zEGkJ0a37xe4ouDkH9v3Ab8tIXzwG3s3/tL37cV40Fm2Xl7EVr/c+BkkZNq787fuhNICu0/Abx/O8lbbdgVXoxJMw0ihpQXQo4/tvTT/ZnB6YPZ8SM49UErpfzps/Nj2dFo93yYhV7RtN5h0y+Hr869e0PLw0Q6nTWBKqQ4XtsZiEXEAfwL+pxX73iAi+SKSv2vXMY5gGSKXT+hLwMA7K0tC+0UOJ5z7e3uHP/c8iOlp2xNOvQPGzz6wX9pgiE60E7pHJ8Hu9Xb9ztX2rt8EbKPvn4+zr/XVB0oCpQX2dcvntr//6T+xSaCxqffZ3kAJve1yygA7uF5y/yN380zIAE98W/9LKKXaWSgTQTHQ+HHQ7OC6BgnAccBHIlIInAjMb67B2BgzxxiTZ4zJS0tLa7o5rAamx9M/LY63VoQ4EQAMmGwv/AGfHZP/cA2pInb+5IaLe+Mnl796FMq22lnDtjSaUrJ0A9RXwUe/g5hkGDXz0OOmD4NRM2zjLti5F77zIFz/QfONxEqpTi+U/3IXA4NEJBebAGYC+7ucGGPKgP19CkXkI+B2Y0x+CGNqdyLCucdl8o+PN7Knqp7kuKjQfuHkn9px+RMyj7xvykDbk+cvo2x1jyfRNvZu+MD23qndZ+cXBlu9U7wE/nGyrSo65/eHr6qJT7evyf2D1T06Y5hSXVXISgTGGB9wM/AOsBp43hizUkR+LSIXhOp7w+G8kb3xBwyvfnOM4w8dDZHWJQGwiaC2zPb3L9sC/SbZB7uM386tPPR8ux5s99Oir2wSuOIlOPHGwx87Odc+D9D/9LacjVKqEwhpWd4YswBY0GTdPS3se3ooYwml4b0TGde3B898sZmrJ+bgcHSSB6B6j7VdPmfNs3MB9B4Db95u32eNtyOCrnzZVvP0GmO7ng6YAoPOPPKxPQm2N5FSqsvTJ4vbyeyJOWzaXcUn6ztRY/bAM+HOjTB4qp3iMWOEbdwFyMqzF/24dDsnQsZwu37SLeGLVykVFtq6107OPa4Xv0taw5/fW89pg9OQzjAsgojtXdRY5ijbLTRrnG3cnfUv2z6QPpXcRmYAABsgSURBVAz+6xM7vINSKqJoiaCdRLkc3HbWYJZt3ccby7eHO5yW5V0DNy8+0OMoO89OBu9wahJQKkJpImhHF4/LZkhGAg9/WHDscxWEmtMNPfuFOwqlVCeiiaAdOR3ClSf1Y01JBSu3lYc7HKWUahVNBO3sgtG98bgc/Hvx1iPvrJRSnYAmgnaWFOPmnOMyeW1pMbVef7jDUUqpI9JEEAKX5vWhvNYX+vGHlFKqHWgiCIGT+qeQ3TOGF/KLwh2KUkodkSaCEHA4hBnj+/BpwW627mmHqSyVUiqENBGEyHfHZyECL3/dAeMPKaVUG2giCJHsnrFMHJDCi19vJRDopM8UKKUUrUwEIhIXnEgGERksIheIiDu0oXV9M8b3YeueGhZtaGbSeaWU6iRaWyL4BIgWkSzgXeB7wNxQBdVdnHNcJr2SornvzVV4/YFwh6OUUs1qbSIQY0w1cDHw/4wxM4ARoQure4h2O7n3ghGsKangqUWF4Q5HKaWa1epEICInAVcAbwbXOUMTUvcydXgGJw9M5R8fb9AHzJRSnVJrE8GtwN3AK8FZxvoDH4YurO5DRPjhlIHsrqxn3ldbwh2OUkodolXzERhjPgY+Bgg2Gu82xvwolIF1Jyf0T2FCbjL3v7OWjMRozh3ZK9whKaXUfq3tNfSciCSKSBywAlglIneENrTu5a+zxjI4I4Ef/usbdlXUhTscpZTar7VVQ8ONMeXAhcBbQC6255BqpYzEaH570XH4AoaP1u4MdzhKKbVfaxOBO/jcwIXAfGOMF9CnpI7S8F6JZCR6+FATgVKqE2ltIngEKATigE9EpB+gM68cJRFh8pB0Fq7brc8VKKU6jVYlAmPMQ8aYLGPMNGNtBiaHOLZuafLQdCrqfHyyble4Q1FKKaD1jcVJIvInEckP/v0RWzo40ufOEZG1IlIgInc1s/1GEflWRJaKyKciMvwYzqFLmTwkneyeMfz5vXU6BpFSqlNobdXQE0AFcGnwrxx48nAfEBEn8DBwLjAcmNXMhf45Y8xIY8wY4H7gT0cRe5cU5XLw4zMHs6K4nFeX6sikSqnwa20iGGCM+aUxZmPw71dA/yN8ZgJQENy/HpgHTG+8Q7AnUoM4IqQB+sKxWYzt24OfvvIt3xaVhTscpVSEa20iqBGRkxsWRGQSUHOEz2QBjWdwLwquO4iI/EBENmBLBM0+pCYiNzRUS+3a1fXr1p0OYc738kiJ8/Cjed9Q79OGY6VU+LQ2EdwIPCwihSJSCPwN+K/2CMAY87AxZgDwE+DnLewzxxiTZ4zJS0tLa4+vDbu0BA/3XXgcm3ZX6YB0Sqmwam2voWXGmNHAKGCUMWYsMOUIHysG+jRazg6ua8k87HMKEWPy0HQmD0njrx+s1wHplFJhc1QzlBljyhvV6992hN0XA4NEJFdEooCZwPzGO4jIoEaL5wHrjyae7uD6U/tTXuvjnZUl4Q5FKRWh2jJVpRxuozHGB9wMvAOsBp4Pjlz6axG5ILjbzSKyUkSWYhPL7DbE0yWdmJtCds8YXlxSFO5QlFIRqlWjj7bgiD18jDELgAVN1t3T6P0tbfj+bsHhEL47LpuHPljPmpJyhmYmhjskpVSEOWyJQEQqRKS8mb8KoHcHxdjtXXVSP1LiPPzwuW+oqde2AqVUxzpsIjDGJBhjEpv5SzDGtKU0oRpJiffw58tGs35nJU98tinc4SilIkxb2ghUOzplUBqTBqbw7BebWVFcxvKifeEOSSkVITQRdCLXTMxle1kt5//1U65+crGOUKqU6hCaCDqRKUPTGdu3B0MzE9hTVc+iDaXhDkkpFQE0EXQiDofwyn9P4tUfTCLB4+L1ZdvCHZJSKgJoIuiEot1Opo7I5O0VJawo1kHplFKhpYmgk7rp9P7Ee1xc/PdFrNymyUApFTqaCDqpgekJvP7Dk/E4Hfzj443hDkcp1Y1pIujE0hI8zDqhLwu+3U7xviON+q2UUsdGE0End/XEHAT4zeurMCYi5u1RSnUwTQSdXO8eMdx17lDeXlnCxX9fxL3zV4Y7JKVUN6OJoAu47uRcbjxtAHuq6pm7qJD1OyrCHZJSqhvRRNAFiAh3nTuUF248CYfAa0v1+QKlVPvRRNCFpCdEM2lgKq8tK9b2AqVUu9FE0MVcPC6LrXtq+GDNTpYX7WNXRV24Q1JKdXGaCLqY80f1Jicllp+9soILH/6MH/97abhDUkp1cZoIuhi308EdZw+lpLyWaLeTTwt2s2FXZbjDUkp1YZoIuqBpIzN55HvjmX/zybidwjOfbw53SEqpLkwTQRckIpw9IpOB6fF8Z3RvnvtyC2tKysMdllKqi9JE0MX9bNowEmNcXDc3n7teWs6O8tpwh6SU6mI0EXRxKfEeHpo1ltQED68uLeaqx7+irNob7rCUUl1ISBOBiJwjImtFpEBE7mpm+20iskpElovI+yLSL5TxdFcTB6Ty2g8m8cTs49m0u4oZjyxic2lVuMNSSnURIUsEIuIEHgbOBYYDs0RkeJPdvgHyjDGjgBeB+0MVTySYODCVudccz/ayWiY/8BHXzl3MB2t2hDsspVQnF8oSwQSgwBiz0RhTD8wDpjfewRjzoTGmOrj4BZAdwngiwsSBqbx966n89+kDWbmtjGvn5vPGch2SQinVslAmgixga6PlouC6llwHvNXcBhG5QUTyRSR/165d7Rhi95TVI4bbzx7Cpz+ZwujsJO55bSU7K7QRWSnVvE7RWCwiVwJ5wB+a226MmWOMyTPG5KWlpXVscF2Y2+ngDzNGU1Xn47JHvuDRTzby4dqd4Q5LKdXJhDIRFAN9Gi1nB9cdRETOBH4GXGCM0YFz2tngjASeu/4E9lXX89sFq7nmycX84+MN4Q5LKdWJuEJ47MXAIBHJxSaAmcDljXcQkbHAI8A5xhi9VQ2R8f2S+fzuM6is8/Hr11fxu7fWMCo7iYkDUsMdmlKqEwhZicAY4wNuBt4BVgPPG2NWisivReSC4G5/AOKBF0RkqYjMD1U8kS7a7SQ13sP9l4yib3Is97y2knpfINxhKaU6Aelq49rn5eWZ/Pz8cIfRpX2wZgfXzs1n2shMHpw5FrezUzQVKaVCSESWGGPymtumV4AINGVoBj8/bxgLvi3hntd0DmSlIl0o2whUJ/b9U/pTWlXP3z/aQGp8FBePyyY3NS7cYSmlwkATQQT7n7MGs66kgr9+UMBfPyggr19P/nb5OJZu3cfA9HgGpseHO0SlVAfQNgLF1j3VvL2ihAffX4/bKeyt9nLmsAwem91sdaJSqgvSNgJ1WH2SY7n+1P7M+d546oI9iep8/jBHpZTqKJoI1H4TB6ay/JdTmTI0nT1V9eEORynVQTQRqIO4nA5S46PYXakPeSsVKTQRqEOkxnsoraynq7UfKaWOjSYCdYiUeA++gKGsRmc6UyoSaCJQh0iNjwLQ6iGlIoQmAnWItHgPALsrtcFYqUigiUAdImV/ItASgVKRQBOBOsT+qqEKTQRKRQJNBOoQPWKjcAiU6rMESkUETQTqEE6HkBzn0aohpSKEJgLVLPtQmZYIlIoEmghUs1LjtUSgVKTQRKCa1Sc5lg07KwkE9Olipbo7TQSqWeP69qC81sfG3ZXhDkUpFWKaCFSzxvXrCcCSzXvDHIlSKtQ0Eahm9U+No0esm6837wt3KEqpENNEoJolIozr25MlW7REoFR3p4lAtWh8v54U7KykYGdFuENRSoVQSBOBiJwjImtFpEBE7mpm+6ki8rWI+ETkklDGoo7ezOP7kBDt4r43V+vcBEp1YyFLBCLiBB4GzgWGA7NEZHiT3bYAVwPPhSoOdexS4j3ccsYgPlq7i5H3vsuD763XhKBUN+QK4bEnAAXGmI0AIjIPmA6satjBGFMY3BYIYRyqDa6emENitJv3Vu/gz++t47GFG8nqGcMLN57Eows3MWN8Nn2SY8MdplKqDUKZCLKArY2Wi4ATjuVAInIDcANA37592x6ZajWX08Glx/dhRl42z321hS837mH+sm1c+sgXrN5ezrKt+3jq2gnhDlMp1QZdorHYGDPHGJNnjMlLS0sLdzgRSUS44oR+PDhzDCfkJrN6ezmp8R4+XreLd1eWhDs8pVQbhDIRFAN9Gi1nB9epLkxEuOc7w5k8JI2Xb5pI/9Q4bnhmCd97/Ese/WQjz+dvpdbrZ/2OCgp3Vx302ep6nw5ZoVQnFMqqocXAIBHJxSaAmcDlIfw+1UFG9E7iyWtsddD8H57Mk59u4vklW1m4fjcALy4pYnnRPuI9Lt780SlkJEazp6qeaQ8u5JRBqfxhxuhwhq+UakJC2QtERKYBfwGcwBPGmN+KyK+BfGPMfBE5HngF6AnUAiXGmBGHO2ZeXp7Jz88PWczq2Bhj2FvtZf7SYu59fRU5KbHsrKhjQFo8d587lGe/3MyCb0twCLx322n0T4sPd8hKRRQRWWKMyWt2W1frDqiJoPNbtGE3QzISyN+8lztfXE5ZjRewPZDmLd7CtJG9+NOlY5r9rM8fwOXsEk1XSnUph0sEoawaUhFq4oBUAM4ekclJA1JYVLCbXkkxjMpOIsrl4NGFG7l2Ui6ZSdHc98Yqzh/VmzOHZ/DYwo384Z213HH2EGbk9SEpxh3mM1EqMmiJQHWoshovUx74iMQYN7VeP9vLavG4HFwwujcvLCkiI9HDjnI7Ic7M4/vwfxePRETCHLVSXd/hSgRaBlcdKinGzT3fGU5lnY++ybHMveZ4MpOieW3ZNi4Zn83Hd0zm0avymHl8H+Yt3srTn28Od8hKdXtaIlBhV1XnwwDxngM1lYGA4fLHvmDrnho+u2tK+IJTqpvQEoHq1OI8roOSAIDDIZwyKI3ifTVU1HrDFJlSkUETgeq0hmQkALB+p06XqVQoaSJQndbgYCJYV6LzISgVSpoIVKeV3TOGGLeTtTs0ESgVSpoIVKflcAiDM+JZv0OrhpQKJU0EqlMblJGgJQKlQkwTgerURvROZFdFHYsL94Q7FKW6LU0EqlO7NK8PWT1iuOul5Tz52SYWbdiN139gQrvNpVUHLSuljp6ONaQ6tTiPi/+9eCTXP5XPr163s5xGOR2c0D+ZE/un8Id31pLdM4afnzeMqcMzeXVpMS/kF3H/JaN0Ck2lWkmfLFZdQlmNl3pfgK+37CW/cA/zFm+lotbHif2Tqaj1sXJbOekJHnZW2HGKbp86mJunDApz1Ep1HjoMtep2ivZW8/aKEq48sR8OEf747loKdlZy0bgs/vHxBjwuJy/dNDHcYSrVaegw1Krbye4Zy/dP6b9/+e5pw/a/X7ejkr99sJ69VfX0jIsKR3hKdSnaWKy6nSlD0wkY+GDNznCHolSXoCUC1e2MykpiQFocv3t7DSOyEol1u4jzOFm4fjefrNvF0F4JXDMpF7fTQSBgKCytIjMpmtgo/eegIpO2EahuaU1JORc+/Bm13oO7liZGuyiv9RHldBDlcuAPGGq8fmKjnEwdnsEVJ/bj+JzkMEWtVOhoG4GKOEMzE3nxxoksK9qHyyHsrfZyQm4yo7N78PH6XXyxoRSv32AwDEpP4Nvifby5fDtvLN/Oc9efyIRcTQYqcmiJQKmg8lovF/7tM8pqvFxxYj+ye8QQ63ESF5wrYVd5HcN7JxLtdlBaWU9pVT0AU4dn4HJqc5vq3LREoFQrJEa7mXPVeH7y0rf89YP1tPYe6aT+Kdw9bSjH9U5CBOr9ATwuJ/W+AG6n6JzLqtMLaYlARM4BHgScwGPGmN812e4BngbGA6XAZcaYwsMdU0sEqiNU1vnYV11PVZ2fyjofxhh6xkWxorgMgJQ4D8nB5Xvmr6DWG8Ah4HQIXr8h2u2g1hsgMdrF4IwE+iTH4nE5cDsd1Hj9RLsdpMR5SIh2UVPvJybKSWK0m1qfn4paHzFuJ2U1XqJcDup9AQyQnuAhJS6Ken+Amno/VfV+6n3ND6/hcTnITIpm274a3E4HqfE23sQYOxvcroo6tu6tpqY+QJzHSVWdn8wkDx6Xk4RoF2kJHrw+Q73fT73PUO8PUO8LEDCGuCgX9f4AdV4/ANFRTlwOobzGh9spxHlcwT8nboeD4n01lAfPxekQymq89EqKwR+w156YKCfRbgcxbifRbidef4CSsloq6nykxXsorarH5RASom3sLqeD3ZV1eFwO4oPf5Q8YdlfWEeV0EB3lJMbtBKC63k+t149DhCinA7dLqK73s6O8lozEaALGUFnrIz7aRXpCNLVePx6XY/8DjB6Xkxqvnxi3kziPc3/JzxjDvmovPWLd+xN9IGCo9vqprvdRXecnPtpFz9govP4AvoDB7zckxrgQEYwx1HoD+/+bGGMwxo64GyphKRGIiBN4GDgLKAIWi8h8Y8yqRrtdB+w1xgwUkZnA74HLQhWTUq0V38z0mQAD0uIPWh7eO5GpIzJ4f/VONu2uwm8MscGLeEK0m50VtazfUcniwj3U+wLU+wPEuJ3U+QLsra5vdalDhFbvq45NUoybshovTofsT1JNNSQfX8BQVuMl3uMiyuWgut53SMeE5iR4XMRHuyitrKc+OEZWlMuBzx8gYCA2mMT8xtj/X3wBHA4hLd6DP2C485whXDwuu13PG0JbNTQBKDDGbAQQkXnAdKBxIpgO3Bt8/yLwNxER09UaLlRE6xEbxXfHH/0/Tp8/QFW9vdusqfdTXuvF43KQEO2m1usnMcaN1x/AFbxL3F1ZT2mVvROOjXIRF2UvQs3VPFXV+dheVkvvHjEEjL1b3lNZT3mtl4paH6kJHvr0jCUmykl1nY9Yj4sd5bV4fQH21XgprawnyuXA7RQ8LtvDKsrpRMTeZbudQnTwrrvG68fnNyRG2zvzyjofVfU+qur81PkCZPWIpkdsFHW+AF5fgKRYN9vLanE7BBGo9Qao8fqpqfdT6/PjdjhIS/AQ73Gxu7KO1HgPfmOoqvNRXuPF6zekJkTh9Rkq6nxU1voAyEj04A0Y6ry2FBAwdqyqaLeDgAGvL4DXH8DtdJCRGM3OilpcTgcJHhc7K2rZtLuarB7R1Hj99IyNwuN2Uue1pbWaej9VdX6q6n3BEiLkpsZSvLcGX8AQ53ERG+UM/rmIcTspr/VSXuPD5RRcDsEhwuY9VdTUB0hNiKJHTBT1vgDVXh9uhy0ZVNX5qPb6cTsk+N/fgS9Y2nE5hF5JMcf+P+phhDIRZAFbGy0XASe0tI8xxiciZUAKsLvxTiJyA3ADQN++fUMVr1IdyuV0kBRjqxqiXA6SYt37t8VE2Yus0+Hcvy4zKZrMpOhWHTva7SQl3rN/ObXR+5Zk9QjNRUZ1fl2iq4MxZo4xJs8Yk5eWlhbucJRSqlsJZSIoBvo0Ws4Ormt2HxFxAUnYRmOllFIdJJSJYDEwSERyRSQKmAnMb7LPfGB28P0lwAfaPqCUUh0rZG0EwTr/m4F3sN1HnzDGrBSRXwP5xpj5wOPAMyJSAOzBJgullFIdKKQPlBljFgALmqy7p9H7WmBGKGNQSil1eF2isVgppVToaCJQSqkIp4lAKaUiXJcbfVREdgGbj/HjqTR5WC0CROI5Q2Set55zZDjWc+5njGn2QawulwjaQkTyWxp0qbuKxHOGyDxvPefIEIpz1qohpZSKcJoIlFIqwkVaIpgT7gDCIBLPGSLzvPWcI0O7n3NEtREopZQ6VKSVCJRSSjWhiUAppSJcxCQCETlHRNaKSIGI3BXueEJFRApF5FsRWSoi+cF1ySLyHxFZH3ztGe4420JEnhCRnSKyotG6Zs9RrIeCv/tyERkXvsiPXQvnfK+IFAd/66UiMq3RtruD57xWRM4OT9RtIyJ9RORDEVklIitF5Jbg+m77Wx/mnEP7W9tJk7v3H3b00w1AfyAKWAYMD3dcITrXQiC1ybr7gbuC7+8Cfh/uONt4jqcC44AVRzpHYBrwFiDAicCX4Y6/Hc/5XuD2ZvYdHvx/3APkBv/fd4b7HI7hnHsB44LvE4B1wXPrtr/1Yc45pL91pJQI9s+fbIypBxrmT44U04Gngu+fAi4MYyxtZoz5BDtseWMtneN04GljfQH0EJFeHRNp+2nhnFsyHZhnjKkzxmwCCrD/BroUY8x2Y8zXwfcVwGrs9Lbd9rc+zDm3pF1+60hJBM3Nn3y4/7hdmQHeFZElwbmeATKMMduD70uAjPCEFlItnWN3/+1vDlaDPNGoyq/bnbOI5ABjgS+JkN+6yTlDCH/rSEkEkeRkY8w44FzgByJyauONxpYnu3Wf4Ug4x6C/AwOAMcB24I/hDSc0RCQeeAm41RhT3nhbd/2tmznnkP7WkZIIWjN/crdgjCkOvu4EXsEWE3c0FJGDrzvDF2HItHSO3fa3N8bsMMb4jTEB4FEOVAl0m3MWETf2gvhPY8zLwdXd+rdu7pxD/VtHSiJozfzJXZ6IxIlIQsN7YCqwgoPnhp4NvBaeCEOqpXOcD1wV7FFyIlDWqFqhS2tS/30R9rcGe84zRcQjIrnAIOCrjo6vrUREsNPZrjbG/KnRpm77W7d0ziH/rcPdSt6BrfHTsC3wG4CfhTueEJ1jf2wPgmXAyobzBFKA94H1wHtAcrhjbeN5/gtbPPZi60Sva+kcsT1IHg7+7t8CeeGOvx3P+ZngOS0PXhB6Ndr/Z8FzXgucG+74j/GcT8ZW+ywHlgb/pnXn3/ow5xzS31qHmFBKqQgXKVVDSimlWqCJQCmlIpwmAqWUinCaCJRSKsJpIlBKqQiniUCpJkTE32iUx6XtOVqtiOQ0HkFUqc7AFe4AlOqEaowxY8IdhFIdRUsESrVScK6H+4PzPXwlIgOD63NE5IPggGDvi0jf4PoMEXlFRJYF/yYGD+UUkUeD482/KyIxYTsppdBEoFRzYppUDV3WaFuZMWYk8DfgL8F1fwWeMsaMAv4JPBRc/xDwsTFmNHYugZXB9YOAh40xI4B9wHdDfD5KHZY+WaxUEyJSaYyJb2Z9ITDFGLMxODBYiTEmRUR2Yx/59wbXbzfGpIrILiDbGFPX6Bg5wH+MMYOCyz8B3MaY+0J/Zko1T0sESh0d08L7o1HX6L0fbatTYaaJQKmjc1mj18+D7xdhR7QFuAJYGHz/PnATgIg4RSSpo4JU6mjonYhSh4oRkaWNlt82xjR0Ie0pIsuxd/Wzgut+CDwpIncAu4BrgutvAeaIyHXYO/+bsCOIKtWpaBuBUq0UbCPIM8bsDncsSrUnrRpSSqkIpyUCpZSKcFoiUEqpCKeJQCmlIpwmAqWUinCaCJRSKsJpIlBKqQj3/wE++rC2oWVwxwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo3LlLJWUMYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5f392ab-5d47-41eb-b919-0e20fd72b2d2"
      },
      "source": [
        "# load a saved model\n",
        "from keras.models import load_model\n",
        "saved_model = load_model('best_mlp_model.h5')\n",
        "# evaluate the model\n",
        "_, train_acc = saved_model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = saved_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 0.992, Test: 0.929\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcf555CFtd5o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b335fc38-5c0d-4424-f389-7be6c1a02eb9"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
        "y_pred_class = model.predict_classes(x_test)\n",
        "y_pred = model.predict(x_test)\n",
        "y_test_class = np.argmax(y_test, axis=1)\n",
        " \n",
        "y_test_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xejxMBwA8z69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0c385ac1-dd8b-47a2-92a5-853a3a2c9dcb"
      },
      "source": [
        "confusion_matrix(y_test_class, y_pred_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1990,  278],\n",
              "       [ 262, 5308]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOhz_9tG85Q1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "9245951f-121b-4261-e136-55543cb7b74c"
      },
      "source": [
        " \n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_class, y_pred_class))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      2268\n",
            "           1       0.95      0.95      0.95      5570\n",
            "\n",
            "    accuracy                           0.93      7838\n",
            "   macro avg       0.92      0.92      0.92      7838\n",
            "weighted avg       0.93      0.93      0.93      7838\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH2y8uzp89V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "107a2ede-538f-4bfa-85a0-f584c4306230"
      },
      "source": [
        "#حساب دقة النموذج\n",
        "y_pred = model.predict(x_test)\n",
        "#Converting predictions to label\n",
        "pred = list()\n",
        "for i in range(len(y_pred)):\n",
        "    pred.append(np.argmax(y_pred[i]))\n",
        "#Converting one hot encoded test label to label\n",
        "test = list()\n",
        "for i in range(len(y_test)):\n",
        "    test.append(np.argmax(y_test[i]))\n",
        " \n",
        "from sklearn.metrics import accuracy_score\n",
        "a = accuracy_score(pred,test)\n",
        "print('Accuracy is:', a*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 93.05945394233223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-7ZItEm5WGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "from csv import reader\n",
        "# skip first line i.e. read header first and then iterate over each row od csv as a list\n",
        "with open('/data/data1.csv', 'r') as read_obj:\n",
        "    csv_reader = reader(read_obj)\n",
        "    header = next(csv_reader)\n",
        "    # Check file as empty\n",
        "    if header != None:\n",
        "        # Iterate over each row after the header in the csv\n",
        "        for row in csv_reader:\n",
        "            # row variable is a list that represents a row in csv\n",
        "            print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKQvqx0bsHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "82e98c87-d63c-48f0-fadd-0ff568c7765d"
      },
      "source": [
        " \n",
        "import time\n",
        " \n",
        "Xnew=[['70.39932429', '127673.0908', '-49.57230843', '127648.0176', '-169.5783186', '127723.2374', '65.68961121', '605.91099', '-57.00357104', '626.78553', '-173.5890232', '602.4319', '70.4222426', '127673.0908', '0', '0', '0', '0', '65.00779144', '611.5874', '118.5678861', '13.18392', '-100.8692198', '13.91636', '59.999', '0.01', '6.391383458', '0.076290455', '0', '60.65826798', '124631.8125', '-59.29595943', '124484.3594', '-179.3380777', '124715.0703', '-119.5504813', '612.7967529', '117.7267525', '632.5321045', '0.859680212', '610.1417236', '60.6802407', '124611.9844', '0', '0', '0', '0', '-120.3414991', '618.3013916', '-64.05304275', '12.7658844', '69.39789118', '12.8288269', '59.99900055', '0.02', '6.130100104', '3.135101005', '0', '60.66477135', '124187.9063', '-59.31259095', '124162.833', '-179.3014124', '124212.9796', '-119.7539088', '610.12252', '117.6855311', '628.25041', '0.658901464', '606.82654', '60.68768966', '124187.9063', '0', '0', '0', '0', '-120.4872947', '614.88338', '-64.81298579', '12.08526', '70.38786513', '11.90215', '59.999', '0.02', '6.111439531', '3.140520023', '0', '70.45089049', '127723.2374', '-49.53793097', '127096.4056', '-169.532482', '127773.3839', '65.64377459', '604.44611', '-56.87179074', '621.84156', '-173.8697725', '599.86836', '70.46234965', '127522.6512', '0', '0', '0', '0', '64.95049566', '608.47453', '119.3012721', '12.26837', '-102.060972', '11.71904', '59.999', '0.01', '6.341831592', '0.077897157', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#normal    \n",
        " \n",
        "Q=[['8.508423258', '130832.3229', '-111.4632095', '130782.1763', '128.5258926', '130907.5427', '3.729955246', '500.80585', '-116.6026409', '500.62274', '123.632833', '501.35518', '8.519882414', '130832.3229', '0', '0', '0', '0', '3.586715798', '500.98896', '0', '0', '0', '0', '60', '0', '7.438322484', '0.077833008', '0', '0.920105028', '128836.9766', '-119.0066616', '128699.4609', '120.9347557', '128913.4688', '176.7782594', '505.7258606', '56.64825709', '507.2078857', '-63.31146179', '506.9961548', '0.94757082', '128814.1563', '0', '0', '0', '0', '176.7068566', '506.6356812', '0', '0', '0', '0', '60', '0', '7.257351968', '-3.071651355', '0', '0.928191628', '128375.1424', '-119.0319819', '128350.0691', '120.9743089', '128400.2157', '176.7173728', '502.63695', '56.61968931', '503.36939', '-63.3290251', '503.00317', '0.95110994', '128375.1424', '0', '0', '0', '0', '176.6715361', '503.00317', '0', '0', '0', '0', '60', '0', '7.268424496', '-3.0691986', '0', '8.559989459', '130857.3961', '-111.428832', '130230.5644', '128.5831884', '130932.6159', '3.707036934', '497.50987', '-116.3619986', '498.24231', '123.741695', '496.59432', '8.571448615', '130681.8832', '0', '0', '0', '0', '3.689848201', '497.50987', '0', '0', '0', '0', '60', '0', '7.489104114', '0.086553421', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        " \n",
        "N=[['70.97801166', '130957.6892', '-48.99362106', '130932.6159', '-169.0053608', '131032.909', '66.42872677', '482.31174', '-53.82938485', '483.22729', '-173.646319', '483.77662', '70.99520039', '130982.7625', '0', '0', '0', '0', '66.31986479', '483.04418', '0', '0', '0', '0', '59.999', '0', '7.742501568', '0.074426263', '0', '63.67401507', '129107.1016', '-56.26922599', '128963.9531', '-176.3168392', '129194.8594', '-120.8908103', '488.6340942', '119.1714468', '489.3894043', '-0.826721188', '489.4866943', '63.70147926', '129088.1484', '0', '0', '0', '0', '-120.8468648', '489.1662598', '0', '0', '0', '0', '60', '0', '7.521482155', '-3.061590456', '0', '63.67279977', '128650.9484', '-56.28737379', '128625.8751', '-176.2819248', '128676.0216', '-120.8826356', '485.79083', '119.1981397', '485.42461', '-0.916732472', '485.42461', '63.70144766', '128650.9484', '0', '0', '0', '0', '-120.8711765', '485.42461', '0', '0', '0', '0', '59.999', '0', '7.510130448', '-3.061004926', '0', '71.03530744', '131007.8358', '-48.97070275', '130381.004', '-168.9423355', '131083.0556', '66.42872677', '480.11442', '-53.53717638', '480.11442', '-173.5890232', '479.38198', '71.0467666', '130832.3229', '0', '0', '0', '0', '66.44018592', '479.93131', '0', '0', '0', '0', '60', '0', '7.746914069', '0.08213434', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']] #normal\n",
        " \n",
        "A=[['174.2765725', '98337.36494', '59.20372897', '128550.6553', '-72.71407378', '127347.1383', '100.4796085', '1111.29459', '-4.566473627', '241.33898', '-85.22174245', '621.84156', '173.5546457', '117618.7096', '-9.786119141', '17325.62957', '-13.22959549', '1980.78833', '118.854365', '599.68525', '82.17933656', '420.23745', '78.74731936', '154.36173', '60.016', '-0.04', '1.907633306', '1.477346313', '0', '173.1802387', '47452.82422', '50.77606342', '120554.2734', '-77.66510434', '132296.1563', '-80.08758802', '1147.247314', '170.1754746', '297.0142212', '89.53582968', '636.0683594', '167.4398882', '99778.42969', '-10.8901979', '31715.72461', '-27.77069198', '21617.71094', '-63.9047252', '642.6830444', '-97.7590974', '408.9145508', '-101.4559988', '150.7930756', '60.01599884', '-0.050000001', '0.903115513', '-1.682575032', '0', '173.1822232', '47288.18722', '50.75260149', '120226.3297', '-77.62432208', '131760.0339', '85.92075096', '1062.40422', '-112.9070631', '589.06487', '123.4151091', '150.88264', '167.4526452', '99440.58882', '-10.94922346', '31567.24693', '-27.58791784', '21588.08547', '54.61433703', '466.74739', '103.9746511', '479.01576', '110.2084319', '228.33817', '60.016', '-0.04', '0.793198105', '1.484402105', '0', '169.0225496', '41797.14109', '63.34048425', '121254.3337', '-75.34967964', '121881.1655', '122.4983766', '1748.15117', '72.4218653', '600.23458', '-38.82934978', '177.06737', '173.2395189', '93849.24961', '-3.907572163', '37534.68519', '-2.056918485', '14743.08276', '142.8441079', '707.17082', '112.8039307', '436.16802', '107.7561725', '677.507', '60.014', '-0.04', '0.341509407', '1.111764282', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]#'Attack'\n",
        " \n",
        " \n",
        "transformer= Normalizer() \n",
        "Z=transformer.transform(N)\n",
        "V = ss.transform(Z)\n",
        "start_time = time.time()\n",
        " \n",
        "# make a prediction\n",
        "ynew = saved_model.predict_classes(V)\n",
        "# show the inputs and predicted outputs\n",
        "#for i in range(len(Xnew)):\n",
        "    #print(\"X=%s, Predicted=%s\" % (Xnew[i], ynew[i]))\n",
        " \n",
        "duration = time.time() - start_time\n",
        "print(\"time of test (s)\", duration)\n",
        "if ynew == [0]:\n",
        "     print(\"natural\",ynew)\n",
        "else:\n",
        "     print(\"attack\",ynew)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time of test (s) 0.040961265563964844\n",
            "natural [0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:1829: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
            "  X = check_array(X, accept_sparse='csr')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtTJ1AOIETmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}